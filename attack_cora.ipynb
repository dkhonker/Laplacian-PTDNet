{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 09:41:43.020316: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 09:41:43.242107: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 09:41:44.346687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 09:41:44.346799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 09:41:44.346807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/zsp/ljx/PTDNet/utils.py:221: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "2023-06-21 09:41:47.429121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 09:41:48.372163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3f:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   0%|          | 0/138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7543929815292358\n",
      "GCN acc on unlabled data: 0.663\n",
      "attack loss: 1.7481796741485596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   1%|          | 1/138 [03:15<7:26:06, 195.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7465808391571045\n",
      "GCN acc on unlabled data: 0.669\n",
      "attack loss: 1.7421966791152954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   1%|▏         | 2/138 [06:29<7:20:53, 194.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.757859468460083\n",
      "GCN acc on unlabled data: 0.692\n",
      "attack loss: 1.7530452013015747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   2%|▏         | 3/138 [09:48<7:22:22, 196.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7578752040863037\n",
      "GCN acc on unlabled data: 0.633\n",
      "attack loss: 1.751731276512146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   3%|▎         | 4/138 [13:05<7:19:51, 196.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7562236785888672\n",
      "GCN acc on unlabled data: 0.644\n",
      "attack loss: 1.7518296241760254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:   4%|▎         | 5/138 [17:03<7:33:52, 204.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m ptb_rate \u001b[39m=\u001b[39m \u001b[39m0.03\u001b[39m\n\u001b[1;32m     55\u001b[0m perturbations \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(ptb_rate \u001b[39m*\u001b[39m (adj\u001b[39m.\u001b[39msum()\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m---> 58\u001b[0m model\u001b[39m.\u001b[39;49mattack(features, adj, single_label, idx_train, idx_unlabeled, n_perturbations\u001b[39m=\u001b[39;49mperturbations, ll_constraint\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     59\u001b[0m modified_adj \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodified_adj\n\u001b[1;32m     60\u001b[0m \u001b[39m# print(adj)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# print(\"shiy\")\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# print(modified_adj)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deeprobust/graph/global_attack/mettack.py:351\u001b[0m, in \u001b[0;36mMetattack.attack\u001b[0;34m(self, ori_features, ori_adj, labels, idx_train, idx_unlabeled, n_perturbations, ll_constraint, ll_cutoff)\u001b[0m\n\u001b[1;32m    348\u001b[0m     modified_features \u001b[39m=\u001b[39m ori_features \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_changes\n\u001b[1;32m    350\u001b[0m adj_norm \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mnormalize_adj_tensor(modified_adj)\n\u001b[0;32m--> 351\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_train(modified_features, adj_norm, idx_train, idx_unlabeled, labels)\n\u001b[1;32m    353\u001b[0m adj_grad, feature_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_meta_grad(modified_features, adj_norm, idx_train, idx_unlabeled, labels, labels_self_training)\n\u001b[1;32m    355\u001b[0m adj_meta_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deeprobust/graph/global_attack/mettack.py:255\u001b[0m, in \u001b[0;36mMetattack.inner_train\u001b[0;34m(self, features, adj_norm, idx_train, idx_unlabeled, labels)\u001b[0m\n\u001b[1;32m    253\u001b[0m     hidden \u001b[39m=\u001b[39m adj_norm \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mspmm(hidden, w) \u001b[39m+\u001b[39m b\n\u001b[1;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     hidden \u001b[39m=\u001b[39m adj_norm \u001b[39m@\u001b[39;49m hidden \u001b[39m@\u001b[39m w \u001b[39m+\u001b[39m b\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_relu \u001b[39mand\u001b[39;00m ix \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    258\u001b[0m     hidden \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(hidden)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Settings\n",
    "dataset_name='citeseer'\n",
    "args.dropout=0.0\n",
    "args.dataset=dataset_name\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features_tmp=features.copy()\n",
    "features = preprocess_features(features).A\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "from deeprobust.graph.data import Dataset\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.global_attack import Metattack\n",
    "# Setup Surrogate model\n",
    "idx_train=np.array(np.where(train_mask==1)).tolist()[0]\n",
    "idx_val=np.array(np.where(val_mask==1)).tolist()[0]\n",
    "idx_unlabeled=np.array(np.where(test_mask==1)).tolist()[0]\n",
    "surrogate = GCN(nfeat=features.shape[1], nclass=single_label.max().item()+1,\n",
    "                nhid=256, dropout=0,lr=0.001, with_relu=True, with_bias=False, device='cpu').to('cpu')\n",
    "print(type(features))\n",
    "surrogate.fit(features, adj, single_label, idx_train, idx_val, patience=100)\n",
    "# Setup Attack Model\n",
    "model = Metattack(surrogate, nnodes=adj.shape[0], feature_shape=features.shape,\n",
    "        attack_structure=True, attack_features=False, device='cpu', lambda_=0).to('cpu')\n",
    "# Attack\n",
    "ptb_rate = 0.03\n",
    "perturbations = int(ptb_rate * (adj.sum()//2))\n",
    "\n",
    "\n",
    "model.attack(features, adj, single_label, idx_train, idx_unlabeled, n_perturbations=perturbations, ll_constraint=False)\n",
    "modified_adj = model.modified_adj\n",
    "# print(adj)\n",
    "# print(\"shiy\")\n",
    "# print(modified_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_adj=sp.csr_array(modified_adj.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(modified_adj.toarray()))\n",
    "np.save('tmp'+str(ptb_rate/0.01)+'.npy',modified_adj.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(args.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsp/ljx/PTDNet/utils.py:221: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "2023-06-20 23:30:23.060400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-20 23:30:23.646199: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55ed5abf1330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-20 23:30:23.646226: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-06-20 23:30:23.651527: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-20 23:30:23.784817: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f0ca53b5700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f0ca53b5700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "edge_vol 4687.5312\n",
      "Epoch: 0001 train_loss= 1.79177 val_loss= 1.79114 train_acc= 0.40833 val_acc= 0.26400 best_val_acc_trail= 0.26400 test_acc= 0.26200\n",
      "time  2.971179485321045\n",
      "edge_vol 4675.9277\n",
      "Epoch: 0002 train_loss= 1.78975 val_loss= 1.79047 train_acc= 0.56667 val_acc= 0.30600 best_val_acc_trail= 0.30600 test_acc= 0.31700\n",
      "time  3.3047103881835938\n",
      "edge_vol 4663.8853\n",
      "Epoch: 0003 train_loss= 1.78768 val_loss= 1.78978 train_acc= 0.80000 val_acc= 0.36800 best_val_acc_trail= 0.36800 test_acc= 0.37200\n",
      "time  3.5706982612609863\n",
      "edge_vol 4651.415\n",
      "Epoch: 0004 train_loss= 1.78564 val_loss= 1.78905 train_acc= 0.83333 val_acc= 0.39800 best_val_acc_trail= 0.39800 test_acc= 0.41700\n",
      "time  3.835038423538208\n",
      "edge_vol 4638.62\n",
      "Epoch: 0005 train_loss= 1.78344 val_loss= 1.78826 train_acc= 0.86667 val_acc= 0.44600 best_val_acc_trail= 0.44600 test_acc= 0.45000\n",
      "time  4.112331867218018\n",
      "edge_vol 4625.558\n",
      "Epoch: 0006 train_loss= 1.78126 val_loss= 1.78741 train_acc= 0.87500 val_acc= 0.47400 best_val_acc_trail= 0.47400 test_acc= 0.47400\n",
      "time  4.396091461181641\n",
      "edge_vol 4612.304\n",
      "Epoch: 0007 train_loss= 1.77898 val_loss= 1.78647 train_acc= 0.89167 val_acc= 0.49800 best_val_acc_trail= 0.49800 test_acc= 0.49400\n",
      "time  4.682877063751221\n",
      "edge_vol 4598.911\n",
      "Epoch: 0008 train_loss= 1.77650 val_loss= 1.78546 train_acc= 0.90000 val_acc= 0.51400 best_val_acc_trail= 0.51400 test_acc= 0.51700\n",
      "time  4.9626476764678955\n",
      "edge_vol 4585.3623\n",
      "Epoch: 0009 train_loss= 1.77369 val_loss= 1.78437 train_acc= 0.90000 val_acc= 0.52800 best_val_acc_trail= 0.52800 test_acc= 0.53200\n",
      "time  5.249080657958984\n",
      "edge_vol 4571.689\n",
      "Epoch: 0010 train_loss= 1.77092 val_loss= 1.78320 train_acc= 0.89167 val_acc= 0.55000 best_val_acc_trail= 0.55000 test_acc= 0.54700\n",
      "time  5.533940553665161\n",
      "edge_vol 4557.873\n",
      "Epoch: 0011 train_loss= 1.76801 val_loss= 1.78196 train_acc= 0.89167 val_acc= 0.56200 best_val_acc_trail= 0.56200 test_acc= 0.56000\n",
      "time  5.820615768432617\n",
      "edge_vol 4543.926\n",
      "Epoch: 0012 train_loss= 1.76516 val_loss= 1.78065 train_acc= 0.89167 val_acc= 0.57800 best_val_acc_trail= 0.57800 test_acc= 0.57500\n",
      "time  6.098778009414673\n",
      "edge_vol 4529.8413\n",
      "Epoch: 0013 train_loss= 1.76192 val_loss= 1.77927 train_acc= 0.89167 val_acc= 0.59000 best_val_acc_trail= 0.59000 test_acc= 0.58400\n",
      "time  6.378627300262451\n",
      "edge_vol 4515.584\n",
      "Epoch: 0014 train_loss= 1.75850 val_loss= 1.77783 train_acc= 0.89167 val_acc= 0.61000 best_val_acc_trail= 0.61000 test_acc= 0.59800\n",
      "time  6.683661222457886\n",
      "edge_vol 4501.1436\n",
      "Epoch: 0015 train_loss= 1.75469 val_loss= 1.77634 train_acc= 0.89167 val_acc= 0.61600 best_val_acc_trail= 0.61600 test_acc= 0.61000\n",
      "time  6.964078426361084\n",
      "edge_vol 4486.5635\n",
      "Epoch: 0016 train_loss= 1.75083 val_loss= 1.77479 train_acc= 0.89167 val_acc= 0.62000 best_val_acc_trail= 0.62000 test_acc= 0.61300\n",
      "time  7.2364232540130615\n",
      "edge_vol 4471.7896\n",
      "Epoch: 0017 train_loss= 1.74729 val_loss= 1.77318 train_acc= 0.89167 val_acc= 0.63000 best_val_acc_trail= 0.63000 test_acc= 0.61600\n",
      "time  7.50960898399353\n",
      "edge_vol 4456.875\n",
      "Epoch: 0018 train_loss= 1.74334 val_loss= 1.77153 train_acc= 0.89167 val_acc= 0.63800 best_val_acc_trail= 0.63800 test_acc= 0.61800\n",
      "time  7.817741394042969\n",
      "edge_vol 4441.796\n",
      "Epoch: 0019 train_loss= 1.73882 val_loss= 1.76984 train_acc= 0.90833 val_acc= 0.64600 best_val_acc_trail= 0.64600 test_acc= 0.62400\n",
      "time  8.110744953155518\n",
      "edge_vol 4426.5405\n",
      "Epoch: 0020 train_loss= 1.73527 val_loss= 1.76810 train_acc= 0.91667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.62600\n",
      "time  8.393329620361328\n",
      "edge_vol 4411.104\n",
      "Epoch: 0021 train_loss= 1.73006 val_loss= 1.76633 train_acc= 0.91667 val_acc= 0.65600 best_val_acc_trail= 0.65600 test_acc= 0.63200\n",
      "time  8.69161057472229\n",
      "edge_vol 4395.4556\n",
      "Epoch: 0022 train_loss= 1.72517 val_loss= 1.76450 train_acc= 0.91667 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.63600\n",
      "time  8.970216274261475\n",
      "edge_vol 4379.622\n",
      "Epoch: 0023 train_loss= 1.72117 val_loss= 1.76265 train_acc= 0.91667 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  9.253542184829712\n",
      "edge_vol 4363.6104\n",
      "Epoch: 0024 train_loss= 1.71631 val_loss= 1.76076 train_acc= 0.91667 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  9.538388729095459\n",
      "edge_vol 4347.408\n",
      "Epoch: 0025 train_loss= 1.71117 val_loss= 1.75885 train_acc= 0.91667 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  9.830612659454346\n",
      "edge_vol 4330.9614\n",
      "Epoch: 0026 train_loss= 1.70566 val_loss= 1.75690 train_acc= 0.91667 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  10.124267339706421\n",
      "edge_vol 4314.29\n",
      "Epoch: 0027 train_loss= 1.70006 val_loss= 1.75493 train_acc= 0.91667 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  10.421570301055908\n",
      "edge_vol 4297.4277\n",
      "Epoch: 0028 train_loss= 1.69520 val_loss= 1.75292 train_acc= 0.91667 val_acc= 0.65800 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  10.708297491073608\n",
      "edge_vol 4280.386\n",
      "Epoch: 0029 train_loss= 1.68984 val_loss= 1.75089 train_acc= 0.91667 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  10.997889041900635\n",
      "edge_vol 4263.1406\n",
      "Epoch: 0030 train_loss= 1.68440 val_loss= 1.74883 train_acc= 0.91667 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  11.29862117767334\n",
      "edge_vol 4245.67\n",
      "Epoch: 0031 train_loss= 1.67788 val_loss= 1.74676 train_acc= 0.92500 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  11.581924676895142\n",
      "edge_vol 4227.955\n",
      "Epoch: 0032 train_loss= 1.67304 val_loss= 1.74465 train_acc= 0.92500 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64000\n",
      "time  11.857885837554932\n",
      "edge_vol 4210.0254\n",
      "Epoch: 0033 train_loss= 1.66804 val_loss= 1.74252 train_acc= 0.92500 val_acc= 0.66600 best_val_acc_trail= 0.66600 test_acc= 0.65700\n",
      "time  12.149293184280396\n",
      "edge_vol 4191.8584\n",
      "Epoch: 0034 train_loss= 1.66079 val_loss= 1.74035 train_acc= 0.92500 val_acc= 0.66800 best_val_acc_trail= 0.66800 test_acc= 0.65800\n",
      "time  12.414789199829102\n",
      "edge_vol 4173.491\n",
      "Epoch: 0035 train_loss= 1.65616 val_loss= 1.73815 train_acc= 0.92500 val_acc= 0.66800 best_val_acc_trail= 0.66800 test_acc= 0.65800\n",
      "time  12.718907117843628\n",
      "edge_vol 4154.923\n",
      "Epoch: 0036 train_loss= 1.65021 val_loss= 1.73593 train_acc= 0.92500 val_acc= 0.66600 best_val_acc_trail= 0.66800 test_acc= 0.65800\n",
      "time  12.999453783035278\n",
      "edge_vol 4136.131\n",
      "Epoch: 0037 train_loss= 1.64352 val_loss= 1.73368 train_acc= 0.92500 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.66100\n",
      "time  13.292910099029541\n",
      "edge_vol 4117.155\n",
      "Epoch: 0038 train_loss= 1.63671 val_loss= 1.73140 train_acc= 0.92500 val_acc= 0.67200 best_val_acc_trail= 0.67400 test_acc= 0.66100\n",
      "time  13.567670106887817\n",
      "edge_vol 4097.9907\n",
      "Epoch: 0039 train_loss= 1.63090 val_loss= 1.72909 train_acc= 0.92500 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.66100\n",
      "time  13.849972248077393\n",
      "edge_vol 4078.6934\n",
      "Epoch: 0040 train_loss= 1.62416 val_loss= 1.72678 train_acc= 0.92500 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  14.124496221542358\n",
      "edge_vol 4059.1948\n",
      "Epoch: 0041 train_loss= 1.61672 val_loss= 1.72446 train_acc= 0.92500 val_acc= 0.67400 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  14.420692443847656\n",
      "edge_vol 4039.4746\n",
      "Epoch: 0042 train_loss= 1.61127 val_loss= 1.72215 train_acc= 0.93333 val_acc= 0.67400 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  14.71555233001709\n",
      "edge_vol 4019.56\n",
      "Epoch: 0043 train_loss= 1.60450 val_loss= 1.71982 train_acc= 0.93333 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  15.113704919815063\n",
      "edge_vol 3999.434\n",
      "Epoch: 0044 train_loss= 1.59786 val_loss= 1.71745 train_acc= 0.93333 val_acc= 0.67400 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  15.513474941253662\n",
      "edge_vol 3979.177\n",
      "Epoch: 0045 train_loss= 1.59303 val_loss= 1.71509 train_acc= 0.94167 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.66500\n",
      "time  15.911014795303345\n",
      "edge_vol 3958.8247\n",
      "Epoch: 0046 train_loss= 1.58631 val_loss= 1.71273 train_acc= 0.94167 val_acc= 0.67800 best_val_acc_trail= 0.67800 test_acc= 0.66700\n",
      "time  16.303855180740356\n",
      "edge_vol 3938.1782\n",
      "Epoch: 0047 train_loss= 1.57736 val_loss= 1.71036 train_acc= 0.94167 val_acc= 0.68000 best_val_acc_trail= 0.68000 test_acc= 0.66700\n",
      "time  16.69789147377014\n",
      "edge_vol 3917.3855\n",
      "Epoch: 0048 train_loss= 1.56887 val_loss= 1.70798 train_acc= 0.94167 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.66800\n",
      "time  17.089545726776123\n",
      "edge_vol 3896.5156\n",
      "Epoch: 0049 train_loss= 1.56413 val_loss= 1.70559 train_acc= 0.94167 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.66800\n",
      "time  17.494458436965942\n",
      "edge_vol 3875.331\n",
      "Epoch: 0050 train_loss= 1.55432 val_loss= 1.70325 train_acc= 0.94167 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.66900\n",
      "time  17.888975143432617\n",
      "edge_vol 3854.0017\n",
      "Epoch: 0051 train_loss= 1.55215 val_loss= 1.70093 train_acc= 0.94167 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.66900\n",
      "time  18.27277374267578\n",
      "edge_vol 3832.5674\n",
      "Epoch: 0052 train_loss= 1.54054 val_loss= 1.69864 train_acc= 0.95000 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.66600\n",
      "time  18.667277336120605\n",
      "edge_vol 3811.0398\n",
      "Epoch: 0053 train_loss= 1.53512 val_loss= 1.69641 train_acc= 0.95833 val_acc= 0.68800 best_val_acc_trail= 0.68800 test_acc= 0.66700\n",
      "time  19.06562066078186\n",
      "edge_vol 3789.2087\n",
      "Epoch: 0054 train_loss= 1.53106 val_loss= 1.69421 train_acc= 0.95833 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  19.462008476257324\n",
      "edge_vol 3767.1277\n",
      "Epoch: 0055 train_loss= 1.52004 val_loss= 1.69203 train_acc= 0.95833 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  19.853071689605713\n",
      "edge_vol 3744.9377\n",
      "Epoch: 0056 train_loss= 1.51392 val_loss= 1.68987 train_acc= 0.96667 val_acc= 0.68800 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  20.24080753326416\n",
      "edge_vol 3722.4736\n",
      "Epoch: 0057 train_loss= 1.50846 val_loss= 1.68775 train_acc= 0.96667 val_acc= 0.68600 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  20.62988543510437\n",
      "edge_vol 3699.7744\n",
      "Epoch: 0058 train_loss= 1.50107 val_loss= 1.68572 train_acc= 0.97500 val_acc= 0.68600 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  21.020375967025757\n",
      "edge_vol 3676.7937\n",
      "Epoch: 0059 train_loss= 1.49419 val_loss= 1.68373 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  21.40459394454956\n",
      "edge_vol 3653.658\n",
      "Epoch: 0060 train_loss= 1.48583 val_loss= 1.68175 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  21.78443431854248\n",
      "edge_vol 3630.636\n",
      "Epoch: 0061 train_loss= 1.48040 val_loss= 1.67985 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  22.161200046539307\n",
      "edge_vol 3607.32\n",
      "Epoch: 0062 train_loss= 1.47134 val_loss= 1.67794 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  22.55675721168518\n",
      "edge_vol 3583.972\n",
      "Epoch: 0063 train_loss= 1.46051 val_loss= 1.67612 train_acc= 0.98333 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.66600\n",
      "time  22.953102588653564\n",
      "edge_vol 3560.2168\n",
      "Epoch: 0064 train_loss= 1.45724 val_loss= 1.67440 train_acc= 0.98333 val_acc= 0.69400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  23.376169204711914\n",
      "edge_vol 3536.1892\n",
      "Epoch: 0065 train_loss= 1.44893 val_loss= 1.67280 train_acc= 0.98333 val_acc= 0.69400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  23.758734941482544\n",
      "edge_vol 3511.902\n",
      "Epoch: 0066 train_loss= 1.44153 val_loss= 1.67126 train_acc= 0.98333 val_acc= 0.69200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  24.148213148117065\n",
      "edge_vol 3487.3833\n",
      "Epoch: 0067 train_loss= 1.43865 val_loss= 1.66989 train_acc= 0.99167 val_acc= 0.69200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  24.53187370300293\n",
      "edge_vol 3462.9434\n",
      "Epoch: 0068 train_loss= 1.43226 val_loss= 1.66861 train_acc= 0.99167 val_acc= 0.69400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  24.920653820037842\n",
      "edge_vol 3438.0366\n",
      "Epoch: 0069 train_loss= 1.42419 val_loss= 1.66735 train_acc= 0.99167 val_acc= 0.69000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  25.2914559841156\n",
      "edge_vol 3412.8447\n",
      "Epoch: 0070 train_loss= 1.41420 val_loss= 1.66611 train_acc= 0.99167 val_acc= 0.69000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  25.662075996398926\n",
      "edge_vol 3387.3838\n",
      "Epoch: 0071 train_loss= 1.41045 val_loss= 1.66488 train_acc= 0.99167 val_acc= 0.68800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  26.073331832885742\n",
      "edge_vol 3361.6213\n",
      "Epoch: 0072 train_loss= 1.40734 val_loss= 1.66362 train_acc= 0.99167 val_acc= 0.68800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  26.481800079345703\n",
      "edge_vol 3335.769\n",
      "Epoch: 0073 train_loss= 1.39706 val_loss= 1.66240 train_acc= 0.99167 val_acc= 0.68800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  26.866151094436646\n",
      "edge_vol 3309.9287\n",
      "Epoch: 0074 train_loss= 1.39585 val_loss= 1.66128 train_acc= 0.99167 val_acc= 0.68400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  27.23757767677307\n",
      "edge_vol 3284.1895\n",
      "Epoch: 0075 train_loss= 1.38801 val_loss= 1.66028 train_acc= 0.99167 val_acc= 0.68400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  27.63034677505493\n",
      "edge_vol 3257.8257\n",
      "Epoch: 0076 train_loss= 1.37451 val_loss= 1.65932 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  28.025206804275513\n",
      "edge_vol 3230.8486\n",
      "Epoch: 0077 train_loss= 1.37153 val_loss= 1.65843 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  28.44054889678955\n",
      "edge_vol 3203.708\n",
      "Epoch: 0078 train_loss= 1.37138 val_loss= 1.65769 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  28.833662748336792\n",
      "edge_vol 3176.9465\n",
      "Epoch: 0079 train_loss= 1.36189 val_loss= 1.65709 train_acc= 0.99167 val_acc= 0.67800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  29.219204425811768\n",
      "edge_vol 3149.8853\n",
      "Epoch: 0080 train_loss= 1.34818 val_loss= 1.65660 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  29.610870122909546\n",
      "edge_vol 3122.4172\n",
      "Epoch: 0081 train_loss= 1.35072 val_loss= 1.65623 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  29.991307020187378\n",
      "edge_vol 3094.541\n",
      "Epoch: 0082 train_loss= 1.34200 val_loss= 1.65580 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  30.406147241592407\n",
      "edge_vol 3066.849\n",
      "Epoch: 0083 train_loss= 1.33632 val_loss= 1.65553 train_acc= 0.99167 val_acc= 0.67200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  30.78965735435486\n",
      "edge_vol 3038.841\n",
      "Epoch: 0084 train_loss= 1.33228 val_loss= 1.65530 train_acc= 0.99167 val_acc= 0.67200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  31.1644868850708\n",
      "edge_vol 3010.5415\n",
      "Epoch: 0085 train_loss= 1.32622 val_loss= 1.65513 train_acc= 0.99167 val_acc= 0.67400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  31.62391448020935\n",
      "edge_vol 2981.7441\n",
      "Epoch: 0086 train_loss= 1.32169 val_loss= 1.65504 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  32.02206015586853\n",
      "edge_vol 2952.413\n",
      "Epoch: 0087 train_loss= 1.32076 val_loss= 1.65499 train_acc= 1.00000 val_acc= 0.67000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  32.421085596084595\n",
      "edge_vol 2922.641\n",
      "Epoch: 0088 train_loss= 1.31001 val_loss= 1.65502 train_acc= 1.00000 val_acc= 0.67400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  32.809131383895874\n",
      "edge_vol 2892.6948\n",
      "Epoch: 0089 train_loss= 1.30609 val_loss= 1.65504 train_acc= 1.00000 val_acc= 0.67400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  33.22288370132446\n",
      "edge_vol 2861.847\n",
      "Epoch: 0090 train_loss= 1.29952 val_loss= 1.65501 train_acc= 1.00000 val_acc= 0.67400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  33.604219913482666\n",
      "edge_vol 2830.9045\n",
      "Epoch: 0091 train_loss= 1.29835 val_loss= 1.65497 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  33.985992193222046\n",
      "edge_vol 2799.842\n",
      "Epoch: 0092 train_loss= 1.29014 val_loss= 1.65487 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  34.42310380935669\n",
      "edge_vol 2767.9124\n",
      "Epoch: 0093 train_loss= 1.28138 val_loss= 1.65472 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  34.812395095825195\n",
      "edge_vol 2735.0898\n",
      "Epoch: 0094 train_loss= 1.27683 val_loss= 1.65450 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  35.20964980125427\n",
      "edge_vol 2701.6055\n",
      "Epoch: 0095 train_loss= 1.26402 val_loss= 1.65430 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  35.61803436279297\n",
      "edge_vol 2667.604\n",
      "Epoch: 0096 train_loss= 1.26602 val_loss= 1.65408 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  36.02145028114319\n",
      "edge_vol 2632.9912\n",
      "Epoch: 0097 train_loss= 1.26095 val_loss= 1.65391 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  36.40566897392273\n",
      "edge_vol 2597.4995\n",
      "Epoch: 0098 train_loss= 1.25772 val_loss= 1.65377 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  36.79051446914673\n",
      "edge_vol 2561.3203\n",
      "Epoch: 0099 train_loss= 1.25542 val_loss= 1.65367 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  37.17599844932556\n",
      "edge_vol 2524.5474\n",
      "Epoch: 0100 train_loss= 1.24053 val_loss= 1.65357 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  37.57431221008301\n",
      "edge_vol 2486.9548\n",
      "Epoch: 0101 train_loss= 1.24783 val_loss= 1.65340 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  37.966161251068115\n",
      "edge_vol 2449.208\n",
      "Epoch: 0102 train_loss= 1.23588 val_loss= 1.65332 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  38.35607361793518\n",
      "edge_vol 2411.3765\n",
      "Epoch: 0103 train_loss= 1.22363 val_loss= 1.65321 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  38.76955842971802\n",
      "edge_vol 2372.8416\n",
      "Epoch: 0104 train_loss= 1.22574 val_loss= 1.65308 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  39.16481041908264\n",
      "edge_vol 2333.689\n",
      "Epoch: 0105 train_loss= 1.22473 val_loss= 1.65291 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  39.54608702659607\n",
      "edge_vol 2293.842\n",
      "Epoch: 0106 train_loss= 1.20792 val_loss= 1.65274 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  39.948580265045166\n",
      "edge_vol 2253.4775\n",
      "Epoch: 0107 train_loss= 1.21031 val_loss= 1.65257 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  40.325716495513916\n",
      "edge_vol 2212.8018\n",
      "Epoch: 0108 train_loss= 1.20888 val_loss= 1.65240 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  40.72929930686951\n",
      "edge_vol 2171.438\n",
      "Epoch: 0109 train_loss= 1.20369 val_loss= 1.65230 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  41.14777064323425\n",
      "edge_vol 2129.7856\n",
      "Epoch: 0110 train_loss= 1.19043 val_loss= 1.65211 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  41.536330699920654\n",
      "edge_vol 2088.097\n",
      "Epoch: 0111 train_loss= 1.19535 val_loss= 1.65210 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  41.92454957962036\n",
      "edge_vol 2046.3898\n",
      "Epoch: 0112 train_loss= 1.18457 val_loss= 1.65193 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  42.31091833114624\n",
      "edge_vol 2004.0176\n",
      "Epoch: 0113 train_loss= 1.18088 val_loss= 1.65160 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  42.70794129371643\n",
      "edge_vol 1961.402\n",
      "Epoch: 0114 train_loss= 1.17983 val_loss= 1.65129 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  43.09304738044739\n",
      "edge_vol 1918.1414\n",
      "Epoch: 0115 train_loss= 1.17640 val_loss= 1.65091 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  43.47710585594177\n",
      "edge_vol 1875.293\n",
      "Epoch: 0116 train_loss= 1.16892 val_loss= 1.65048 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  43.84668803215027\n",
      "edge_vol 1832.5142\n",
      "Epoch: 0117 train_loss= 1.16317 val_loss= 1.65008 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  44.229191303253174\n",
      "edge_vol 1789.4255\n",
      "Epoch: 0118 train_loss= 1.15857 val_loss= 1.64964 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  44.61692404747009\n",
      "edge_vol 1746.227\n",
      "Epoch: 0119 train_loss= 1.14992 val_loss= 1.64923 train_acc= 1.00000 val_acc= 0.65600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  45.00158190727234\n",
      "edge_vol 1702.9668\n",
      "Epoch: 0120 train_loss= 1.14760 val_loss= 1.64893 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  45.39242243766785\n",
      "edge_vol 1659.7872\n",
      "Epoch: 0121 train_loss= 1.13519 val_loss= 1.64868 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  45.80034804344177\n",
      "edge_vol 1616.8079\n",
      "Epoch: 0122 train_loss= 1.13673 val_loss= 1.64862 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  46.17916941642761\n",
      "edge_vol 1573.7239\n",
      "Epoch: 0123 train_loss= 1.13301 val_loss= 1.64855 train_acc= 1.00000 val_acc= 0.65400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  46.608577728271484\n",
      "edge_vol 1530.8748\n",
      "Epoch: 0124 train_loss= 1.12601 val_loss= 1.64842 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  46.99389433860779\n",
      "edge_vol 1488.463\n",
      "Epoch: 0125 train_loss= 1.12549 val_loss= 1.64834 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  47.38687014579773\n",
      "edge_vol 1446.4465\n",
      "Epoch: 0126 train_loss= 1.12370 val_loss= 1.64843 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  47.78320837020874\n",
      "edge_vol 1404.8298\n",
      "Epoch: 0127 train_loss= 1.11199 val_loss= 1.64854 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  48.219480991363525\n",
      "edge_vol 1363.626\n",
      "Epoch: 0128 train_loss= 1.10818 val_loss= 1.64874 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  48.61769437789917\n",
      "edge_vol 1323.0673\n",
      "Epoch: 0129 train_loss= 1.09503 val_loss= 1.64873 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  48.99941802024841\n",
      "edge_vol 1282.8745\n",
      "Epoch: 0130 train_loss= 1.09192 val_loss= 1.64863 train_acc= 1.00000 val_acc= 0.64600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  49.42962646484375\n",
      "edge_vol 1243.1536\n",
      "Epoch: 0131 train_loss= 1.08326 val_loss= 1.64867 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  49.82376027107239\n",
      "edge_vol 1203.8063\n",
      "Epoch: 0132 train_loss= 1.07889 val_loss= 1.64863 train_acc= 1.00000 val_acc= 0.64600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  50.24061608314514\n",
      "edge_vol 1165.1136\n",
      "Epoch: 0133 train_loss= 1.08277 val_loss= 1.64852 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  50.62983512878418\n",
      "edge_vol 1127.2181\n",
      "Epoch: 0134 train_loss= 1.07012 val_loss= 1.64837 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  51.0509729385376\n",
      "edge_vol 1089.8097\n",
      "Epoch: 0135 train_loss= 1.06314 val_loss= 1.64804 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  51.42298626899719\n",
      "edge_vol 1052.8379\n",
      "Epoch: 0136 train_loss= 1.05143 val_loss= 1.64758 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  51.848243713378906\n",
      "edge_vol 1016.5607\n",
      "Epoch: 0137 train_loss= 1.04841 val_loss= 1.64693 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  52.26896262168884\n",
      "edge_vol 981.1841\n",
      "Epoch: 0138 train_loss= 1.04151 val_loss= 1.64616 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  52.65400671958923\n",
      "edge_vol 946.39874\n",
      "Epoch: 0139 train_loss= 1.03250 val_loss= 1.64535 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  53.07388091087341\n",
      "edge_vol 912.5027\n",
      "Epoch: 0140 train_loss= 1.02345 val_loss= 1.64443 train_acc= 1.00000 val_acc= 0.62800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  53.48378348350525\n",
      "edge_vol 879.28357\n",
      "Epoch: 0141 train_loss= 1.01910 val_loss= 1.64352 train_acc= 1.00000 val_acc= 0.62800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  53.874500036239624\n",
      "edge_vol 846.48444\n",
      "Epoch: 0142 train_loss= 1.01205 val_loss= 1.64253 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  54.26214551925659\n",
      "edge_vol 814.31104\n",
      "Epoch: 0143 train_loss= 1.00504 val_loss= 1.64149 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  54.67428398132324\n",
      "edge_vol 782.91626\n",
      "Epoch: 0144 train_loss= 0.99965 val_loss= 1.64041 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  55.065637826919556\n",
      "edge_vol 752.36646\n",
      "Epoch: 0145 train_loss= 0.98661 val_loss= 1.63942 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  55.45333647727966\n",
      "edge_vol 722.59686\n",
      "Epoch: 0146 train_loss= 0.98108 val_loss= 1.63820 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  55.84714698791504\n",
      "edge_vol 693.68774\n",
      "Epoch: 0147 train_loss= 0.96332 val_loss= 1.63675 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  56.26163697242737\n",
      "edge_vol 665.735\n",
      "Epoch: 0148 train_loss= 0.96044 val_loss= 1.63538 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  56.64122128486633\n",
      "edge_vol 638.6832\n",
      "Epoch: 0149 train_loss= 0.95343 val_loss= 1.63402 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  57.03152632713318\n",
      "edge_vol 612.3059\n",
      "Epoch: 0150 train_loss= 0.93037 val_loss= 1.63248 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  57.4310736656189\n",
      "edge_vol 586.68134\n",
      "Epoch: 0151 train_loss= 0.93200 val_loss= 1.63091 train_acc= 1.00000 val_acc= 0.62600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  57.842528104782104\n",
      "edge_vol 561.9596\n",
      "Epoch: 0152 train_loss= 0.91998 val_loss= 1.62933 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  58.242634534835815\n",
      "edge_vol 538.34973\n",
      "Epoch: 0153 train_loss= 0.91142 val_loss= 1.62777 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  58.6352219581604\n",
      "edge_vol 515.5581\n",
      "Epoch: 0154 train_loss= 0.89285 val_loss= 1.62619 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  59.031646966934204\n",
      "edge_vol 493.41235\n",
      "Epoch: 0155 train_loss= 0.89079 val_loss= 1.62463 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  59.41854381561279\n",
      "edge_vol 472.17145\n",
      "Epoch: 0156 train_loss= 0.86512 val_loss= 1.62304 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  59.831262826919556\n",
      "edge_vol 451.9002\n",
      "Epoch: 0157 train_loss= 0.86904 val_loss= 1.62134 train_acc= 1.00000 val_acc= 0.62800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  60.216485261917114\n",
      "edge_vol 432.42355\n",
      "Epoch: 0158 train_loss= 0.84975 val_loss= 1.61953 train_acc= 1.00000 val_acc= 0.62600 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  60.604191064834595\n",
      "edge_vol 413.5398\n",
      "Epoch: 0159 train_loss= 0.84947 val_loss= 1.61784 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  61.02203559875488\n",
      "edge_vol 395.25412\n",
      "Epoch: 0160 train_loss= 0.82777 val_loss= 1.61599 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  61.428922176361084\n",
      "edge_vol 377.7754\n",
      "Epoch: 0161 train_loss= 0.81751 val_loss= 1.61399 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  61.82307028770447\n",
      "edge_vol 361.11914\n",
      "Epoch: 0162 train_loss= 0.80098 val_loss= 1.61206 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  62.24349403381348\n",
      "edge_vol 345.28622\n",
      "Epoch: 0163 train_loss= 0.79023 val_loss= 1.61013 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  62.64406752586365\n",
      "edge_vol 330.38123\n",
      "Epoch: 0164 train_loss= 0.77602 val_loss= 1.60826 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "time  63.070281744003296\n",
      "edge_vol 315.8506\n",
      "Epoch: 0165 train_loss= 0.77335 val_loss= 1.60640 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.69400 test_acc= 0.66000\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "##PTDNEt\n",
    "\n",
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Settings\n",
    "args.dataset=dataset_name\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "import time\n",
    "begin = time.time()\n",
    "\n",
    "model = PTDNetGCN(input_dim=features.shape[1], output_dim=y_train.shape[1])\n",
    "model.set_fea_adj(np.array(range(adj.shape[0])), features_tensor, adj_tensor)\n",
    "\n",
    "best_epoch = 0\n",
    "curr_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    temperature = max(0.05,args.init_temperature * pow(args.temperature_decay, epoch))\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = []\n",
    "        for l in range(args.outL):\n",
    "            output = model.call(temperature,training=True)\n",
    "            preds.append(tf.expand_dims(output,0))\n",
    "        all_preds = tf.concat(preds,axis=0)\n",
    "        mean_preds = tf.reduce_mean(preds,axis=0)\n",
    "        consistency_loss = tf.nn.l2_loss(mean_preds-all_preds)\n",
    "\n",
    "        cross_loss = masked_softmax_cross_entropy(mean_preds, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        lossl0 = model.lossl0(temperature)\n",
    "        #nuclear = model.my_nuclear()\n",
    "        nuclear = model.nuclear()\n",
    "        loss = cross_loss + args.weight_decay*lossL2 + args.lambda1*lossl0 + args.lambda3*nuclear + args.coff_consis*consistency_loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call(None, training=False)\n",
    "    edges_volumn = tf.reduce_sum(model.maskes[0])\n",
    "    print('edge_vol',edges_volumn.numpy())\n",
    "\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_epoch = epoch\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        if val_acc>best_val_acc_trail:\n",
    "            best_test_acc = test_acc\n",
    "            best_val_acc_trail = val_acc\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\"best_val_acc_trail=\", \"{:.5f}\".format(best_val_acc_trail),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))\n",
    "\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    print('time ',(end-begin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_vol 4697.9355\n",
      "Epoch: 0001 train_loss= 1.79149 val_loss= 1.79074 train_acc= 0.54167 val_acc= 0.33600 best_val_acc_trail= 0.33600 test_acc= 0.31900\n",
      "time  1.8296616077423096\n",
      "edge_vol 4691.7505\n",
      "Epoch: 0002 train_loss= 1.78958 val_loss= 1.79010 train_acc= 0.77500 val_acc= 0.42200 best_val_acc_trail= 0.42200 test_acc= 0.40300\n",
      "time  2.402817726135254\n",
      "edge_vol 4684.5283\n",
      "Epoch: 0003 train_loss= 1.78762 val_loss= 1.78941 train_acc= 0.89167 val_acc= 0.49400 best_val_acc_trail= 0.49400 test_acc= 0.48100\n",
      "time  2.9739162921905518\n",
      "edge_vol 4676.6523\n",
      "Epoch: 0004 train_loss= 1.78561 val_loss= 1.78868 train_acc= 0.91667 val_acc= 0.53000 best_val_acc_trail= 0.53000 test_acc= 0.53700\n",
      "time  3.6060988903045654\n",
      "edge_vol 4668.3623\n",
      "Epoch: 0005 train_loss= 1.78344 val_loss= 1.78789 train_acc= 0.93333 val_acc= 0.57000 best_val_acc_trail= 0.57000 test_acc= 0.56600\n",
      "time  4.189894199371338\n",
      "edge_vol 4659.7207\n",
      "Epoch: 0006 train_loss= 1.78136 val_loss= 1.78702 train_acc= 0.93333 val_acc= 0.61400 best_val_acc_trail= 0.61400 test_acc= 0.58700\n",
      "time  4.753285646438599\n",
      "edge_vol 4650.7666\n",
      "Epoch: 0007 train_loss= 1.77898 val_loss= 1.78609 train_acc= 0.93333 val_acc= 0.62600 best_val_acc_trail= 0.62600 test_acc= 0.60700\n",
      "time  5.307632684707642\n",
      "edge_vol 4641.589\n",
      "Epoch: 0008 train_loss= 1.77649 val_loss= 1.78506 train_acc= 0.93333 val_acc= 0.64000 best_val_acc_trail= 0.64000 test_acc= 0.62600\n",
      "time  5.902633190155029\n",
      "edge_vol 4632.25\n",
      "Epoch: 0009 train_loss= 1.77402 val_loss= 1.78397 train_acc= 0.93333 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63700\n",
      "time  6.557483673095703\n",
      "edge_vol 4622.747\n",
      "Epoch: 0010 train_loss= 1.77129 val_loss= 1.78280 train_acc= 0.93333 val_acc= 0.65600 best_val_acc_trail= 0.65600 test_acc= 0.64300\n",
      "time  7.144759654998779\n",
      "edge_vol 4613.112\n",
      "Epoch: 0011 train_loss= 1.76845 val_loss= 1.78156 train_acc= 0.94167 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.64400\n",
      "time  7.7045183181762695\n",
      "edge_vol 4603.416\n",
      "Epoch: 0012 train_loss= 1.76513 val_loss= 1.78025 train_acc= 0.94167 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  8.277498960494995\n",
      "edge_vol 4593.584\n",
      "Epoch: 0013 train_loss= 1.76201 val_loss= 1.77889 train_acc= 0.95000 val_acc= 0.66400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  8.799587488174438\n",
      "edge_vol 4583.618\n",
      "Epoch: 0014 train_loss= 1.75849 val_loss= 1.77747 train_acc= 0.95000 val_acc= 0.66800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  9.28831934928894\n",
      "edge_vol 4573.521\n",
      "Epoch: 0015 train_loss= 1.75521 val_loss= 1.77600 train_acc= 0.95000 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  9.838963031768799\n",
      "edge_vol 4563.287\n",
      "Epoch: 0016 train_loss= 1.75154 val_loss= 1.77448 train_acc= 0.95000 val_acc= 0.66800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  10.28868556022644\n",
      "edge_vol 4552.9272\n",
      "Epoch: 0017 train_loss= 1.74736 val_loss= 1.77290 train_acc= 0.95000 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  10.750949144363403\n",
      "edge_vol 4542.4316\n",
      "Epoch: 0018 train_loss= 1.74370 val_loss= 1.77129 train_acc= 0.95000 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  11.22071385383606\n",
      "edge_vol 4531.8057\n",
      "Epoch: 0019 train_loss= 1.73985 val_loss= 1.76963 train_acc= 0.95000 val_acc= 0.66800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  11.713159084320068\n",
      "edge_vol 4521.0527\n",
      "Epoch: 0020 train_loss= 1.73577 val_loss= 1.76793 train_acc= 0.94167 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  12.160813808441162\n",
      "edge_vol 4510.176\n",
      "Epoch: 0021 train_loss= 1.73071 val_loss= 1.76618 train_acc= 0.94167 val_acc= 0.66400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  12.623121976852417\n",
      "edge_vol 4499.1685\n",
      "Epoch: 0022 train_loss= 1.72606 val_loss= 1.76438 train_acc= 0.94167 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  13.109446048736572\n",
      "edge_vol 4488.0977\n",
      "Epoch: 0023 train_loss= 1.72113 val_loss= 1.76254 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  13.576713800430298\n",
      "edge_vol 4476.902\n",
      "Epoch: 0024 train_loss= 1.71678 val_loss= 1.76067 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  14.065889120101929\n",
      "edge_vol 4465.5786\n",
      "Epoch: 0025 train_loss= 1.71121 val_loss= 1.75877 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  14.524750232696533\n",
      "edge_vol 4454.0938\n",
      "Epoch: 0026 train_loss= 1.70612 val_loss= 1.75683 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  14.996119022369385\n",
      "edge_vol 4442.452\n",
      "Epoch: 0027 train_loss= 1.70058 val_loss= 1.75486 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  15.481451272964478\n",
      "edge_vol 4430.6895\n",
      "Epoch: 0028 train_loss= 1.69609 val_loss= 1.75286 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  15.942863941192627\n",
      "edge_vol 4418.827\n",
      "Epoch: 0029 train_loss= 1.69113 val_loss= 1.75082 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  16.4249107837677\n",
      "edge_vol 4406.8457\n",
      "Epoch: 0030 train_loss= 1.68544 val_loss= 1.74875 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  16.889904737472534\n",
      "edge_vol 4394.754\n",
      "Epoch: 0031 train_loss= 1.67968 val_loss= 1.74666 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  17.356910943984985\n",
      "edge_vol 4382.5312\n",
      "Epoch: 0032 train_loss= 1.67322 val_loss= 1.74453 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  17.830326080322266\n",
      "edge_vol 4370.1885\n",
      "Epoch: 0033 train_loss= 1.66776 val_loss= 1.74237 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  18.316922903060913\n",
      "edge_vol 4357.7114\n",
      "Epoch: 0034 train_loss= 1.66138 val_loss= 1.74016 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  18.778656721115112\n",
      "edge_vol 4345.065\n",
      "Epoch: 0035 train_loss= 1.65628 val_loss= 1.73791 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  19.268056392669678\n",
      "edge_vol 4332.2383\n",
      "Epoch: 0036 train_loss= 1.64973 val_loss= 1.73562 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  19.75553059577942\n",
      "edge_vol 4319.2505\n",
      "Epoch: 0037 train_loss= 1.64359 val_loss= 1.73330 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  20.20961880683899\n",
      "edge_vol 4306.0127\n",
      "Epoch: 0038 train_loss= 1.63791 val_loss= 1.73094 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  20.644366025924683\n",
      "edge_vol 4292.5425\n",
      "Epoch: 0039 train_loss= 1.63103 val_loss= 1.72854 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  21.088847875595093\n",
      "edge_vol 4278.838\n",
      "Epoch: 0040 train_loss= 1.62490 val_loss= 1.72612 train_acc= 0.95000 val_acc= 0.65000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  21.52895712852478\n",
      "edge_vol 4264.9424\n",
      "Epoch: 0041 train_loss= 1.61709 val_loss= 1.72364 train_acc= 0.95000 val_acc= 0.65000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  22.027453660964966\n",
      "edge_vol 4250.79\n",
      "Epoch: 0042 train_loss= 1.61027 val_loss= 1.72112 train_acc= 0.95000 val_acc= 0.65200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  22.486109018325806\n",
      "edge_vol 4236.334\n",
      "Epoch: 0043 train_loss= 1.60409 val_loss= 1.71857 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  22.943130493164062\n",
      "edge_vol 4221.6562\n",
      "Epoch: 0044 train_loss= 1.59834 val_loss= 1.71597 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  23.392371654510498\n",
      "edge_vol 4206.714\n",
      "Epoch: 0045 train_loss= 1.59071 val_loss= 1.71336 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  23.84866952896118\n",
      "edge_vol 4191.4717\n",
      "Epoch: 0046 train_loss= 1.58384 val_loss= 1.71070 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  24.303267002105713\n",
      "edge_vol 4175.842\n",
      "Epoch: 0047 train_loss= 1.57649 val_loss= 1.70799 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  24.757490396499634\n",
      "edge_vol 4159.926\n",
      "Epoch: 0048 train_loss= 1.56973 val_loss= 1.70523 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  25.256085634231567\n",
      "edge_vol 4143.734\n",
      "Epoch: 0049 train_loss= 1.56078 val_loss= 1.70243 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  25.725520849227905\n",
      "edge_vol 4127.2217\n",
      "Epoch: 0050 train_loss= 1.55332 val_loss= 1.69960 train_acc= 0.95000 val_acc= 0.65400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  26.257138967514038\n",
      "edge_vol 4110.2573\n",
      "Epoch: 0051 train_loss= 1.54500 val_loss= 1.69668 train_acc= 0.95000 val_acc= 0.65600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  26.725027561187744\n",
      "edge_vol 4092.976\n",
      "Epoch: 0052 train_loss= 1.54133 val_loss= 1.69375 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  27.19241952896118\n",
      "edge_vol 4075.2383\n",
      "Epoch: 0053 train_loss= 1.53141 val_loss= 1.69080 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  27.698558568954468\n",
      "edge_vol 4057.0803\n",
      "Epoch: 0054 train_loss= 1.52463 val_loss= 1.68783 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  28.207006216049194\n",
      "edge_vol 4038.4104\n",
      "Epoch: 0055 train_loss= 1.51543 val_loss= 1.68482 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  28.68522000312805\n",
      "edge_vol 4019.4402\n",
      "Epoch: 0056 train_loss= 1.50978 val_loss= 1.68181 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  29.1522798538208\n",
      "edge_vol 3999.845\n",
      "Epoch: 0057 train_loss= 1.50169 val_loss= 1.67874 train_acc= 0.95000 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  29.618914127349854\n",
      "edge_vol 3979.785\n",
      "Epoch: 0058 train_loss= 1.49330 val_loss= 1.67563 train_acc= 0.95000 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  30.081599473953247\n",
      "edge_vol 3959.3088\n",
      "Epoch: 0059 train_loss= 1.48401 val_loss= 1.67247 train_acc= 0.95000 val_acc= 0.66200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  30.57235860824585\n",
      "edge_vol 3938.3716\n",
      "Epoch: 0060 train_loss= 1.47457 val_loss= 1.66926 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  31.043017864227295\n",
      "edge_vol 3917.053\n",
      "Epoch: 0061 train_loss= 1.46736 val_loss= 1.66600 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  31.508321523666382\n",
      "edge_vol 3895.2034\n",
      "Epoch: 0062 train_loss= 1.45832 val_loss= 1.66271 train_acc= 0.95000 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  32.012890338897705\n",
      "edge_vol 3872.9043\n",
      "Epoch: 0063 train_loss= 1.44927 val_loss= 1.65938 train_acc= 0.95833 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  32.4901864528656\n",
      "edge_vol 3850.1875\n",
      "Epoch: 0064 train_loss= 1.44106 val_loss= 1.65599 train_acc= 0.95833 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  32.947592973709106\n",
      "edge_vol 3826.7993\n",
      "Epoch: 0065 train_loss= 1.43654 val_loss= 1.65259 train_acc= 0.95833 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  33.40549564361572\n",
      "edge_vol 3803.0388\n",
      "Epoch: 0066 train_loss= 1.42232 val_loss= 1.64910 train_acc= 0.95833 val_acc= 0.66000 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  33.868866205215454\n",
      "edge_vol 3778.842\n",
      "Epoch: 0067 train_loss= 1.41288 val_loss= 1.64554 train_acc= 0.95833 val_acc= 0.66200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  34.33193516731262\n",
      "edge_vol 3754.2192\n",
      "Epoch: 0068 train_loss= 1.40752 val_loss= 1.64197 train_acc= 0.95833 val_acc= 0.66200 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  34.865039587020874\n",
      "edge_vol 3729.059\n",
      "Epoch: 0069 train_loss= 1.39421 val_loss= 1.63838 train_acc= 0.95833 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  35.34933853149414\n",
      "edge_vol 3703.6355\n",
      "Epoch: 0070 train_loss= 1.39268 val_loss= 1.63479 train_acc= 0.95833 val_acc= 0.66400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  35.85436391830444\n",
      "edge_vol 3677.544\n",
      "Epoch: 0071 train_loss= 1.37561 val_loss= 1.63123 train_acc= 0.95833 val_acc= 0.66400 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  36.31459665298462\n",
      "edge_vol 3651.121\n",
      "Epoch: 0072 train_loss= 1.37018 val_loss= 1.62763 train_acc= 0.95833 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  36.831260681152344\n",
      "edge_vol 3624.1416\n",
      "Epoch: 0073 train_loss= 1.35984 val_loss= 1.62395 train_acc= 0.95833 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  37.3175835609436\n",
      "edge_vol 3596.6973\n",
      "Epoch: 0074 train_loss= 1.34657 val_loss= 1.62023 train_acc= 0.96667 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  37.79078817367554\n",
      "edge_vol 3569.06\n",
      "Epoch: 0075 train_loss= 1.34750 val_loss= 1.61651 train_acc= 0.96667 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  38.250338315963745\n",
      "edge_vol 3541.041\n",
      "Epoch: 0076 train_loss= 1.32539 val_loss= 1.61274 train_acc= 0.96667 val_acc= 0.66600 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  38.69337463378906\n",
      "edge_vol 3512.7922\n",
      "Epoch: 0077 train_loss= 1.32060 val_loss= 1.60888 train_acc= 0.96667 val_acc= 0.66800 best_val_acc_trail= 0.67000 test_acc= 0.64500\n",
      "time  39.15900206565857\n",
      "edge_vol 3484.0417\n",
      "Epoch: 0078 train_loss= 1.31239 val_loss= 1.60501 train_acc= 0.96667 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.64700\n",
      "time  39.636820793151855\n",
      "edge_vol 3454.7732\n",
      "Epoch: 0079 train_loss= 1.30578 val_loss= 1.60123 train_acc= 0.95833 val_acc= 0.67200 best_val_acc_trail= 0.67200 test_acc= 0.64600\n",
      "time  40.10779166221619\n",
      "edge_vol 3424.7856\n",
      "Epoch: 0080 train_loss= 1.28811 val_loss= 1.59741 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.64700\n",
      "time  40.60673975944519\n",
      "edge_vol 3394.1472\n",
      "Epoch: 0081 train_loss= 1.27772 val_loss= 1.59359 train_acc= 0.96667 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.64700\n",
      "time  41.11217164993286\n",
      "edge_vol 3363.1445\n",
      "Epoch: 0082 train_loss= 1.26608 val_loss= 1.58970 train_acc= 0.96667 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.64700\n",
      "time  41.5806200504303\n",
      "edge_vol 3331.5718\n",
      "Epoch: 0083 train_loss= 1.26011 val_loss= 1.58579 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.67400 test_acc= 0.64700\n",
      "time  42.02222537994385\n",
      "edge_vol 3299.6074\n",
      "Epoch: 0084 train_loss= 1.25200 val_loss= 1.58180 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.64700\n",
      "time  42.46726155281067\n",
      "edge_vol 3267.0977\n",
      "Epoch: 0085 train_loss= 1.23967 val_loss= 1.57778 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  42.95073914527893\n",
      "edge_vol 3234.268\n",
      "Epoch: 0086 train_loss= 1.22715 val_loss= 1.57371 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  43.41006278991699\n",
      "edge_vol 3200.947\n",
      "Epoch: 0087 train_loss= 1.21910 val_loss= 1.56964 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  43.880950689315796\n",
      "edge_vol 3167.609\n",
      "Epoch: 0088 train_loss= 1.21315 val_loss= 1.56560 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  44.34735441207886\n",
      "edge_vol 3133.7754\n",
      "Epoch: 0089 train_loss= 1.18696 val_loss= 1.56149 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  44.80966854095459\n",
      "edge_vol 3099.0938\n",
      "Epoch: 0090 train_loss= 1.17711 val_loss= 1.55733 train_acc= 0.97500 val_acc= 0.67000 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  45.267812967300415\n",
      "edge_vol 3064.1182\n",
      "Epoch: 0091 train_loss= 1.17046 val_loss= 1.55314 train_acc= 0.98333 val_acc= 0.67000 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  45.72874855995178\n",
      "edge_vol 3028.577\n",
      "Epoch: 0092 train_loss= 1.15731 val_loss= 1.54896 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  46.19031381607056\n",
      "edge_vol 2992.2764\n",
      "Epoch: 0093 train_loss= 1.14743 val_loss= 1.54474 train_acc= 0.99167 val_acc= 0.67400 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  46.653276205062866\n",
      "edge_vol 2955.2283\n",
      "Epoch: 0094 train_loss= 1.13283 val_loss= 1.54051 train_acc= 0.99167 val_acc= 0.67200 best_val_acc_trail= 0.67600 test_acc= 0.64900\n",
      "time  47.124494552612305\n",
      "edge_vol 2917.6704\n",
      "Epoch: 0095 train_loss= 1.11881 val_loss= 1.53620 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.65100\n",
      "time  47.59350848197937\n",
      "edge_vol 2879.599\n",
      "Epoch: 0096 train_loss= 1.10486 val_loss= 1.53187 train_acc= 0.99167 val_acc= 0.67800 best_val_acc_trail= 0.67800 test_acc= 0.65200\n",
      "time  48.06007099151611\n",
      "edge_vol 2840.9111\n",
      "Epoch: 0097 train_loss= 1.08953 val_loss= 1.52751 train_acc= 0.99167 val_acc= 0.67800 best_val_acc_trail= 0.67800 test_acc= 0.65200\n",
      "time  48.56127429008484\n",
      "edge_vol 2801.7446\n",
      "Epoch: 0098 train_loss= 1.08124 val_loss= 1.52311 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.65200\n",
      "time  49.04332995414734\n",
      "edge_vol 2761.9595\n",
      "Epoch: 0099 train_loss= 1.07094 val_loss= 1.51864 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.65200\n",
      "time  49.536553621292114\n",
      "edge_vol 2721.9487\n",
      "Epoch: 0100 train_loss= 1.06611 val_loss= 1.51422 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68000 test_acc= 0.65700\n",
      "time  50.007901191711426\n",
      "edge_vol 2681.5815\n",
      "Epoch: 0101 train_loss= 1.04480 val_loss= 1.50977 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  50.46578335762024\n",
      "edge_vol 2640.6323\n",
      "Epoch: 0102 train_loss= 1.03153 val_loss= 1.50532 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  50.930861949920654\n",
      "edge_vol 2599.232\n",
      "Epoch: 0103 train_loss= 1.01923 val_loss= 1.50100 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  51.40452003479004\n",
      "edge_vol 2557.627\n",
      "Epoch: 0104 train_loss= 1.00875 val_loss= 1.49669 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  51.87603974342346\n",
      "edge_vol 2515.7876\n",
      "Epoch: 0105 train_loss= 0.97541 val_loss= 1.49248 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  52.341289043426514\n",
      "edge_vol 2473.7063\n",
      "Epoch: 0106 train_loss= 0.97417 val_loss= 1.48827 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  52.79684019088745\n",
      "edge_vol 2431.0005\n",
      "Epoch: 0107 train_loss= 0.95841 val_loss= 1.48406 train_acc= 0.99167 val_acc= 0.68000 best_val_acc_trail= 0.68200 test_acc= 0.65700\n",
      "time  53.25135612487793\n",
      "edge_vol 2388.0742\n",
      "Epoch: 0108 train_loss= 0.94264 val_loss= 1.47992 train_acc= 0.99167 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  53.759310483932495\n",
      "edge_vol 2345.0708\n",
      "Epoch: 0109 train_loss= 0.91391 val_loss= 1.47581 train_acc= 0.99167 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  54.29804968833923\n",
      "edge_vol 2301.5845\n",
      "Epoch: 0110 train_loss= 0.92671 val_loss= 1.47171 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  54.782660245895386\n",
      "edge_vol 2257.5723\n",
      "Epoch: 0111 train_loss= 0.89032 val_loss= 1.46769 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  55.246023416519165\n",
      "edge_vol 2213.7227\n",
      "Epoch: 0112 train_loss= 0.88815 val_loss= 1.46368 train_acc= 0.99167 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  55.7175817489624\n",
      "edge_vol 2169.6145\n",
      "Epoch: 0113 train_loss= 0.85381 val_loss= 1.45965 train_acc= 0.99167 val_acc= 0.67200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  56.18759775161743\n",
      "edge_vol 2125.2861\n",
      "Epoch: 0114 train_loss= 0.84942 val_loss= 1.45575 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  56.67051029205322\n",
      "edge_vol 2080.9731\n",
      "Epoch: 0115 train_loss= 0.83255 val_loss= 1.45197 train_acc= 0.99167 val_acc= 0.66800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  57.17006850242615\n",
      "edge_vol 2036.5947\n",
      "Epoch: 0116 train_loss= 0.80158 val_loss= 1.44816 train_acc= 0.99167 val_acc= 0.66600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  57.66682291030884\n",
      "edge_vol 1992.2456\n",
      "Epoch: 0117 train_loss= 0.80915 val_loss= 1.44437 train_acc= 0.99167 val_acc= 0.66600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  58.15187931060791\n",
      "edge_vol 1948.2036\n",
      "Epoch: 0118 train_loss= 0.78332 val_loss= 1.44072 train_acc= 0.99167 val_acc= 0.66600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  58.651451587677\n",
      "edge_vol 1904.5725\n",
      "Epoch: 0119 train_loss= 0.76581 val_loss= 1.43714 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  59.12868547439575\n",
      "edge_vol 1861.4048\n",
      "Epoch: 0120 train_loss= 0.75746 val_loss= 1.43364 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  59.600101470947266\n",
      "edge_vol 1818.6763\n",
      "Epoch: 0121 train_loss= 0.74013 val_loss= 1.43030 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  60.07891893386841\n",
      "edge_vol 1776.2495\n",
      "Epoch: 0122 train_loss= 0.70866 val_loss= 1.42705 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  60.54826045036316\n",
      "edge_vol 1734.0608\n",
      "Epoch: 0123 train_loss= 0.71017 val_loss= 1.42385 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  61.01027059555054\n",
      "edge_vol 1692.5554\n",
      "Epoch: 0124 train_loss= 0.69434 val_loss= 1.42067 train_acc= 1.00000 val_acc= 0.65400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  61.47729516029358\n",
      "edge_vol 1651.7495\n",
      "Epoch: 0125 train_loss= 0.66287 val_loss= 1.41746 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  61.95539379119873\n",
      "edge_vol 1611.122\n",
      "Epoch: 0126 train_loss= 0.66632 val_loss= 1.41420 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  62.42112350463867\n",
      "edge_vol 1570.9496\n",
      "Epoch: 0127 train_loss= 0.64491 val_loss= 1.41100 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  62.882317543029785\n",
      "edge_vol 1531.2665\n",
      "Epoch: 0128 train_loss= 0.63500 val_loss= 1.40780 train_acc= 1.00000 val_acc= 0.64600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  63.34941911697388\n",
      "edge_vol 1491.6973\n",
      "Epoch: 0129 train_loss= 0.63989 val_loss= 1.40465 train_acc= 1.00000 val_acc= 0.64600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  63.82168364524841\n",
      "edge_vol 1452.4673\n",
      "Epoch: 0130 train_loss= 0.62512 val_loss= 1.40162 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  64.28512835502625\n",
      "edge_vol 1413.9987\n",
      "Epoch: 0131 train_loss= 0.60174 val_loss= 1.39863 train_acc= 1.00000 val_acc= 0.64400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  64.78519034385681\n",
      "edge_vol 1376.2002\n",
      "Epoch: 0132 train_loss= 0.58835 val_loss= 1.39572 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  65.2679615020752\n",
      "edge_vol 1339.3964\n",
      "Epoch: 0133 train_loss= 0.57939 val_loss= 1.39288 train_acc= 1.00000 val_acc= 0.64400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  65.76554131507874\n",
      "edge_vol 1303.1963\n",
      "Epoch: 0134 train_loss= 0.54720 val_loss= 1.39011 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  66.27632212638855\n",
      "edge_vol 1267.639\n",
      "Epoch: 0135 train_loss= 0.55503 val_loss= 1.38767 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  66.73139071464539\n",
      "edge_vol 1232.7578\n",
      "Epoch: 0136 train_loss= 0.53566 val_loss= 1.38537 train_acc= 1.00000 val_acc= 0.64600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  67.18947505950928\n",
      "edge_vol 1198.3955\n",
      "Epoch: 0137 train_loss= 0.53082 val_loss= 1.38307 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  67.6652524471283\n",
      "edge_vol 1164.7498\n",
      "Epoch: 0138 train_loss= 0.52193 val_loss= 1.38094 train_acc= 1.00000 val_acc= 0.64400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  68.15584754943848\n",
      "edge_vol 1132.4099\n",
      "Epoch: 0139 train_loss= 0.49921 val_loss= 1.37905 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  68.61957812309265\n",
      "edge_vol 1100.8157\n",
      "Epoch: 0140 train_loss= 0.47703 val_loss= 1.37731 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  69.1605269908905\n",
      "edge_vol 1070.1948\n",
      "Epoch: 0141 train_loss= 0.49479 val_loss= 1.37560 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  69.62984919548035\n",
      "edge_vol 1040.8448\n",
      "Epoch: 0142 train_loss= 0.46785 val_loss= 1.37395 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  70.099924325943\n",
      "edge_vol 1012.0929\n",
      "Epoch: 0143 train_loss= 0.47006 val_loss= 1.37227 train_acc= 1.00000 val_acc= 0.64200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  70.54450821876526\n",
      "edge_vol 984.22864\n",
      "Epoch: 0144 train_loss= 0.46610 val_loss= 1.37051 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  71.00012040138245\n",
      "edge_vol 957.35425\n",
      "Epoch: 0145 train_loss= 0.44363 val_loss= 1.36884 train_acc= 1.00000 val_acc= 0.64400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  71.507239818573\n",
      "edge_vol 930.9707\n",
      "Epoch: 0146 train_loss= 0.44248 val_loss= 1.36717 train_acc= 1.00000 val_acc= 0.64400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  71.97473621368408\n",
      "edge_vol 905.1052\n",
      "Epoch: 0147 train_loss= 0.45368 val_loss= 1.36549 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  72.45808815956116\n",
      "edge_vol 880.10114\n",
      "Epoch: 0148 train_loss= 0.42331 val_loss= 1.36398 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  72.94101428985596\n",
      "edge_vol 855.8679\n",
      "Epoch: 0149 train_loss= 0.42488 val_loss= 1.36245 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  73.42322754859924\n",
      "edge_vol 832.6415\n",
      "Epoch: 0150 train_loss= 0.41175 val_loss= 1.36108 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  73.9968523979187\n",
      "edge_vol 810.2333\n",
      "Epoch: 0151 train_loss= 0.40585 val_loss= 1.35973 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  74.46419620513916\n",
      "edge_vol 788.5513\n",
      "Epoch: 0152 train_loss= 0.39813 val_loss= 1.35836 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  74.92386674880981\n",
      "edge_vol 767.81116\n",
      "Epoch: 0153 train_loss= 0.39451 val_loss= 1.35700 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  75.40097236633301\n",
      "edge_vol 747.36194\n",
      "Epoch: 0154 train_loss= 0.38449 val_loss= 1.35579 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  75.86550307273865\n",
      "edge_vol 727.94794\n",
      "Epoch: 0155 train_loss= 0.37917 val_loss= 1.35468 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  76.39500975608826\n",
      "edge_vol 709.1614\n",
      "Epoch: 0156 train_loss= 0.38282 val_loss= 1.35370 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  76.87670469284058\n",
      "edge_vol 691.1265\n",
      "Epoch: 0157 train_loss= 0.35598 val_loss= 1.35284 train_acc= 1.00000 val_acc= 0.64000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  77.44871950149536\n",
      "edge_vol 673.7295\n",
      "Epoch: 0158 train_loss= 0.36902 val_loss= 1.35219 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  77.93356776237488\n",
      "edge_vol 656.5304\n",
      "Epoch: 0159 train_loss= 0.36188 val_loss= 1.35150 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  78.39043188095093\n",
      "edge_vol 639.7899\n",
      "Epoch: 0160 train_loss= 0.35935 val_loss= 1.35072 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  78.85147905349731\n",
      "edge_vol 623.68396\n",
      "Epoch: 0161 train_loss= 0.34759 val_loss= 1.34985 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  79.35940289497375\n",
      "edge_vol 608.3158\n",
      "Epoch: 0162 train_loss= 0.34528 val_loss= 1.34898 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  79.84300637245178\n",
      "edge_vol 593.5731\n",
      "Epoch: 0163 train_loss= 0.33547 val_loss= 1.34819 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  80.36273097991943\n",
      "edge_vol 579.43835\n",
      "Epoch: 0164 train_loss= 0.33121 val_loss= 1.34748 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  80.8541464805603\n",
      "edge_vol 565.57153\n",
      "Epoch: 0165 train_loss= 0.32462 val_loss= 1.34688 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  81.31920027732849\n",
      "edge_vol 552.1644\n",
      "Epoch: 0166 train_loss= 0.32604 val_loss= 1.34638 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  81.81366562843323\n",
      "edge_vol 539.2228\n",
      "Epoch: 0167 train_loss= 0.31376 val_loss= 1.34594 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  82.28452277183533\n",
      "edge_vol 526.64276\n",
      "Epoch: 0168 train_loss= 0.31980 val_loss= 1.34553 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  82.77623629570007\n",
      "edge_vol 514.3352\n",
      "Epoch: 0169 train_loss= 0.32048 val_loss= 1.34519 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  83.24573183059692\n",
      "edge_vol 502.48245\n",
      "Epoch: 0170 train_loss= 0.31350 val_loss= 1.34497 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  83.70827269554138\n",
      "edge_vol 490.9917\n",
      "Epoch: 0171 train_loss= 0.30288 val_loss= 1.34478 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  84.19688582420349\n",
      "edge_vol 480.11206\n",
      "Epoch: 0172 train_loss= 0.30486 val_loss= 1.34453 train_acc= 1.00000 val_acc= 0.63800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  84.67086839675903\n",
      "edge_vol 469.75162\n",
      "Epoch: 0173 train_loss= 0.29582 val_loss= 1.34443 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  85.12264752388\n",
      "edge_vol 459.7348\n",
      "Epoch: 0174 train_loss= 0.30104 val_loss= 1.34422 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  85.5645272731781\n",
      "edge_vol 450.0224\n",
      "Epoch: 0175 train_loss= 0.28803 val_loss= 1.34393 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  86.01213431358337\n",
      "edge_vol 440.77338\n",
      "Epoch: 0176 train_loss= 0.27912 val_loss= 1.34376 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  86.49512434005737\n",
      "edge_vol 431.67047\n",
      "Epoch: 0177 train_loss= 0.29260 val_loss= 1.34351 train_acc= 1.00000 val_acc= 0.63600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  86.95237922668457\n",
      "edge_vol 422.8731\n",
      "Epoch: 0178 train_loss= 0.29482 val_loss= 1.34331 train_acc= 1.00000 val_acc= 0.63400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  87.42049264907837\n",
      "edge_vol 414.46353\n",
      "Epoch: 0179 train_loss= 0.28748 val_loss= 1.34327 train_acc= 1.00000 val_acc= 0.63200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  87.89268684387207\n",
      "edge_vol 406.36157\n",
      "Epoch: 0180 train_loss= 0.27848 val_loss= 1.34333 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  88.35469794273376\n",
      "edge_vol 398.546\n",
      "Epoch: 0181 train_loss= 0.27222 val_loss= 1.34337 train_acc= 1.00000 val_acc= 0.63000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  88.81825113296509\n",
      "edge_vol 390.72217\n",
      "Epoch: 0182 train_loss= 0.28098 val_loss= 1.34337 train_acc= 1.00000 val_acc= 0.62800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  89.28431296348572\n",
      "edge_vol 383.111\n",
      "Epoch: 0183 train_loss= 0.27553 val_loss= 1.34334 train_acc= 1.00000 val_acc= 0.62600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  89.74869894981384\n",
      "edge_vol 375.83438\n",
      "Epoch: 0184 train_loss= 0.26201 val_loss= 1.34317 train_acc= 1.00000 val_acc= 0.62400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  90.21899747848511\n",
      "edge_vol 368.8963\n",
      "Epoch: 0185 train_loss= 0.27429 val_loss= 1.34301 train_acc= 1.00000 val_acc= 0.62400 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  90.71911716461182\n",
      "edge_vol 362.11304\n",
      "Epoch: 0186 train_loss= 0.26822 val_loss= 1.34283 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  91.2489652633667\n",
      "edge_vol 355.71237\n",
      "Epoch: 0187 train_loss= 0.25415 val_loss= 1.34258 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  91.74178218841553\n",
      "edge_vol 349.55347\n",
      "Epoch: 0188 train_loss= 0.26054 val_loss= 1.34223 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  92.22418117523193\n",
      "edge_vol 343.50623\n",
      "Epoch: 0189 train_loss= 0.25746 val_loss= 1.34203 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  92.70011377334595\n",
      "edge_vol 337.5403\n",
      "Epoch: 0190 train_loss= 0.25276 val_loss= 1.34185 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  93.16421222686768\n",
      "edge_vol 331.53473\n",
      "Epoch: 0191 train_loss= 0.26450 val_loss= 1.34172 train_acc= 1.00000 val_acc= 0.62200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  93.64364457130432\n",
      "edge_vol 325.6333\n",
      "Epoch: 0192 train_loss= 0.26073 val_loss= 1.34177 train_acc= 1.00000 val_acc= 0.62200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  94.11088442802429\n",
      "edge_vol 320.1356\n",
      "Epoch: 0193 train_loss= 0.23722 val_loss= 1.34186 train_acc= 1.00000 val_acc= 0.62200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  94.62378978729248\n",
      "edge_vol 314.86725\n",
      "Epoch: 0194 train_loss= 0.24226 val_loss= 1.34192 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  95.08716869354248\n",
      "edge_vol 309.83032\n",
      "Epoch: 0195 train_loss= 0.24878 val_loss= 1.34184 train_acc= 1.00000 val_acc= 0.62200 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  95.59934616088867\n",
      "edge_vol 304.85977\n",
      "Epoch: 0196 train_loss= 0.24444 val_loss= 1.34178 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  96.06790089607239\n",
      "edge_vol 300.1106\n",
      "Epoch: 0197 train_loss= 0.24941 val_loss= 1.34156 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  96.53191065788269\n",
      "edge_vol 295.26465\n",
      "Epoch: 0198 train_loss= 0.24882 val_loss= 1.34120 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  97.06351923942566\n",
      "edge_vol 290.42548\n",
      "Epoch: 0199 train_loss= 0.24491 val_loss= 1.34082 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  97.53089714050293\n",
      "edge_vol 285.63974\n",
      "Epoch: 0200 train_loss= 0.24114 val_loss= 1.34057 train_acc= 1.00000 val_acc= 0.62000 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  98.00570034980774\n",
      "edge_vol 281.24396\n",
      "Epoch: 0201 train_loss= 0.22799 val_loss= 1.34032 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  98.47389769554138\n",
      "edge_vol 276.8659\n",
      "Epoch: 0202 train_loss= 0.23962 val_loss= 1.34016 train_acc= 1.00000 val_acc= 0.61600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  98.92388677597046\n",
      "edge_vol 272.68927\n",
      "Epoch: 0203 train_loss= 0.23987 val_loss= 1.33983 train_acc= 1.00000 val_acc= 0.61600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  99.3703887462616\n",
      "edge_vol 268.59332\n",
      "Epoch: 0204 train_loss= 0.23553 val_loss= 1.33941 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  99.8245210647583\n",
      "edge_vol 264.74747\n",
      "Epoch: 0205 train_loss= 0.22766 val_loss= 1.33911 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  100.27863383293152\n",
      "edge_vol 261.08276\n",
      "Epoch: 0206 train_loss= 0.23236 val_loss= 1.33884 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  100.77173209190369\n",
      "edge_vol 257.5681\n",
      "Epoch: 0207 train_loss= 0.22828 val_loss= 1.33860 train_acc= 1.00000 val_acc= 0.61800 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  101.23749279975891\n",
      "edge_vol 254.05655\n",
      "Epoch: 0208 train_loss= 0.22536 val_loss= 1.33844 train_acc= 1.00000 val_acc= 0.61600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "time  101.70376992225647\n",
      "edge_vol 250.64572\n",
      "Epoch: 0209 train_loss= 0.22763 val_loss= 1.33817 train_acc= 1.00000 val_acc= 0.61600 best_val_acc_trail= 0.68400 test_acc= 0.66100\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "## 我们的模型\n",
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Settings\n",
    "args.dataset=dataset_name\n",
    "args.dropout=0.0\n",
    "\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "import time\n",
    "begin = time.time()\n",
    "\n",
    "model = PTDNetGCN(input_dim=features.shape[1], output_dim=y_train.shape[1])\n",
    "model.set_fea_adj(np.array(range(adj.shape[0])), features_tensor, adj_tensor)\n",
    "\n",
    "best_epoch = 0\n",
    "curr_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    temperature = max(0.05,args.init_temperature * pow(args.temperature_decay, epoch))\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = []\n",
    "        for l in range(args.outL):\n",
    "            output = model.call(temperature,training=True)\n",
    "            preds.append(tf.expand_dims(output,0))\n",
    "        all_preds = tf.concat(preds,axis=0)\n",
    "        mean_preds = tf.reduce_mean(preds,axis=0)\n",
    "        consistency_loss = tf.nn.l2_loss(mean_preds-all_preds)\n",
    "\n",
    "        cross_loss = masked_softmax_cross_entropy(mean_preds, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        lossl0 = model.lossl0(temperature)\n",
    "        nuclear = model.my_nuclear()\n",
    "        #nuclear = model.nuclear()\n",
    "        loss = cross_loss + args.lambda1*lossl0 + args.lambda3*nuclear+ args.weight_decay*lossL2 #+ args.coff_consis*consistency_loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call(None, training=False)\n",
    "    edges_volumn = tf.reduce_sum(model.maskes[0])\n",
    "    print('edge_vol',edges_volumn.numpy())\n",
    "\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_epoch = epoch\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        if val_acc>best_val_acc_trail:\n",
    "            best_test_acc = test_acc\n",
    "            best_val_acc_trail = val_acc\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\"best_val_acc_trail=\", \"{:.5f}\".format(best_val_acc_trail),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))\n",
    "\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    print('time ',(end-begin))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch: 0001 train_loss= 1.79190 val_loss= 1.79126 train_acc= 0.24200 val_acc= 0.24200 test_acc= 0.23900\n",
      "Epoch: 0002 train_loss= 1.78946 val_loss= 1.79044 train_acc= 0.38800 val_acc= 0.38800 test_acc= 0.37200\n",
      "Epoch: 0003 train_loss= 1.78690 val_loss= 1.78957 train_acc= 0.49200 val_acc= 0.49200 test_acc= 0.47500\n",
      "Epoch: 0004 train_loss= 1.78424 val_loss= 1.78861 train_acc= 0.56800 val_acc= 0.56800 test_acc= 0.52000\n",
      "Epoch: 0005 train_loss= 1.78143 val_loss= 1.78756 train_acc= 0.60400 val_acc= 0.60400 test_acc= 0.55500\n",
      "Epoch: 0006 train_loss= 1.77844 val_loss= 1.78639 train_acc= 0.60800 val_acc= 0.60800 test_acc= 0.58000\n",
      "Epoch: 0007 train_loss= 1.77523 val_loss= 1.78511 train_acc= 0.63200 val_acc= 0.63200 test_acc= 0.58800\n",
      "Epoch: 0008 train_loss= 1.77178 val_loss= 1.78370 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.60600\n",
      "Epoch: 0009 train_loss= 1.76811 val_loss= 1.78217 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.61500\n",
      "Epoch: 0010 train_loss= 1.76421 val_loss= 1.78053 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.62000\n",
      "Epoch: 0011 train_loss= 1.76007 val_loss= 1.77879 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.62000\n",
      "Epoch: 0012 train_loss= 1.75572 val_loss= 1.77696 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.62000\n",
      "Epoch: 0013 train_loss= 1.75116 val_loss= 1.77504 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.62000\n",
      "Epoch: 0014 train_loss= 1.74639 val_loss= 1.77305 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.62000\n",
      "Epoch: 0015 train_loss= 1.74141 val_loss= 1.77098 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.62000\n",
      "Epoch: 0016 train_loss= 1.73624 val_loss= 1.76883 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.62000\n",
      "Epoch: 0017 train_loss= 1.73086 val_loss= 1.76661 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.62000\n",
      "Epoch: 0018 train_loss= 1.72529 val_loss= 1.76432 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.62000\n",
      "Epoch: 0019 train_loss= 1.71951 val_loss= 1.76195 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.62000\n",
      "Epoch: 0020 train_loss= 1.71355 val_loss= 1.75951 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.62000\n",
      "Epoch: 0021 train_loss= 1.70739 val_loss= 1.75699 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0022 train_loss= 1.70104 val_loss= 1.75441 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64300\n",
      "Epoch: 0023 train_loss= 1.69450 val_loss= 1.75176 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64300\n",
      "Epoch: 0024 train_loss= 1.68777 val_loss= 1.74903 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64300\n",
      "Epoch: 0025 train_loss= 1.68086 val_loss= 1.74624 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64300\n",
      "Epoch: 0026 train_loss= 1.67377 val_loss= 1.74338 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.64400\n",
      "Epoch: 0027 train_loss= 1.66649 val_loss= 1.74045 train_acc= 0.66400 val_acc= 0.66400 test_acc= 0.64400\n",
      "Epoch: 0028 train_loss= 1.65903 val_loss= 1.73746 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64300\n",
      "Epoch: 0029 train_loss= 1.65139 val_loss= 1.73439 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.64300\n",
      "Epoch: 0030 train_loss= 1.64356 val_loss= 1.73126 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.64300\n",
      "Epoch: 0031 train_loss= 1.63555 val_loss= 1.72805 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0032 train_loss= 1.62735 val_loss= 1.72478 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0033 train_loss= 1.61897 val_loss= 1.72144 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0034 train_loss= 1.61041 val_loss= 1.71802 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0035 train_loss= 1.60166 val_loss= 1.71454 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0036 train_loss= 1.59273 val_loss= 1.71099 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64400\n",
      "Epoch: 0037 train_loss= 1.58362 val_loss= 1.70736 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64500\n",
      "Epoch: 0038 train_loss= 1.57432 val_loss= 1.70367 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64500\n",
      "Epoch: 0039 train_loss= 1.56484 val_loss= 1.69990 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64300\n",
      "Epoch: 0040 train_loss= 1.55518 val_loss= 1.69606 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64300\n",
      "Epoch: 0041 train_loss= 1.54533 val_loss= 1.69215 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64300\n",
      "Epoch: 0042 train_loss= 1.53530 val_loss= 1.68817 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64300\n",
      "Epoch: 0043 train_loss= 1.52509 val_loss= 1.68411 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64300\n",
      "Epoch: 0044 train_loss= 1.51470 val_loss= 1.67998 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64400\n",
      "Epoch: 0045 train_loss= 1.50414 val_loss= 1.67579 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64400\n",
      "Epoch: 0046 train_loss= 1.49340 val_loss= 1.67151 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0047 train_loss= 1.48250 val_loss= 1.66717 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0048 train_loss= 1.47142 val_loss= 1.66276 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0049 train_loss= 1.46017 val_loss= 1.65828 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0050 train_loss= 1.44876 val_loss= 1.65373 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0051 train_loss= 1.43718 val_loss= 1.64911 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0052 train_loss= 1.42545 val_loss= 1.64442 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0053 train_loss= 1.41356 val_loss= 1.63966 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0054 train_loss= 1.40152 val_loss= 1.63485 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0055 train_loss= 1.38934 val_loss= 1.62996 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0056 train_loss= 1.37701 val_loss= 1.62501 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0057 train_loss= 1.36454 val_loss= 1.62000 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0058 train_loss= 1.35194 val_loss= 1.61494 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0059 train_loss= 1.33921 val_loss= 1.60980 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0060 train_loss= 1.32636 val_loss= 1.60462 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0061 train_loss= 1.31338 val_loss= 1.59938 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0062 train_loss= 1.30030 val_loss= 1.59407 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0063 train_loss= 1.28711 val_loss= 1.58872 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0064 train_loss= 1.27382 val_loss= 1.58332 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0065 train_loss= 1.26044 val_loss= 1.57787 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0066 train_loss= 1.24696 val_loss= 1.57237 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0067 train_loss= 1.23341 val_loss= 1.56683 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64300\n",
      "Epoch: 0068 train_loss= 1.21977 val_loss= 1.56124 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0069 train_loss= 1.20607 val_loss= 1.55561 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0070 train_loss= 1.19231 val_loss= 1.54993 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0071 train_loss= 1.17848 val_loss= 1.54423 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0072 train_loss= 1.16463 val_loss= 1.53848 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0073 train_loss= 1.15073 val_loss= 1.53271 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0074 train_loss= 1.13677 val_loss= 1.52691 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0075 train_loss= 1.12282 val_loss= 1.52109 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0076 train_loss= 1.10883 val_loss= 1.51523 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0077 train_loss= 1.09484 val_loss= 1.50936 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0078 train_loss= 1.08085 val_loss= 1.50346 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0079 train_loss= 1.06685 val_loss= 1.49755 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0080 train_loss= 1.05288 val_loss= 1.49163 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0081 train_loss= 1.03892 val_loss= 1.48569 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0082 train_loss= 1.02499 val_loss= 1.47975 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0083 train_loss= 1.01109 val_loss= 1.47379 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0084 train_loss= 0.99722 val_loss= 1.46782 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0085 train_loss= 0.98341 val_loss= 1.46186 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0086 train_loss= 0.96964 val_loss= 1.45591 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0087 train_loss= 0.95595 val_loss= 1.44997 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0088 train_loss= 0.94232 val_loss= 1.44402 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0089 train_loss= 0.92876 val_loss= 1.43808 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0090 train_loss= 0.91528 val_loss= 1.43216 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0091 train_loss= 0.90188 val_loss= 1.42626 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0092 train_loss= 0.88859 val_loss= 1.42037 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0093 train_loss= 0.87539 val_loss= 1.41450 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0094 train_loss= 0.86229 val_loss= 1.40865 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0095 train_loss= 0.84929 val_loss= 1.40283 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0096 train_loss= 0.83642 val_loss= 1.39705 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0097 train_loss= 0.82366 val_loss= 1.39129 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0098 train_loss= 0.81101 val_loss= 1.38557 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0099 train_loss= 0.79850 val_loss= 1.37987 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0100 train_loss= 0.78611 val_loss= 1.37423 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64300\n",
      "Epoch: 0101 train_loss= 0.77385 val_loss= 1.36862 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0102 train_loss= 0.76175 val_loss= 1.36305 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0103 train_loss= 0.74977 val_loss= 1.35751 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64300\n",
      "Epoch: 0104 train_loss= 0.73793 val_loss= 1.35204 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0105 train_loss= 0.72625 val_loss= 1.34659 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64300\n",
      "Epoch: 0106 train_loss= 0.71469 val_loss= 1.34121 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0107 train_loss= 0.70331 val_loss= 1.33587 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0108 train_loss= 0.69206 val_loss= 1.33059 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0109 train_loss= 0.68098 val_loss= 1.32536 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0110 train_loss= 0.67004 val_loss= 1.32019 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0111 train_loss= 0.65926 val_loss= 1.31506 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0112 train_loss= 0.64863 val_loss= 1.31000 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0113 train_loss= 0.63816 val_loss= 1.30502 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0114 train_loss= 0.62786 val_loss= 1.30006 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0115 train_loss= 0.61769 val_loss= 1.29518 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0116 train_loss= 0.60770 val_loss= 1.29037 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0117 train_loss= 0.59785 val_loss= 1.28561 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0118 train_loss= 0.58816 val_loss= 1.28091 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0119 train_loss= 0.57865 val_loss= 1.27629 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0120 train_loss= 0.56927 val_loss= 1.27174 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64800\n",
      "Epoch: 0121 train_loss= 0.56006 val_loss= 1.26725 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64800\n",
      "Epoch: 0122 train_loss= 0.55101 val_loss= 1.26281 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64800\n",
      "Epoch: 0123 train_loss= 0.54209 val_loss= 1.25845 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0124 train_loss= 0.53334 val_loss= 1.25416 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0125 train_loss= 0.52473 val_loss= 1.24992 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0126 train_loss= 0.51628 val_loss= 1.24575 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64800\n",
      "Epoch: 0127 train_loss= 0.50796 val_loss= 1.24168 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0128 train_loss= 0.49982 val_loss= 1.23765 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64800\n",
      "Epoch: 0129 train_loss= 0.49179 val_loss= 1.23369 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64800\n",
      "Epoch: 0130 train_loss= 0.48393 val_loss= 1.22979 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64800\n",
      "Epoch: 0131 train_loss= 0.47620 val_loss= 1.22597 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64800\n",
      "Epoch: 0132 train_loss= 0.46861 val_loss= 1.22220 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.64700\n",
      "Epoch: 0133 train_loss= 0.46116 val_loss= 1.21852 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.64700\n",
      "Epoch: 0134 train_loss= 0.45385 val_loss= 1.21490 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.64700\n",
      "Epoch: 0135 train_loss= 0.44668 val_loss= 1.21132 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.64700\n",
      "Epoch: 0136 train_loss= 0.43964 val_loss= 1.20782 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.64700\n",
      "Epoch: 0137 train_loss= 0.43272 val_loss= 1.20438 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.64700\n",
      "Epoch: 0138 train_loss= 0.42593 val_loss= 1.20101 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0139 train_loss= 0.41928 val_loss= 1.19770 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0140 train_loss= 0.41275 val_loss= 1.19445 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0141 train_loss= 0.40634 val_loss= 1.19126 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0142 train_loss= 0.40005 val_loss= 1.18812 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0143 train_loss= 0.39388 val_loss= 1.18506 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0144 train_loss= 0.38783 val_loss= 1.18205 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0145 train_loss= 0.38189 val_loss= 1.17909 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0146 train_loss= 0.37606 val_loss= 1.17620 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0147 train_loss= 0.37035 val_loss= 1.17334 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0148 train_loss= 0.36473 val_loss= 1.17058 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0149 train_loss= 0.35924 val_loss= 1.16786 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0150 train_loss= 0.35385 val_loss= 1.16518 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0151 train_loss= 0.34855 val_loss= 1.16255 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0152 train_loss= 0.34335 val_loss= 1.15999 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0153 train_loss= 0.33826 val_loss= 1.15747 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0154 train_loss= 0.33326 val_loss= 1.15501 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0155 train_loss= 0.32836 val_loss= 1.15259 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0156 train_loss= 0.32356 val_loss= 1.15021 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0157 train_loss= 0.31883 val_loss= 1.14788 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0158 train_loss= 0.31420 val_loss= 1.14561 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0159 train_loss= 0.30966 val_loss= 1.14339 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0160 train_loss= 0.30521 val_loss= 1.14120 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.64700\n",
      "Epoch: 0161 train_loss= 0.30083 val_loss= 1.13906 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0162 train_loss= 0.29654 val_loss= 1.13698 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0163 train_loss= 0.29233 val_loss= 1.13492 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0164 train_loss= 0.28820 val_loss= 1.13291 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0165 train_loss= 0.28415 val_loss= 1.13094 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0166 train_loss= 0.28017 val_loss= 1.12903 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0167 train_loss= 0.27627 val_loss= 1.12714 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0168 train_loss= 0.27243 val_loss= 1.12529 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0169 train_loss= 0.26867 val_loss= 1.12348 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0170 train_loss= 0.26498 val_loss= 1.12172 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0171 train_loss= 0.26136 val_loss= 1.11999 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0172 train_loss= 0.25779 val_loss= 1.11829 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0173 train_loss= 0.25431 val_loss= 1.11663 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.64700\n",
      "Epoch: 0174 train_loss= 0.25088 val_loss= 1.11502 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0175 train_loss= 0.24751 val_loss= 1.11343 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0176 train_loss= 0.24421 val_loss= 1.11187 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0177 train_loss= 0.24097 val_loss= 1.11035 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0178 train_loss= 0.23778 val_loss= 1.10885 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0179 train_loss= 0.23465 val_loss= 1.10740 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0180 train_loss= 0.23157 val_loss= 1.10596 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0181 train_loss= 0.22856 val_loss= 1.10456 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.64700\n",
      "Epoch: 0182 train_loss= 0.22559 val_loss= 1.10318 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.64700\n",
      "Epoch: 0183 train_loss= 0.22268 val_loss= 1.10185 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0184 train_loss= 0.21982 val_loss= 1.10054 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0185 train_loss= 0.21701 val_loss= 1.09926 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0186 train_loss= 0.21426 val_loss= 1.09801 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0187 train_loss= 0.21155 val_loss= 1.09678 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0188 train_loss= 0.20888 val_loss= 1.09557 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0189 train_loss= 0.20627 val_loss= 1.09441 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0190 train_loss= 0.20370 val_loss= 1.09326 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0191 train_loss= 0.20117 val_loss= 1.09213 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0192 train_loss= 0.19868 val_loss= 1.09104 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.64700\n",
      "Epoch: 0193 train_loss= 0.19624 val_loss= 1.08998 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.64700\n",
      "Epoch: 0194 train_loss= 0.19385 val_loss= 1.08894 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0195 train_loss= 0.19149 val_loss= 1.08791 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0196 train_loss= 0.18917 val_loss= 1.08690 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64700\n",
      "Epoch: 0197 train_loss= 0.18689 val_loss= 1.08593 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64700\n",
      "Epoch: 0198 train_loss= 0.18465 val_loss= 1.08498 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64700\n",
      "Epoch: 0199 train_loss= 0.18245 val_loss= 1.08405 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0200 train_loss= 0.18028 val_loss= 1.08315 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0201 train_loss= 0.17815 val_loss= 1.08225 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0202 train_loss= 0.17605 val_loss= 1.08139 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0203 train_loss= 0.17399 val_loss= 1.08054 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64700\n",
      "Epoch: 0204 train_loss= 0.17197 val_loss= 1.07970 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64700\n",
      "Epoch: 0205 train_loss= 0.16997 val_loss= 1.07891 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0206 train_loss= 0.16802 val_loss= 1.07812 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0207 train_loss= 0.16608 val_loss= 1.07735 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0208 train_loss= 0.16419 val_loss= 1.07659 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0209 train_loss= 0.16232 val_loss= 1.07587 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0210 train_loss= 0.16049 val_loss= 1.07514 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0211 train_loss= 0.15867 val_loss= 1.07445 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0212 train_loss= 0.15689 val_loss= 1.07376 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0213 train_loss= 0.15514 val_loss= 1.07309 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0214 train_loss= 0.15342 val_loss= 1.07245 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0215 train_loss= 0.15172 val_loss= 1.07183 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0216 train_loss= 0.15005 val_loss= 1.07122 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0217 train_loss= 0.14841 val_loss= 1.07063 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0218 train_loss= 0.14679 val_loss= 1.07005 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0219 train_loss= 0.14520 val_loss= 1.06949 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0220 train_loss= 0.14363 val_loss= 1.06893 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0221 train_loss= 0.14208 val_loss= 1.06840 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0222 train_loss= 0.14057 val_loss= 1.06788 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0223 train_loss= 0.13907 val_loss= 1.06738 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0224 train_loss= 0.13759 val_loss= 1.06689 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0225 train_loss= 0.13614 val_loss= 1.06642 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0226 train_loss= 0.13470 val_loss= 1.06596 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0227 train_loss= 0.13330 val_loss= 1.06548 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0228 train_loss= 0.13191 val_loss= 1.06505 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0229 train_loss= 0.13054 val_loss= 1.06462 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0230 train_loss= 0.12920 val_loss= 1.06420 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0231 train_loss= 0.12787 val_loss= 1.06381 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Epoch: 0232 train_loss= 0.12656 val_loss= 1.06343 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0233 train_loss= 0.12527 val_loss= 1.06303 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64700\n",
      "Epoch: 0234 train_loss= 0.12400 val_loss= 1.06267 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64700\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "##GCN(被攻击）\n",
    "\n",
    "from config import args\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import *\n",
    "from models import GCN_dropedge\n",
    "from metrics import *\n",
    "\n",
    "# Settings\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "args.dataset=dataset_name\n",
    "args.dropout=0.0\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "\n",
    "features = preprocess_features(features)\n",
    "\n",
    "model = GCN_dropedge(input_dim=features.shape[1], output_dim=y_train.shape[1], adj=adj_tensor)\n",
    "\n",
    "\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=tf.float32)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=tf.float32)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=tf.float32)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "\n",
    "curr_step = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model.call((features_tensor),training=True)\n",
    "        cross_loss = masked_softmax_cross_entropy(output, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        loss = cross_loss# + args.weight_decay*lossL2\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call((features_tensor), training=False)\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_test_acc = test_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        # Print results\n",
    "\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(val_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "Epoch: 0001 train_loss= 1.79210 val_loss= 1.79151 train_acc= 0.21800 val_acc= 0.21800 test_acc= 0.19700\n",
      "Epoch: 0002 train_loss= 1.79006 val_loss= 1.79123 train_acc= 0.29600 val_acc= 0.29600 test_acc= 0.23300\n",
      "Epoch: 0003 train_loss= 1.79054 val_loss= 1.79094 train_acc= 0.33400 val_acc= 0.33400 test_acc= 0.28100\n",
      "Epoch: 0004 train_loss= 1.78963 val_loss= 1.79063 train_acc= 0.36800 val_acc= 0.36800 test_acc= 0.31800\n",
      "Epoch: 0005 train_loss= 1.78919 val_loss= 1.79031 train_acc= 0.39400 val_acc= 0.39400 test_acc= 0.35400\n",
      "Epoch: 0006 train_loss= 1.78930 val_loss= 1.78999 train_acc= 0.43800 val_acc= 0.43800 test_acc= 0.38300\n",
      "Epoch: 0007 train_loss= 1.78824 val_loss= 1.78966 train_acc= 0.46200 val_acc= 0.46200 test_acc= 0.42100\n",
      "Epoch: 0008 train_loss= 1.78689 val_loss= 1.78931 train_acc= 0.49600 val_acc= 0.49600 test_acc= 0.45100\n",
      "Epoch: 0009 train_loss= 1.78683 val_loss= 1.78895 train_acc= 0.52000 val_acc= 0.52000 test_acc= 0.46900\n",
      "Epoch: 0010 train_loss= 1.78622 val_loss= 1.78858 train_acc= 0.53600 val_acc= 0.53600 test_acc= 0.48700\n",
      "Epoch: 0011 train_loss= 1.78547 val_loss= 1.78821 train_acc= 0.55600 val_acc= 0.55600 test_acc= 0.52000\n",
      "Epoch: 0012 train_loss= 1.78453 val_loss= 1.78782 train_acc= 0.56600 val_acc= 0.56600 test_acc= 0.53900\n",
      "Epoch: 0013 train_loss= 1.78405 val_loss= 1.78743 train_acc= 0.57400 val_acc= 0.57400 test_acc= 0.55200\n",
      "Epoch: 0014 train_loss= 1.78386 val_loss= 1.78703 train_acc= 0.58600 val_acc= 0.58600 test_acc= 0.56100\n",
      "Epoch: 0015 train_loss= 1.78321 val_loss= 1.78663 train_acc= 0.59200 val_acc= 0.59200 test_acc= 0.57100\n",
      "Epoch: 0016 train_loss= 1.78211 val_loss= 1.78620 train_acc= 0.59800 val_acc= 0.59800 test_acc= 0.58200\n",
      "Epoch: 0017 train_loss= 1.78193 val_loss= 1.78577 train_acc= 0.60800 val_acc= 0.60800 test_acc= 0.59500\n",
      "Epoch: 0018 train_loss= 1.77937 val_loss= 1.78531 train_acc= 0.62200 val_acc= 0.62200 test_acc= 0.60300\n",
      "Epoch: 0019 train_loss= 1.77922 val_loss= 1.78484 train_acc= 0.63400 val_acc= 0.63400 test_acc= 0.61000\n",
      "Epoch: 0020 train_loss= 1.77976 val_loss= 1.78436 train_acc= 0.63200 val_acc= 0.63200 test_acc= 0.61000\n",
      "Epoch: 0021 train_loss= 1.77632 val_loss= 1.78385 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.62200\n",
      "Epoch: 0022 train_loss= 1.77629 val_loss= 1.78334 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.62300\n",
      "Epoch: 0023 train_loss= 1.77507 val_loss= 1.78280 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.62300\n",
      "Epoch: 0024 train_loss= 1.77552 val_loss= 1.78224 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.63200\n",
      "Epoch: 0025 train_loss= 1.77463 val_loss= 1.78168 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.63400\n",
      "Epoch: 0026 train_loss= 1.77245 val_loss= 1.78109 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63800\n",
      "Epoch: 0027 train_loss= 1.77141 val_loss= 1.78048 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0028 train_loss= 1.76976 val_loss= 1.77986 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.63900\n",
      "Epoch: 0029 train_loss= 1.76962 val_loss= 1.77924 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64000\n",
      "Epoch: 0030 train_loss= 1.76959 val_loss= 1.77860 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0031 train_loss= 1.76580 val_loss= 1.77795 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0032 train_loss= 1.76539 val_loss= 1.77729 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64000\n",
      "Epoch: 0033 train_loss= 1.76402 val_loss= 1.77660 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64000\n",
      "Epoch: 0034 train_loss= 1.76310 val_loss= 1.77589 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64000\n",
      "Epoch: 0035 train_loss= 1.76098 val_loss= 1.77517 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64000\n",
      "Epoch: 0036 train_loss= 1.75665 val_loss= 1.77444 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64000\n",
      "Epoch: 0037 train_loss= 1.75629 val_loss= 1.77369 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.64000\n",
      "Epoch: 0038 train_loss= 1.75619 val_loss= 1.77293 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0039 train_loss= 1.75798 val_loss= 1.77216 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64000\n",
      "Epoch: 0040 train_loss= 1.75336 val_loss= 1.77139 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0041 train_loss= 1.75080 val_loss= 1.77061 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0042 train_loss= 1.75061 val_loss= 1.76982 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0043 train_loss= 1.74715 val_loss= 1.76901 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0044 train_loss= 1.74391 val_loss= 1.76817 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0045 train_loss= 1.74413 val_loss= 1.76732 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0046 train_loss= 1.74364 val_loss= 1.76646 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0047 train_loss= 1.74258 val_loss= 1.76560 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0048 train_loss= 1.73776 val_loss= 1.76471 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0049 train_loss= 1.73766 val_loss= 1.76380 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0050 train_loss= 1.73725 val_loss= 1.76289 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.64000\n",
      "Epoch: 0051 train_loss= 1.73007 val_loss= 1.76196 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0052 train_loss= 1.73261 val_loss= 1.76103 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0053 train_loss= 1.73209 val_loss= 1.76009 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0054 train_loss= 1.72973 val_loss= 1.75913 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0055 train_loss= 1.72693 val_loss= 1.75813 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0056 train_loss= 1.72250 val_loss= 1.75713 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0057 train_loss= 1.71485 val_loss= 1.75611 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0058 train_loss= 1.72164 val_loss= 1.75509 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0059 train_loss= 1.71279 val_loss= 1.75404 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0060 train_loss= 1.70703 val_loss= 1.75297 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0061 train_loss= 1.71182 val_loss= 1.75189 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0062 train_loss= 1.70758 val_loss= 1.75079 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0063 train_loss= 1.70809 val_loss= 1.74967 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0064 train_loss= 1.70010 val_loss= 1.74855 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0065 train_loss= 1.70456 val_loss= 1.74743 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0066 train_loss= 1.70546 val_loss= 1.74629 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0067 train_loss= 1.69569 val_loss= 1.74516 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0068 train_loss= 1.69667 val_loss= 1.74402 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0069 train_loss= 1.69099 val_loss= 1.74288 train_acc= 0.64600 val_acc= 0.64600 test_acc= 0.64000\n",
      "Epoch: 0070 train_loss= 1.68795 val_loss= 1.74172 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0071 train_loss= 1.68188 val_loss= 1.74054 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0072 train_loss= 1.69208 val_loss= 1.73935 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0073 train_loss= 1.68424 val_loss= 1.73814 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0074 train_loss= 1.68233 val_loss= 1.73690 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0075 train_loss= 1.67688 val_loss= 1.73565 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0076 train_loss= 1.67572 val_loss= 1.73442 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0077 train_loss= 1.66075 val_loss= 1.73317 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0078 train_loss= 1.67393 val_loss= 1.73192 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0079 train_loss= 1.66183 val_loss= 1.73062 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0080 train_loss= 1.66183 val_loss= 1.72931 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0081 train_loss= 1.65543 val_loss= 1.72798 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0082 train_loss= 1.65697 val_loss= 1.72667 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0083 train_loss= 1.64846 val_loss= 1.72534 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.64000\n",
      "Epoch: 0084 train_loss= 1.64196 val_loss= 1.72399 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.64000\n",
      "Epoch: 0085 train_loss= 1.64799 val_loss= 1.72264 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0086 train_loss= 1.64214 val_loss= 1.72126 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0087 train_loss= 1.63304 val_loss= 1.71987 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0088 train_loss= 1.63431 val_loss= 1.71846 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0089 train_loss= 1.63857 val_loss= 1.71701 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0090 train_loss= 1.63210 val_loss= 1.71559 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0091 train_loss= 1.62530 val_loss= 1.71415 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0092 train_loss= 1.62461 val_loss= 1.71267 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0093 train_loss= 1.61817 val_loss= 1.71115 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0094 train_loss= 1.61063 val_loss= 1.70959 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0095 train_loss= 1.60886 val_loss= 1.70801 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0096 train_loss= 1.61356 val_loss= 1.70639 train_acc= 0.63800 val_acc= 0.63800 test_acc= 0.64000\n",
      "Epoch: 0097 train_loss= 1.60374 val_loss= 1.70477 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.64000\n",
      "Epoch: 0098 train_loss= 1.60069 val_loss= 1.70311 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0099 train_loss= 1.59647 val_loss= 1.70143 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.64000\n",
      "Epoch: 0100 train_loss= 1.58222 val_loss= 1.69973 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0101 train_loss= 1.59325 val_loss= 1.69806 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0102 train_loss= 1.58497 val_loss= 1.69637 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.64000\n",
      "Epoch: 0103 train_loss= 1.58718 val_loss= 1.69467 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0104 train_loss= 1.56981 val_loss= 1.69297 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.64000\n",
      "Epoch: 0105 train_loss= 1.57191 val_loss= 1.69125 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0106 train_loss= 1.57510 val_loss= 1.68953 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0107 train_loss= 1.56361 val_loss= 1.68780 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0108 train_loss= 1.56012 val_loss= 1.68606 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0109 train_loss= 1.54906 val_loss= 1.68435 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.64000\n",
      "Epoch: 0110 train_loss= 1.55794 val_loss= 1.68262 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0111 train_loss= 1.55800 val_loss= 1.68091 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0112 train_loss= 1.54808 val_loss= 1.67917 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0113 train_loss= 1.54482 val_loss= 1.67741 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0114 train_loss= 1.54110 val_loss= 1.67562 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.64000\n",
      "Epoch: 0115 train_loss= 1.55752 val_loss= 1.67384 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.64000\n",
      "Epoch: 0116 train_loss= 1.53795 val_loss= 1.67205 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.64000\n",
      "Epoch: 0117 train_loss= 1.53803 val_loss= 1.67026 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.64000\n",
      "Epoch: 0118 train_loss= 1.53179 val_loss= 1.66845 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.64000\n",
      "Epoch: 0119 train_loss= 1.52243 val_loss= 1.66660 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0120 train_loss= 1.50589 val_loss= 1.66477 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0121 train_loss= 1.52166 val_loss= 1.66293 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0122 train_loss= 1.49748 val_loss= 1.66107 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0123 train_loss= 1.49825 val_loss= 1.65916 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0124 train_loss= 1.49955 val_loss= 1.65722 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0125 train_loss= 1.49708 val_loss= 1.65529 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0126 train_loss= 1.49624 val_loss= 1.65334 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0127 train_loss= 1.47407 val_loss= 1.65137 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0128 train_loss= 1.48079 val_loss= 1.64943 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0129 train_loss= 1.47817 val_loss= 1.64745 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.64000\n",
      "Epoch: 0130 train_loss= 1.49052 val_loss= 1.64547 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0131 train_loss= 1.47551 val_loss= 1.64344 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0132 train_loss= 1.47304 val_loss= 1.64139 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0133 train_loss= 1.48201 val_loss= 1.63938 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0134 train_loss= 1.45439 val_loss= 1.63734 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0135 train_loss= 1.48162 val_loss= 1.63528 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64200\n",
      "Epoch: 0136 train_loss= 1.46766 val_loss= 1.63322 train_acc= 0.66400 val_acc= 0.66400 test_acc= 0.64200\n",
      "Epoch: 0137 train_loss= 1.44297 val_loss= 1.63114 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.64300\n",
      "Epoch: 0138 train_loss= 1.45613 val_loss= 1.62908 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.64300\n",
      "Epoch: 0139 train_loss= 1.41761 val_loss= 1.62697 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64500\n",
      "Epoch: 0140 train_loss= 1.44166 val_loss= 1.62485 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64500\n",
      "Epoch: 0141 train_loss= 1.45928 val_loss= 1.62278 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.64500\n",
      "Epoch: 0142 train_loss= 1.44231 val_loss= 1.62069 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64400\n",
      "Epoch: 0143 train_loss= 1.42037 val_loss= 1.61853 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.64400\n",
      "Epoch: 0144 train_loss= 1.44621 val_loss= 1.61636 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64400\n",
      "Epoch: 0145 train_loss= 1.40551 val_loss= 1.61418 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64400\n",
      "Epoch: 0146 train_loss= 1.41151 val_loss= 1.61201 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.64400\n",
      "Epoch: 0147 train_loss= 1.41852 val_loss= 1.60985 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0148 train_loss= 1.41992 val_loss= 1.60772 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0149 train_loss= 1.40063 val_loss= 1.60557 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0150 train_loss= 1.40604 val_loss= 1.60345 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0151 train_loss= 1.38770 val_loss= 1.60129 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0152 train_loss= 1.39124 val_loss= 1.59912 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0153 train_loss= 1.38846 val_loss= 1.59696 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.64700\n",
      "Epoch: 0154 train_loss= 1.34958 val_loss= 1.59476 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0155 train_loss= 1.38298 val_loss= 1.59255 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0156 train_loss= 1.36326 val_loss= 1.59033 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0157 train_loss= 1.38595 val_loss= 1.58810 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0158 train_loss= 1.38492 val_loss= 1.58588 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.65200\n",
      "Epoch: 0159 train_loss= 1.37952 val_loss= 1.58363 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.65200\n",
      "Epoch: 0160 train_loss= 1.38005 val_loss= 1.58139 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0161 train_loss= 1.35922 val_loss= 1.57920 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.65200\n",
      "Epoch: 0162 train_loss= 1.36806 val_loss= 1.57700 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.65400\n",
      "Epoch: 0163 train_loss= 1.35582 val_loss= 1.57482 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.65400\n",
      "Epoch: 0164 train_loss= 1.35339 val_loss= 1.57262 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.65600\n",
      "Epoch: 0165 train_loss= 1.32680 val_loss= 1.57041 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.65700\n",
      "Epoch: 0166 train_loss= 1.33702 val_loss= 1.56822 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.65700\n",
      "Epoch: 0167 train_loss= 1.31080 val_loss= 1.56603 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0168 train_loss= 1.30837 val_loss= 1.56390 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0169 train_loss= 1.31192 val_loss= 1.56177 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0170 train_loss= 1.28551 val_loss= 1.55966 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0171 train_loss= 1.31229 val_loss= 1.55757 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0172 train_loss= 1.27628 val_loss= 1.55550 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.65800\n",
      "Epoch: 0173 train_loss= 1.30257 val_loss= 1.55345 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66000\n",
      "Epoch: 0174 train_loss= 1.28519 val_loss= 1.55134 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66000\n",
      "Epoch: 0175 train_loss= 1.31100 val_loss= 1.54924 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.66000\n",
      "Epoch: 0176 train_loss= 1.29713 val_loss= 1.54715 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0177 train_loss= 1.27849 val_loss= 1.54504 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0178 train_loss= 1.28760 val_loss= 1.54297 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66000\n",
      "Epoch: 0179 train_loss= 1.29814 val_loss= 1.54090 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0180 train_loss= 1.26008 val_loss= 1.53879 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0181 train_loss= 1.28355 val_loss= 1.53668 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0182 train_loss= 1.27231 val_loss= 1.53457 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0183 train_loss= 1.27959 val_loss= 1.53250 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0184 train_loss= 1.23720 val_loss= 1.53044 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0185 train_loss= 1.24549 val_loss= 1.52837 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0186 train_loss= 1.23374 val_loss= 1.52621 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0187 train_loss= 1.25715 val_loss= 1.52411 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0188 train_loss= 1.24774 val_loss= 1.52198 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0189 train_loss= 1.26623 val_loss= 1.51989 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0190 train_loss= 1.23561 val_loss= 1.51773 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0191 train_loss= 1.22767 val_loss= 1.51561 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0192 train_loss= 1.22852 val_loss= 1.51350 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0193 train_loss= 1.21205 val_loss= 1.51134 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0194 train_loss= 1.19251 val_loss= 1.50917 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0195 train_loss= 1.22943 val_loss= 1.50703 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0196 train_loss= 1.20761 val_loss= 1.50492 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0197 train_loss= 1.22539 val_loss= 1.50282 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0198 train_loss= 1.21325 val_loss= 1.50072 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0199 train_loss= 1.21294 val_loss= 1.49865 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0200 train_loss= 1.18735 val_loss= 1.49659 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0201 train_loss= 1.20005 val_loss= 1.49447 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0202 train_loss= 1.19795 val_loss= 1.49236 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0203 train_loss= 1.17516 val_loss= 1.49025 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66000\n",
      "Epoch: 0204 train_loss= 1.16638 val_loss= 1.48813 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0205 train_loss= 1.18938 val_loss= 1.48602 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0206 train_loss= 1.16568 val_loss= 1.48393 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66000\n",
      "Epoch: 0207 train_loss= 1.16457 val_loss= 1.48184 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.66000\n",
      "Epoch: 0208 train_loss= 1.19684 val_loss= 1.47978 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.66000\n",
      "Epoch: 0209 train_loss= 1.13909 val_loss= 1.47768 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66700\n",
      "Epoch: 0210 train_loss= 1.12835 val_loss= 1.47556 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66700\n",
      "Epoch: 0211 train_loss= 1.13264 val_loss= 1.47343 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66700\n",
      "Epoch: 0212 train_loss= 1.17268 val_loss= 1.47133 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66700\n",
      "Epoch: 0213 train_loss= 1.12960 val_loss= 1.46924 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66700\n",
      "Epoch: 0214 train_loss= 1.12995 val_loss= 1.46716 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0215 train_loss= 1.08890 val_loss= 1.46510 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0216 train_loss= 1.13278 val_loss= 1.46298 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0217 train_loss= 1.11919 val_loss= 1.46087 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0218 train_loss= 1.11859 val_loss= 1.45877 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0219 train_loss= 1.11642 val_loss= 1.45673 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66900\n",
      "Epoch: 0220 train_loss= 1.11457 val_loss= 1.45468 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66900\n",
      "Epoch: 0221 train_loss= 1.08401 val_loss= 1.45260 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66900\n",
      "Epoch: 0222 train_loss= 1.15639 val_loss= 1.45064 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66900\n",
      "Epoch: 0223 train_loss= 1.11264 val_loss= 1.44869 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.67000\n",
      "Epoch: 0224 train_loss= 1.06435 val_loss= 1.44678 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67000\n",
      "Epoch: 0225 train_loss= 1.09706 val_loss= 1.44488 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67000\n",
      "Epoch: 0226 train_loss= 1.11076 val_loss= 1.44300 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0227 train_loss= 1.11068 val_loss= 1.44121 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0228 train_loss= 1.07325 val_loss= 1.43946 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0229 train_loss= 1.09533 val_loss= 1.43769 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0230 train_loss= 1.07179 val_loss= 1.43594 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0231 train_loss= 1.09332 val_loss= 1.43420 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0232 train_loss= 1.07346 val_loss= 1.43240 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0233 train_loss= 1.07326 val_loss= 1.43063 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0234 train_loss= 1.05462 val_loss= 1.42893 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0235 train_loss= 1.08610 val_loss= 1.42730 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0236 train_loss= 1.07252 val_loss= 1.42569 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0237 train_loss= 1.08213 val_loss= 1.42405 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0238 train_loss= 1.09898 val_loss= 1.42238 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0239 train_loss= 1.02521 val_loss= 1.42074 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0240 train_loss= 1.02947 val_loss= 1.41912 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0241 train_loss= 1.02084 val_loss= 1.41746 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0242 train_loss= 1.08407 val_loss= 1.41586 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0243 train_loss= 1.04858 val_loss= 1.41426 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0244 train_loss= 1.07682 val_loss= 1.41267 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0245 train_loss= 1.03141 val_loss= 1.41101 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0246 train_loss= 1.00514 val_loss= 1.40938 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0247 train_loss= 1.03189 val_loss= 1.40772 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0248 train_loss= 0.99782 val_loss= 1.40613 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0249 train_loss= 1.05694 val_loss= 1.40458 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0250 train_loss= 1.04351 val_loss= 1.40316 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0251 train_loss= 0.99784 val_loss= 1.40179 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0252 train_loss= 1.04636 val_loss= 1.40041 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0253 train_loss= 1.01398 val_loss= 1.39913 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0254 train_loss= 1.00498 val_loss= 1.39780 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0255 train_loss= 0.98623 val_loss= 1.39643 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0256 train_loss= 1.03803 val_loss= 1.39497 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0257 train_loss= 1.01879 val_loss= 1.39342 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0258 train_loss= 1.02233 val_loss= 1.39174 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0259 train_loss= 1.00021 val_loss= 1.39000 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67000\n",
      "Epoch: 0260 train_loss= 0.98205 val_loss= 1.38829 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0261 train_loss= 0.95169 val_loss= 1.38657 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0262 train_loss= 1.03750 val_loss= 1.38489 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0263 train_loss= 0.98680 val_loss= 1.38321 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0264 train_loss= 0.99553 val_loss= 1.38145 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.67000\n",
      "Epoch: 0265 train_loss= 0.97499 val_loss= 1.37973 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.67000\n",
      "Epoch: 0266 train_loss= 0.98750 val_loss= 1.37806 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.67000\n",
      "Epoch: 0267 train_loss= 0.96991 val_loss= 1.37638 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.67000\n",
      "Epoch: 0268 train_loss= 0.97298 val_loss= 1.37465 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0269 train_loss= 1.00798 val_loss= 1.37295 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0270 train_loss= 0.96895 val_loss= 1.37125 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0271 train_loss= 0.98351 val_loss= 1.36959 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0272 train_loss= 0.89839 val_loss= 1.36791 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0273 train_loss= 0.95897 val_loss= 1.36633 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0274 train_loss= 0.98593 val_loss= 1.36473 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0275 train_loss= 0.92513 val_loss= 1.36317 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0276 train_loss= 0.91500 val_loss= 1.36164 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0277 train_loss= 0.98087 val_loss= 1.36017 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0278 train_loss= 0.95222 val_loss= 1.35877 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0279 train_loss= 0.90574 val_loss= 1.35733 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67000\n",
      "Epoch: 0280 train_loss= 0.92875 val_loss= 1.35590 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67000\n",
      "Epoch: 0281 train_loss= 0.93362 val_loss= 1.35450 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.67000\n",
      "Epoch: 0282 train_loss= 0.93357 val_loss= 1.35321 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.67000\n",
      "Epoch: 0283 train_loss= 0.93759 val_loss= 1.35193 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.67000\n",
      "Epoch: 0284 train_loss= 0.97319 val_loss= 1.35062 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0285 train_loss= 0.97239 val_loss= 1.34929 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68100\n",
      "Epoch: 0286 train_loss= 0.92806 val_loss= 1.34796 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0287 train_loss= 0.93512 val_loss= 1.34665 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0288 train_loss= 0.93098 val_loss= 1.34533 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0289 train_loss= 0.91315 val_loss= 1.34399 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0290 train_loss= 0.96985 val_loss= 1.34265 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0291 train_loss= 0.92469 val_loss= 1.34128 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0292 train_loss= 0.92500 val_loss= 1.33988 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0293 train_loss= 0.92202 val_loss= 1.33861 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0294 train_loss= 0.88196 val_loss= 1.33723 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0295 train_loss= 0.93274 val_loss= 1.33582 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0296 train_loss= 0.88572 val_loss= 1.33439 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0297 train_loss= 0.92510 val_loss= 1.33303 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68000\n",
      "Epoch: 0298 train_loss= 0.90626 val_loss= 1.33166 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68400\n",
      "Epoch: 0299 train_loss= 0.88256 val_loss= 1.33020 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68400\n",
      "Epoch: 0300 train_loss= 0.86237 val_loss= 1.32878 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0301 train_loss= 0.89666 val_loss= 1.32738 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0302 train_loss= 0.91512 val_loss= 1.32587 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0303 train_loss= 0.84959 val_loss= 1.32438 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0304 train_loss= 0.89062 val_loss= 1.32288 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0305 train_loss= 0.86881 val_loss= 1.32143 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0306 train_loss= 0.95143 val_loss= 1.32003 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0307 train_loss= 0.90113 val_loss= 1.31858 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0308 train_loss= 0.89228 val_loss= 1.31727 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0309 train_loss= 0.91204 val_loss= 1.31586 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0310 train_loss= 0.85785 val_loss= 1.31454 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0311 train_loss= 0.90000 val_loss= 1.31321 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0312 train_loss= 0.89837 val_loss= 1.31193 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0313 train_loss= 0.87454 val_loss= 1.31070 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0314 train_loss= 0.87001 val_loss= 1.30950 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0315 train_loss= 0.87117 val_loss= 1.30830 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0316 train_loss= 0.89360 val_loss= 1.30717 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0317 train_loss= 0.86053 val_loss= 1.30605 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0318 train_loss= 0.85375 val_loss= 1.30497 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0319 train_loss= 0.87638 val_loss= 1.30389 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0320 train_loss= 0.84416 val_loss= 1.30284 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0321 train_loss= 0.83122 val_loss= 1.30173 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0322 train_loss= 0.87003 val_loss= 1.30060 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0323 train_loss= 0.85390 val_loss= 1.29951 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0324 train_loss= 0.88159 val_loss= 1.29846 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0325 train_loss= 0.86751 val_loss= 1.29737 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0326 train_loss= 0.82516 val_loss= 1.29625 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0327 train_loss= 0.86968 val_loss= 1.29511 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0328 train_loss= 0.87495 val_loss= 1.29402 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0329 train_loss= 0.89017 val_loss= 1.29307 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0330 train_loss= 0.86645 val_loss= 1.29225 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0331 train_loss= 0.88897 val_loss= 1.29138 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0332 train_loss= 0.90344 val_loss= 1.29053 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0333 train_loss= 0.87426 val_loss= 1.28963 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0334 train_loss= 0.83108 val_loss= 1.28876 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0335 train_loss= 0.82455 val_loss= 1.28778 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0336 train_loss= 0.79751 val_loss= 1.28683 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0337 train_loss= 0.82739 val_loss= 1.28595 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0338 train_loss= 0.82628 val_loss= 1.28512 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0339 train_loss= 0.85373 val_loss= 1.28440 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0340 train_loss= 0.82231 val_loss= 1.28358 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0341 train_loss= 0.83327 val_loss= 1.28287 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0342 train_loss= 0.84585 val_loss= 1.28216 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0343 train_loss= 0.78220 val_loss= 1.28141 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.68300\n",
      "Epoch: 0344 train_loss= 0.83121 val_loss= 1.28062 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.68300\n",
      "Epoch: 0345 train_loss= 0.80744 val_loss= 1.27985 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.68300\n",
      "Epoch: 0346 train_loss= 0.81040 val_loss= 1.27910 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.68300\n",
      "Epoch: 0347 train_loss= 0.85043 val_loss= 1.27835 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.68300\n",
      "Epoch: 0348 train_loss= 0.85092 val_loss= 1.27770 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0349 train_loss= 0.76384 val_loss= 1.27704 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0350 train_loss= 0.82523 val_loss= 1.27636 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.68300\n",
      "Epoch: 0351 train_loss= 0.79829 val_loss= 1.27562 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0352 train_loss= 0.81482 val_loss= 1.27493 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0353 train_loss= 0.81384 val_loss= 1.27421 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0354 train_loss= 0.81059 val_loss= 1.27337 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0355 train_loss= 0.79679 val_loss= 1.27260 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0356 train_loss= 0.85848 val_loss= 1.27190 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0357 train_loss= 0.84361 val_loss= 1.27107 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.68300\n",
      "Epoch: 0358 train_loss= 0.82543 val_loss= 1.27021 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0359 train_loss= 0.81578 val_loss= 1.26929 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0360 train_loss= 0.82216 val_loss= 1.26836 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.68300\n",
      "Epoch: 0361 train_loss= 0.77561 val_loss= 1.26745 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0362 train_loss= 0.82745 val_loss= 1.26654 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0363 train_loss= 0.80909 val_loss= 1.26566 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0364 train_loss= 0.82460 val_loss= 1.26473 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0365 train_loss= 0.78713 val_loss= 1.26383 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0366 train_loss= 0.74416 val_loss= 1.26305 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0367 train_loss= 0.76165 val_loss= 1.26232 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0368 train_loss= 0.75903 val_loss= 1.26155 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0369 train_loss= 0.81139 val_loss= 1.26073 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0370 train_loss= 0.76796 val_loss= 1.25994 train_acc= 0.71400 val_acc= 0.71400 test_acc= 0.67900\n",
      "Epoch: 0371 train_loss= 0.80178 val_loss= 1.25925 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.67900\n",
      "Epoch: 0372 train_loss= 0.84691 val_loss= 1.25857 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.67900\n",
      "Epoch: 0373 train_loss= 0.77236 val_loss= 1.25792 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.67900\n",
      "Epoch: 0374 train_loss= 0.76130 val_loss= 1.25728 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.67900\n",
      "Epoch: 0375 train_loss= 0.80931 val_loss= 1.25654 train_acc= 0.71200 val_acc= 0.71200 test_acc= 0.67900\n",
      "Epoch: 0376 train_loss= 0.78392 val_loss= 1.25584 train_acc= 0.71000 val_acc= 0.71000 test_acc= 0.67900\n",
      "Epoch: 0377 train_loss= 0.78338 val_loss= 1.25522 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0378 train_loss= 0.75307 val_loss= 1.25457 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0379 train_loss= 0.72163 val_loss= 1.25394 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0380 train_loss= 0.74252 val_loss= 1.25313 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0381 train_loss= 0.81287 val_loss= 1.25236 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0382 train_loss= 0.80916 val_loss= 1.25154 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0383 train_loss= 0.77227 val_loss= 1.25066 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0384 train_loss= 0.75069 val_loss= 1.24980 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0385 train_loss= 0.77059 val_loss= 1.24893 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0386 train_loss= 0.82004 val_loss= 1.24813 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0387 train_loss= 0.79958 val_loss= 1.24734 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0388 train_loss= 0.74729 val_loss= 1.24660 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0389 train_loss= 0.72584 val_loss= 1.24589 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0390 train_loss= 0.77861 val_loss= 1.24523 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0391 train_loss= 0.72725 val_loss= 1.24453 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0392 train_loss= 0.81126 val_loss= 1.24390 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0393 train_loss= 0.85024 val_loss= 1.24341 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0394 train_loss= 0.73433 val_loss= 1.24288 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0395 train_loss= 0.73741 val_loss= 1.24234 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0396 train_loss= 0.75358 val_loss= 1.24185 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0397 train_loss= 0.80748 val_loss= 1.24139 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0398 train_loss= 0.74340 val_loss= 1.24094 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0399 train_loss= 0.76716 val_loss= 1.24040 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0400 train_loss= 0.76995 val_loss= 1.23994 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0401 train_loss= 0.71938 val_loss= 1.23941 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0402 train_loss= 0.72439 val_loss= 1.23889 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0403 train_loss= 0.75283 val_loss= 1.23833 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0404 train_loss= 0.71534 val_loss= 1.23775 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0405 train_loss= 0.70012 val_loss= 1.23717 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0406 train_loss= 0.75837 val_loss= 1.23652 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0407 train_loss= 0.70108 val_loss= 1.23592 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0408 train_loss= 0.76982 val_loss= 1.23534 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0409 train_loss= 0.72969 val_loss= 1.23477 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0410 train_loss= 0.75029 val_loss= 1.23423 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0411 train_loss= 0.78746 val_loss= 1.23367 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0412 train_loss= 0.73804 val_loss= 1.23301 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0413 train_loss= 0.77559 val_loss= 1.23231 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0414 train_loss= 0.71835 val_loss= 1.23160 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0415 train_loss= 0.76312 val_loss= 1.23101 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0416 train_loss= 0.76854 val_loss= 1.23042 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0417 train_loss= 0.74258 val_loss= 1.22988 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0418 train_loss= 0.72419 val_loss= 1.22932 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0419 train_loss= 0.80080 val_loss= 1.22873 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0420 train_loss= 0.71091 val_loss= 1.22813 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0421 train_loss= 0.72967 val_loss= 1.22764 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0422 train_loss= 0.71962 val_loss= 1.22712 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0423 train_loss= 0.69498 val_loss= 1.22657 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0424 train_loss= 0.72847 val_loss= 1.22605 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0425 train_loss= 0.74683 val_loss= 1.22546 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0426 train_loss= 0.76922 val_loss= 1.22485 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0427 train_loss= 0.74170 val_loss= 1.22425 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0428 train_loss= 0.72149 val_loss= 1.22370 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0429 train_loss= 0.77102 val_loss= 1.22319 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0430 train_loss= 0.74112 val_loss= 1.22263 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0431 train_loss= 0.69227 val_loss= 1.22208 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0432 train_loss= 0.68606 val_loss= 1.22163 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0433 train_loss= 0.73200 val_loss= 1.22117 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0434 train_loss= 0.73340 val_loss= 1.22075 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0435 train_loss= 0.72578 val_loss= 1.22038 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0436 train_loss= 0.71908 val_loss= 1.21984 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0437 train_loss= 0.75129 val_loss= 1.21939 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0438 train_loss= 0.79015 val_loss= 1.21902 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0439 train_loss= 0.71133 val_loss= 1.21857 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0440 train_loss= 0.67833 val_loss= 1.21807 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0441 train_loss= 0.76977 val_loss= 1.21761 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0442 train_loss= 0.67200 val_loss= 1.21715 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0443 train_loss= 0.68900 val_loss= 1.21662 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0444 train_loss= 0.78473 val_loss= 1.21619 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0445 train_loss= 0.73928 val_loss= 1.21587 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0446 train_loss= 0.71049 val_loss= 1.21564 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0447 train_loss= 0.72680 val_loss= 1.21539 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.67900\n",
      "Epoch: 0448 train_loss= 0.74145 val_loss= 1.21530 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0449 train_loss= 0.71279 val_loss= 1.21531 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0450 train_loss= 0.72579 val_loss= 1.21522 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0451 train_loss= 0.69309 val_loss= 1.21511 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0452 train_loss= 0.67406 val_loss= 1.21502 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0453 train_loss= 0.68503 val_loss= 1.21500 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0454 train_loss= 0.69710 val_loss= 1.21488 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67900\n",
      "Epoch: 0455 train_loss= 0.69911 val_loss= 1.21468 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0456 train_loss= 0.66839 val_loss= 1.21449 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0457 train_loss= 0.72560 val_loss= 1.21429 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0458 train_loss= 0.70389 val_loss= 1.21409 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.67900\n",
      "Epoch: 0459 train_loss= 0.71237 val_loss= 1.21390 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67900\n",
      "Epoch: 0460 train_loss= 0.73966 val_loss= 1.21361 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.67900\n",
      "Epoch: 0461 train_loss= 0.75124 val_loss= 1.21334 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67900\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "##GCN_dropedge(0.2)\n",
    "\n",
    "\n",
    "from config import args\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import *\n",
    "from models import GCN_dropedge\n",
    "from metrics import *\n",
    "\n",
    "# Settings\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "args.dataset=dataset_name\n",
    "args.dropout=0.2\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "\n",
    "features = preprocess_features(features)\n",
    "\n",
    "model = GCN_dropedge(input_dim=features.shape[1], output_dim=y_train.shape[1], adj=adj_tensor)\n",
    "\n",
    "\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=tf.float32)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=tf.float32)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=tf.float32)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "\n",
    "curr_step = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model.call((features_tensor),training=True)\n",
    "        cross_loss = masked_softmax_cross_entropy(output, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        loss = cross_loss + args.weight_decay*lossL2\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call((features_tensor), training=False)\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_test_acc = test_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        # Print results\n",
    "\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(val_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch: 0001 train_loss= 1.79149 val_loss= 1.79074 train_acc= 0.37800 val_acc= 0.37800 test_acc= 0.35700\n",
      "Epoch: 0002 train_loss= 1.78846 val_loss= 1.78982 train_acc= 0.48600 val_acc= 0.48600 test_acc= 0.46100\n",
      "Epoch: 0003 train_loss= 1.78532 val_loss= 1.78882 train_acc= 0.54600 val_acc= 0.54600 test_acc= 0.52600\n",
      "Epoch: 0004 train_loss= 1.78208 val_loss= 1.78773 train_acc= 0.60400 val_acc= 0.60400 test_acc= 0.57400\n",
      "Epoch: 0005 train_loss= 1.77869 val_loss= 1.78651 train_acc= 0.62800 val_acc= 0.62800 test_acc= 0.60600\n",
      "Epoch: 0006 train_loss= 1.77509 val_loss= 1.78517 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.62900\n",
      "Epoch: 0007 train_loss= 1.77125 val_loss= 1.78370 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.64000\n",
      "Epoch: 0008 train_loss= 1.76715 val_loss= 1.78209 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.65800\n",
      "Epoch: 0009 train_loss= 1.76278 val_loss= 1.78035 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.66100\n",
      "Epoch: 0010 train_loss= 1.75813 val_loss= 1.77849 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66700\n",
      "Epoch: 0011 train_loss= 1.75322 val_loss= 1.77651 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66700\n",
      "Epoch: 0012 train_loss= 1.74803 val_loss= 1.77442 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66700\n",
      "Epoch: 0013 train_loss= 1.74259 val_loss= 1.77224 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66700\n",
      "Epoch: 0014 train_loss= 1.73688 val_loss= 1.76996 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.67600\n",
      "Epoch: 0015 train_loss= 1.73091 val_loss= 1.76759 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.67600\n",
      "Epoch: 0016 train_loss= 1.72469 val_loss= 1.76513 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.67600\n",
      "Epoch: 0017 train_loss= 1.71822 val_loss= 1.76259 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.67200\n",
      "Epoch: 0018 train_loss= 1.71151 val_loss= 1.75997 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.67200\n",
      "Epoch: 0019 train_loss= 1.70456 val_loss= 1.75727 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67100\n",
      "Epoch: 0020 train_loss= 1.69737 val_loss= 1.75449 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67100\n",
      "Epoch: 0021 train_loss= 1.68994 val_loss= 1.75162 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.67100\n",
      "Epoch: 0022 train_loss= 1.68228 val_loss= 1.74867 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.66700\n",
      "Epoch: 0023 train_loss= 1.67439 val_loss= 1.74564 train_acc= 0.70800 val_acc= 0.70800 test_acc= 0.66700\n",
      "Epoch: 0024 train_loss= 1.66627 val_loss= 1.74254 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0025 train_loss= 1.65793 val_loss= 1.73937 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0026 train_loss= 1.64936 val_loss= 1.73611 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0027 train_loss= 1.64058 val_loss= 1.73278 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0028 train_loss= 1.63157 val_loss= 1.72936 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0029 train_loss= 1.62234 val_loss= 1.72587 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66700\n",
      "Epoch: 0030 train_loss= 1.61288 val_loss= 1.72230 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0031 train_loss= 1.60321 val_loss= 1.71865 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0032 train_loss= 1.59333 val_loss= 1.71492 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0033 train_loss= 1.58322 val_loss= 1.71112 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0034 train_loss= 1.57289 val_loss= 1.70723 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0035 train_loss= 1.56235 val_loss= 1.70327 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0036 train_loss= 1.55159 val_loss= 1.69923 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0037 train_loss= 1.54062 val_loss= 1.69511 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0038 train_loss= 1.52942 val_loss= 1.69091 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0039 train_loss= 1.51802 val_loss= 1.68663 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66700\n",
      "Epoch: 0040 train_loss= 1.50640 val_loss= 1.68227 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0041 train_loss= 1.49457 val_loss= 1.67784 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0042 train_loss= 1.48254 val_loss= 1.67333 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0043 train_loss= 1.47030 val_loss= 1.66874 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0044 train_loss= 1.45785 val_loss= 1.66407 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0045 train_loss= 1.44520 val_loss= 1.65932 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0046 train_loss= 1.43235 val_loss= 1.65449 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0047 train_loss= 1.41932 val_loss= 1.64959 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0048 train_loss= 1.40608 val_loss= 1.64462 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0049 train_loss= 1.39267 val_loss= 1.63958 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0050 train_loss= 1.37906 val_loss= 1.63446 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66700\n",
      "Epoch: 0051 train_loss= 1.36530 val_loss= 1.62928 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0052 train_loss= 1.35134 val_loss= 1.62401 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0053 train_loss= 1.33724 val_loss= 1.61869 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0054 train_loss= 1.32297 val_loss= 1.61330 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0055 train_loss= 1.30854 val_loss= 1.60784 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0056 train_loss= 1.29398 val_loss= 1.60232 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0057 train_loss= 1.27927 val_loss= 1.59674 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0058 train_loss= 1.26443 val_loss= 1.59110 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0059 train_loss= 1.24946 val_loss= 1.58540 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0060 train_loss= 1.23438 val_loss= 1.57964 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0061 train_loss= 1.21918 val_loss= 1.57383 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0062 train_loss= 1.20388 val_loss= 1.56798 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0063 train_loss= 1.18849 val_loss= 1.56207 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0064 train_loss= 1.17302 val_loss= 1.55610 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0065 train_loss= 1.15746 val_loss= 1.55010 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0066 train_loss= 1.14184 val_loss= 1.54406 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0067 train_loss= 1.12617 val_loss= 1.53797 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0068 train_loss= 1.11044 val_loss= 1.53186 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0069 train_loss= 1.09467 val_loss= 1.52570 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0070 train_loss= 1.07888 val_loss= 1.51952 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0071 train_loss= 1.06307 val_loss= 1.51332 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0072 train_loss= 1.04725 val_loss= 1.50709 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0073 train_loss= 1.03143 val_loss= 1.50083 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0074 train_loss= 1.01562 val_loss= 1.49456 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0075 train_loss= 0.99982 val_loss= 1.48829 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0076 train_loss= 0.98407 val_loss= 1.48200 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0077 train_loss= 0.96836 val_loss= 1.47569 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0078 train_loss= 0.95268 val_loss= 1.46938 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0079 train_loss= 0.93708 val_loss= 1.46306 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0080 train_loss= 0.92153 val_loss= 1.45676 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0081 train_loss= 0.90606 val_loss= 1.45047 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0082 train_loss= 0.89070 val_loss= 1.44418 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0083 train_loss= 0.87542 val_loss= 1.43791 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0084 train_loss= 0.86025 val_loss= 1.43165 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0085 train_loss= 0.84519 val_loss= 1.42540 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0086 train_loss= 0.83025 val_loss= 1.41918 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0087 train_loss= 0.81544 val_loss= 1.41299 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0088 train_loss= 0.80078 val_loss= 1.40684 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0089 train_loss= 0.78625 val_loss= 1.40070 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0090 train_loss= 0.77188 val_loss= 1.39460 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0091 train_loss= 0.75766 val_loss= 1.38855 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0092 train_loss= 0.74361 val_loss= 1.38253 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0093 train_loss= 0.72973 val_loss= 1.37656 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0094 train_loss= 0.71603 val_loss= 1.37062 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0095 train_loss= 0.70250 val_loss= 1.36473 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0096 train_loss= 0.68916 val_loss= 1.35891 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0097 train_loss= 0.67601 val_loss= 1.35314 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0098 train_loss= 0.66305 val_loss= 1.34742 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0099 train_loss= 0.65028 val_loss= 1.34176 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0100 train_loss= 0.63772 val_loss= 1.33615 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.66700\n",
      "Epoch: 0101 train_loss= 0.62535 val_loss= 1.33060 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0102 train_loss= 0.61319 val_loss= 1.32513 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0103 train_loss= 0.60123 val_loss= 1.31972 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0104 train_loss= 0.58950 val_loss= 1.31438 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0105 train_loss= 0.57794 val_loss= 1.30913 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0106 train_loss= 0.56662 val_loss= 1.30392 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0107 train_loss= 0.55549 val_loss= 1.29880 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.66700\n",
      "Epoch: 0108 train_loss= 0.54458 val_loss= 1.29375 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.66700\n",
      "Epoch: 0109 train_loss= 0.53387 val_loss= 1.28878 train_acc= 0.70600 val_acc= 0.70600 test_acc= 0.66700\n",
      "Epoch: 0110 train_loss= 0.52337 val_loss= 1.28387 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0111 train_loss= 0.51308 val_loss= 1.27905 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0112 train_loss= 0.50300 val_loss= 1.27430 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0113 train_loss= 0.49311 val_loss= 1.26962 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0114 train_loss= 0.48344 val_loss= 1.26502 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0115 train_loss= 0.47396 val_loss= 1.26051 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0116 train_loss= 0.46469 val_loss= 1.25607 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0117 train_loss= 0.45561 val_loss= 1.25171 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0118 train_loss= 0.44672 val_loss= 1.24743 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0119 train_loss= 0.43804 val_loss= 1.24322 train_acc= 0.70000 val_acc= 0.70000 test_acc= 0.66700\n",
      "Epoch: 0120 train_loss= 0.42954 val_loss= 1.23909 train_acc= 0.70200 val_acc= 0.70200 test_acc= 0.66700\n",
      "Epoch: 0121 train_loss= 0.42123 val_loss= 1.23505 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Epoch: 0122 train_loss= 0.41311 val_loss= 1.23109 train_acc= 0.70400 val_acc= 0.70400 test_acc= 0.66700\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "##GCN (未攻击)\n",
    "\n",
    "from config import args\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import *\n",
    "from models import GCN_dropedge\n",
    "from metrics import *\n",
    "\n",
    "# Settings\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "args.dataset=dataset_name\n",
    "args.dropout=0.0\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "#adj=modified_adj\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "\n",
    "features = preprocess_features(features)\n",
    "\n",
    "model = GCN_dropedge(input_dim=features.shape[1], output_dim=y_train.shape[1], adj=adj_tensor)\n",
    "\n",
    "\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=tf.float32)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=tf.float32)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=tf.float32)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "\n",
    "curr_step = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model.call((features_tensor),training=True)\n",
    "        cross_loss = masked_softmax_cross_entropy(output, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        loss = cross_loss# + args.weight_decay*lossL2\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call((features_tensor), training=False)\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_test_acc = test_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        # Print results\n",
    "\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(val_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
