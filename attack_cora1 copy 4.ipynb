{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsp/ljx/PTDNet/utils.py:221: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n",
      "Perturbing graph:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.694111704826355\n",
      "GCN acc on unlabled data: 0.668\n",
      "attack loss: 1.6872614622116089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  10%|█         | 1/10 [07:06<1:03:55, 426.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.6943415403366089\n",
      "GCN acc on unlabled data: 0.661\n",
      "attack loss: 1.6871219873428345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  20%|██        | 2/10 [14:06<56:24, 423.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.694126009941101\n",
      "GCN acc on unlabled data: 0.689\n",
      "attack loss: 1.6870925426483154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  30%|███       | 3/10 [21:04<49:02, 420.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7058510780334473\n",
      "GCN acc on unlabled data: 0.68\n",
      "attack loss: 1.6988378763198853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  40%|████      | 4/10 [28:00<41:53, 418.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7042115926742554\n",
      "GCN acc on unlabled data: 0.674\n",
      "attack loss: 1.6977579593658447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  50%|█████     | 5/10 [35:01<34:57, 419.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.6960687637329102\n",
      "GCN acc on unlabled data: 0.656\n",
      "attack loss: 1.6864932775497437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  60%|██████    | 6/10 [42:09<28:10, 422.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7054184675216675\n",
      "GCN acc on unlabled data: 0.673\n",
      "attack loss: 1.6976879835128784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  70%|███████   | 7/10 [49:19<21:14, 424.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7065964937210083\n",
      "GCN acc on unlabled data: 0.683\n",
      "attack loss: 1.7003552913665771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  80%|████████  | 8/10 [56:32<14:15, 427.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7052613496780396\n",
      "GCN acc on unlabled data: 0.652\n",
      "attack loss: 1.6984875202178955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph:  90%|█████████ | 9/10 [1:03:45<07:09, 429.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN loss on unlabled data: 1.7043148279190063\n",
      "GCN acc on unlabled data: 0.666\n",
      "attack loss: 1.6973055601119995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing graph: 100%|██████████| 10/10 [1:10:46<00:00, 424.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Settings\n",
    "dataset_name='citeseer'\n",
    "args.dataset=dataset_name\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features_tmp=features.copy()\n",
    "features = preprocess_features(features).A\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "from deeprobust.graph.data import Dataset\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.global_attack import Metattack\n",
    "# Setup Surrogate model\n",
    "idx_train=np.array(np.where(train_mask==1)).tolist()[0]\n",
    "idx_val=np.array(np.where(val_mask==1)).tolist()[0]\n",
    "idx_unlabeled=np.array(np.where(test_mask==1)).tolist()[0]\n",
    "surrogate = GCN(nfeat=features.shape[1], nclass=single_label.max().item()+1,\n",
    "                nhid=256, dropout=0, with_relu=False, with_bias=False, device='cpu').to('cpu')\n",
    "surrogate.fit(features, adj, single_label, idx_train, idx_val, patience=100)\n",
    "# Setup Attack Model\n",
    "model = Metattack(surrogate, nnodes=adj.shape[0], feature_shape=features.shape,\n",
    "        attack_structure=True, attack_features=False, device='cpu', lambda_=0).to('cpu')\n",
    "# Attack\n",
    "model.attack(features, adj, single_label, idx_train, idx_unlabeled, n_perturbations=10, ll_constraint=False)\n",
    "modified_adj = model.modified_adj\n",
    "# print(adj)\n",
    "# print(\"shiy\")\n",
    "# print(modified_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_adj=sp.csr_array(modified_adj.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsp/ljx/PTDNet/utils.py:221: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f2c34500f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f2c34500f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "edge_vol 4651.329\n",
      "Epoch: 0001 train_loss= 1.79180 val_loss= 1.79106 train_acc= 0.45833 val_acc= 0.27600 best_val_acc_trail= 0.27600 test_acc= 0.26900\n",
      "time  9.325296640396118\n",
      "edge_vol 4639.838\n",
      "Epoch: 0002 train_loss= 1.78934 val_loss= 1.79031 train_acc= 0.74167 val_acc= 0.33600 best_val_acc_trail= 0.33600 test_acc= 0.35000\n",
      "time  11.080352544784546\n",
      "edge_vol 4627.9053\n",
      "Epoch: 0003 train_loss= 1.78680 val_loss= 1.78953 train_acc= 0.86667 val_acc= 0.39800 best_val_acc_trail= 0.39800 test_acc= 0.41000\n",
      "time  12.685429096221924\n",
      "edge_vol 4615.549\n",
      "Epoch: 0004 train_loss= 1.78426 val_loss= 1.78870 train_acc= 0.90833 val_acc= 0.43800 best_val_acc_trail= 0.43800 test_acc= 0.46800\n",
      "time  14.305488109588623\n",
      "edge_vol 4602.8545\n",
      "Epoch: 0005 train_loss= 1.78158 val_loss= 1.78781 train_acc= 0.91667 val_acc= 0.48400 best_val_acc_trail= 0.48400 test_acc= 0.51000\n",
      "time  16.12254047393799\n",
      "edge_vol 4589.9043\n",
      "Epoch: 0006 train_loss= 1.77890 val_loss= 1.78684 train_acc= 0.92500 val_acc= 0.51600 best_val_acc_trail= 0.51600 test_acc= 0.52600\n",
      "time  17.954367637634277\n",
      "edge_vol 4576.787\n",
      "Epoch: 0007 train_loss= 1.77612 val_loss= 1.78577 train_acc= 0.94167 val_acc= 0.54600 best_val_acc_trail= 0.54600 test_acc= 0.55500\n",
      "time  19.593621730804443\n",
      "edge_vol 4563.5474\n",
      "Epoch: 0008 train_loss= 1.77318 val_loss= 1.78462 train_acc= 0.94167 val_acc= 0.55800 best_val_acc_trail= 0.55800 test_acc= 0.57600\n",
      "time  21.345117330551147\n",
      "edge_vol 4550.1416\n",
      "Epoch: 0009 train_loss= 1.77001 val_loss= 1.78338 train_acc= 0.93333 val_acc= 0.56800 best_val_acc_trail= 0.56800 test_acc= 0.58700\n",
      "time  23.344911098480225\n",
      "edge_vol 4536.582\n",
      "Epoch: 0010 train_loss= 1.76634 val_loss= 1.78205 train_acc= 0.93333 val_acc= 0.58000 best_val_acc_trail= 0.58000 test_acc= 0.60100\n",
      "time  25.138846397399902\n",
      "edge_vol 4522.877\n",
      "Epoch: 0011 train_loss= 1.76314 val_loss= 1.78063 train_acc= 0.94167 val_acc= 0.59800 best_val_acc_trail= 0.59800 test_acc= 0.60900\n",
      "time  26.765376567840576\n",
      "edge_vol 4509.025\n",
      "Epoch: 0012 train_loss= 1.75940 val_loss= 1.77913 train_acc= 0.94167 val_acc= 0.61600 best_val_acc_trail= 0.61600 test_acc= 0.61700\n",
      "time  28.849676847457886\n",
      "edge_vol 4495.006\n",
      "Epoch: 0013 train_loss= 1.75551 val_loss= 1.77755 train_acc= 0.94167 val_acc= 0.63000 best_val_acc_trail= 0.63000 test_acc= 0.62800\n",
      "time  30.58564257621765\n",
      "edge_vol 4480.8467\n",
      "Epoch: 0014 train_loss= 1.75141 val_loss= 1.77591 train_acc= 0.94167 val_acc= 0.64400 best_val_acc_trail= 0.64400 test_acc= 0.63500\n",
      "time  32.201316595077515\n",
      "edge_vol 4466.549\n",
      "Epoch: 0015 train_loss= 1.74701 val_loss= 1.77421 train_acc= 0.94167 val_acc= 0.65400 best_val_acc_trail= 0.65400 test_acc= 0.63800\n",
      "time  34.01665425300598\n",
      "edge_vol 4452.0654\n",
      "Epoch: 0016 train_loss= 1.74221 val_loss= 1.77245 train_acc= 0.94167 val_acc= 0.65600 best_val_acc_trail= 0.65600 test_acc= 0.64200\n",
      "time  35.71813178062439\n",
      "edge_vol 4437.4004\n",
      "Epoch: 0017 train_loss= 1.73806 val_loss= 1.77063 train_acc= 0.94167 val_acc= 0.65400 best_val_acc_trail= 0.65600 test_acc= 0.64200\n",
      "time  37.318275451660156\n",
      "edge_vol 4422.5654\n",
      "Epoch: 0018 train_loss= 1.73319 val_loss= 1.76876 train_acc= 0.94167 val_acc= 0.65400 best_val_acc_trail= 0.65600 test_acc= 0.64200\n",
      "time  38.93859386444092\n",
      "edge_vol 4407.555\n",
      "Epoch: 0019 train_loss= 1.72786 val_loss= 1.76682 train_acc= 0.94167 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.65100\n",
      "time  40.5175621509552\n",
      "edge_vol 4392.3984\n",
      "Epoch: 0020 train_loss= 1.72304 val_loss= 1.76483 train_acc= 0.94167 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.65100\n",
      "time  42.286780834198\n",
      "edge_vol 4377.0527\n",
      "Epoch: 0021 train_loss= 1.71729 val_loss= 1.76280 train_acc= 0.94167 val_acc= 0.66000 best_val_acc_trail= 0.66000 test_acc= 0.66200\n",
      "time  44.15091609954834\n",
      "edge_vol 4361.5156\n",
      "Epoch: 0022 train_loss= 1.71223 val_loss= 1.76072 train_acc= 0.95000 val_acc= 0.65800 best_val_acc_trail= 0.66000 test_acc= 0.66200\n",
      "time  46.04298806190491\n",
      "edge_vol 4345.799\n",
      "Epoch: 0023 train_loss= 1.70578 val_loss= 1.75859 train_acc= 0.95000 val_acc= 0.66000 best_val_acc_trail= 0.66000 test_acc= 0.66200\n",
      "time  48.02743911743164\n",
      "edge_vol 4329.865\n",
      "Epoch: 0024 train_loss= 1.70048 val_loss= 1.75641 train_acc= 0.95000 val_acc= 0.66400 best_val_acc_trail= 0.66400 test_acc= 0.66700\n",
      "time  49.71754050254822\n",
      "edge_vol 4313.783\n",
      "Epoch: 0025 train_loss= 1.69473 val_loss= 1.75420 train_acc= 0.95000 val_acc= 0.66600 best_val_acc_trail= 0.66600 test_acc= 0.66800\n",
      "time  51.23197102546692\n",
      "edge_vol 4297.459\n",
      "Epoch: 0026 train_loss= 1.68746 val_loss= 1.75195 train_acc= 0.95000 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.66800\n",
      "time  52.839386224746704\n",
      "edge_vol 4280.941\n",
      "Epoch: 0027 train_loss= 1.68213 val_loss= 1.74967 train_acc= 0.95000 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.66800\n",
      "time  54.2389235496521\n",
      "edge_vol 4264.259\n",
      "Epoch: 0028 train_loss= 1.67519 val_loss= 1.74736 train_acc= 0.95000 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  55.99345111846924\n",
      "edge_vol 4247.3984\n",
      "Epoch: 0029 train_loss= 1.66872 val_loss= 1.74502 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  57.6140501499176\n",
      "edge_vol 4230.338\n",
      "Epoch: 0030 train_loss= 1.66249 val_loss= 1.74265 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  59.26579403877258\n",
      "edge_vol 4213.009\n",
      "Epoch: 0031 train_loss= 1.65465 val_loss= 1.74024 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  61.08600091934204\n",
      "edge_vol 4195.4346\n",
      "Epoch: 0032 train_loss= 1.64845 val_loss= 1.73780 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  62.733898878097534\n",
      "edge_vol 4177.636\n",
      "Epoch: 0033 train_loss= 1.64138 val_loss= 1.73532 train_acc= 0.95833 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.67500\n",
      "time  64.46661043167114\n",
      "edge_vol 4159.6035\n",
      "Epoch: 0034 train_loss= 1.63327 val_loss= 1.73280 train_acc= 0.95833 val_acc= 0.67600 best_val_acc_trail= 0.67600 test_acc= 0.67800\n",
      "time  66.15518379211426\n",
      "edge_vol 4141.2695\n",
      "Epoch: 0035 train_loss= 1.62673 val_loss= 1.73026 train_acc= 0.95833 val_acc= 0.68000 best_val_acc_trail= 0.68000 test_acc= 0.67800\n",
      "time  67.95229148864746\n",
      "edge_vol 4122.717\n",
      "Epoch: 0036 train_loss= 1.61883 val_loss= 1.72770 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  69.71542835235596\n",
      "edge_vol 4103.9517\n",
      "Epoch: 0037 train_loss= 1.61083 val_loss= 1.72512 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  71.4571099281311\n",
      "edge_vol 4085.0217\n",
      "Epoch: 0038 train_loss= 1.60405 val_loss= 1.72251 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  73.30371594429016\n",
      "edge_vol 4066.0542\n",
      "Epoch: 0039 train_loss= 1.59598 val_loss= 1.71985 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  75.08101320266724\n",
      "edge_vol 4046.9585\n",
      "Epoch: 0040 train_loss= 1.58930 val_loss= 1.71717 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  76.6393129825592\n",
      "edge_vol 4027.6494\n",
      "Epoch: 0041 train_loss= 1.58116 val_loss= 1.71445 train_acc= 0.95833 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67700\n",
      "time  78.38494110107422\n",
      "edge_vol 4008.185\n",
      "Epoch: 0042 train_loss= 1.57278 val_loss= 1.71173 train_acc= 0.95833 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.67900\n",
      "time  80.00248098373413\n",
      "edge_vol 3988.538\n",
      "Epoch: 0043 train_loss= 1.56594 val_loss= 1.70902 train_acc= 0.95833 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67900\n",
      "time  81.77821969985962\n",
      "edge_vol 3968.6455\n",
      "Epoch: 0044 train_loss= 1.55581 val_loss= 1.70628 train_acc= 0.95833 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67900\n",
      "time  83.50767803192139\n",
      "edge_vol 3948.5269\n",
      "Epoch: 0045 train_loss= 1.54735 val_loss= 1.70354 train_acc= 0.95833 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67900\n",
      "time  84.93680191040039\n",
      "edge_vol 3928.2192\n",
      "Epoch: 0046 train_loss= 1.53933 val_loss= 1.70081 train_acc= 0.95833 val_acc= 0.68800 best_val_acc_trail= 0.68800 test_acc= 0.68200\n",
      "time  86.61218547821045\n",
      "edge_vol 3907.5889\n",
      "Epoch: 0047 train_loss= 1.53062 val_loss= 1.69809 train_acc= 0.96667 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.68100\n",
      "time  88.35476851463318\n",
      "edge_vol 3886.802\n",
      "Epoch: 0048 train_loss= 1.52526 val_loss= 1.69538 train_acc= 0.96667 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.68100\n",
      "time  90.22170853614807\n",
      "edge_vol 3865.7874\n",
      "Epoch: 0049 train_loss= 1.51315 val_loss= 1.69269 train_acc= 0.96667 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  91.98945212364197\n",
      "edge_vol 3844.5781\n",
      "Epoch: 0050 train_loss= 1.50469 val_loss= 1.68999 train_acc= 0.96667 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  93.80908417701721\n",
      "edge_vol 3823.1716\n",
      "Epoch: 0051 train_loss= 1.49712 val_loss= 1.68732 train_acc= 0.96667 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  95.66755843162537\n",
      "edge_vol 3801.7642\n",
      "Epoch: 0052 train_loss= 1.48763 val_loss= 1.68466 train_acc= 0.96667 val_acc= 0.69000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  97.36104798316956\n",
      "edge_vol 3780.0977\n",
      "Epoch: 0053 train_loss= 1.48196 val_loss= 1.68205 train_acc= 0.96667 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  99.20753407478333\n",
      "edge_vol 3758.2505\n",
      "Epoch: 0054 train_loss= 1.47042 val_loss= 1.67946 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  101.02408218383789\n",
      "edge_vol 3736.3804\n",
      "Epoch: 0055 train_loss= 1.46367 val_loss= 1.67689 train_acc= 0.96667 val_acc= 0.68400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  102.96202850341797\n",
      "edge_vol 3714.2734\n",
      "Epoch: 0056 train_loss= 1.45260 val_loss= 1.67437 train_acc= 0.96667 val_acc= 0.68400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  104.60288882255554\n",
      "edge_vol 3692.1846\n",
      "Epoch: 0057 train_loss= 1.44766 val_loss= 1.67188 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  106.14528107643127\n",
      "edge_vol 3669.7812\n",
      "Epoch: 0058 train_loss= 1.43407 val_loss= 1.66942 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  107.98561096191406\n",
      "edge_vol 3647.045\n",
      "Epoch: 0059 train_loss= 1.42501 val_loss= 1.66698 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  109.79987382888794\n",
      "edge_vol 3624.2468\n",
      "Epoch: 0060 train_loss= 1.41915 val_loss= 1.66463 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  111.49346280097961\n",
      "edge_vol 3601.2534\n",
      "Epoch: 0061 train_loss= 1.41383 val_loss= 1.66232 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  113.1445038318634\n",
      "edge_vol 3577.9775\n",
      "Epoch: 0062 train_loss= 1.40298 val_loss= 1.66008 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  115.12604475021362\n",
      "edge_vol 3554.3994\n",
      "Epoch: 0063 train_loss= 1.39226 val_loss= 1.65795 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  116.91551065444946\n",
      "edge_vol 3530.6685\n",
      "Epoch: 0064 train_loss= 1.38216 val_loss= 1.65590 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  118.64313578605652\n",
      "edge_vol 3506.9014\n",
      "Epoch: 0065 train_loss= 1.37335 val_loss= 1.65395 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  120.21206259727478\n",
      "edge_vol 3483.0613\n",
      "Epoch: 0066 train_loss= 1.36642 val_loss= 1.65206 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  121.93151068687439\n",
      "edge_vol 3459.0747\n",
      "Epoch: 0067 train_loss= 1.36021 val_loss= 1.65028 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  123.64114117622375\n",
      "edge_vol 3434.982\n",
      "Epoch: 0068 train_loss= 1.35374 val_loss= 1.64858 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  125.95830750465393\n",
      "edge_vol 3410.7715\n",
      "Epoch: 0069 train_loss= 1.33838 val_loss= 1.64693 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  128.12573623657227\n",
      "edge_vol 3386.501\n",
      "Epoch: 0070 train_loss= 1.33248 val_loss= 1.64537 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  129.91654086112976\n",
      "edge_vol 3362.2588\n",
      "Epoch: 0071 train_loss= 1.32501 val_loss= 1.64390 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  131.72966861724854\n",
      "edge_vol 3337.6426\n",
      "Epoch: 0072 train_loss= 1.31935 val_loss= 1.64257 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  134.03810167312622\n",
      "edge_vol 3312.878\n",
      "Epoch: 0073 train_loss= 1.30935 val_loss= 1.64134 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  136.18300080299377\n",
      "edge_vol 3287.982\n",
      "Epoch: 0074 train_loss= 1.29814 val_loss= 1.64015 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  138.07143545150757\n",
      "edge_vol 3263.122\n",
      "Epoch: 0075 train_loss= 1.29796 val_loss= 1.63906 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  139.96430087089539\n",
      "edge_vol 3238.0479\n",
      "Epoch: 0076 train_loss= 1.28667 val_loss= 1.63812 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  142.10473251342773\n",
      "edge_vol 3212.8008\n",
      "Epoch: 0077 train_loss= 1.28253 val_loss= 1.63723 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  144.1760458946228\n",
      "edge_vol 3186.9966\n",
      "Epoch: 0078 train_loss= 1.27376 val_loss= 1.63644 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  146.5259644985199\n",
      "edge_vol 3160.9119\n",
      "Epoch: 0079 train_loss= 1.26299 val_loss= 1.63575 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  149.39235734939575\n",
      "edge_vol 3134.5444\n",
      "Epoch: 0080 train_loss= 1.25843 val_loss= 1.63521 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  151.72680759429932\n",
      "edge_vol 3107.6084\n",
      "Epoch: 0081 train_loss= 1.25324 val_loss= 1.63476 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  153.97715163230896\n",
      "edge_vol 3079.9307\n",
      "Epoch: 0082 train_loss= 1.24297 val_loss= 1.63436 train_acc= 0.99167 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  155.44225430488586\n",
      "edge_vol 3051.9355\n",
      "Epoch: 0083 train_loss= 1.23884 val_loss= 1.63391 train_acc= 0.99167 val_acc= 0.67800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  157.7686767578125\n",
      "edge_vol 3023.8071\n",
      "Epoch: 0084 train_loss= 1.22711 val_loss= 1.63349 train_acc= 0.99167 val_acc= 0.67600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  160.10145783424377\n",
      "edge_vol 2996.0283\n",
      "Epoch: 0085 train_loss= 1.22439 val_loss= 1.63303 train_acc= 0.99167 val_acc= 0.67400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  162.32533431053162\n",
      "edge_vol 2967.6519\n",
      "Epoch: 0086 train_loss= 1.22133 val_loss= 1.63261 train_acc= 0.99167 val_acc= 0.67200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  164.6939733028412\n",
      "edge_vol 2938.6929\n",
      "Epoch: 0087 train_loss= 1.20837 val_loss= 1.63226 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  166.5011990070343\n",
      "edge_vol 2909.5898\n",
      "Epoch: 0088 train_loss= 1.20664 val_loss= 1.63206 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  168.56061387062073\n",
      "edge_vol 2879.9114\n",
      "Epoch: 0089 train_loss= 1.20100 val_loss= 1.63182 train_acc= 0.99167 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  170.45098185539246\n",
      "edge_vol 2850.3755\n",
      "Epoch: 0090 train_loss= 1.19423 val_loss= 1.63157 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  172.41395115852356\n",
      "edge_vol 2820.2104\n",
      "Epoch: 0091 train_loss= 1.18792 val_loss= 1.63130 train_acc= 0.99167 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  174.31977581977844\n",
      "edge_vol 2789.3953\n",
      "Epoch: 0092 train_loss= 1.18694 val_loss= 1.63109 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  176.85491371154785\n",
      "edge_vol 2757.9292\n",
      "Epoch: 0093 train_loss= 1.17871 val_loss= 1.63088 train_acc= 1.00000 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  178.9851336479187\n",
      "edge_vol 2726.2021\n",
      "Epoch: 0094 train_loss= 1.17805 val_loss= 1.63069 train_acc= 1.00000 val_acc= 0.67200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  181.48276376724243\n",
      "edge_vol 2693.7285\n",
      "Epoch: 0095 train_loss= 1.16533 val_loss= 1.63055 train_acc= 1.00000 val_acc= 0.67400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  183.46193885803223\n",
      "edge_vol 2660.9805\n",
      "Epoch: 0096 train_loss= 1.17318 val_loss= 1.63051 train_acc= 1.00000 val_acc= 0.67400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  186.0459499359131\n",
      "edge_vol 2627.3015\n",
      "Epoch: 0097 train_loss= 1.15177 val_loss= 1.63055 train_acc= 1.00000 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  188.44851994514465\n",
      "edge_vol 2593.215\n",
      "Epoch: 0098 train_loss= 1.15578 val_loss= 1.63070 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  190.58525228500366\n",
      "edge_vol 2558.2542\n",
      "Epoch: 0099 train_loss= 1.14728 val_loss= 1.63070 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  192.22254514694214\n",
      "edge_vol 2522.1523\n",
      "Epoch: 0100 train_loss= 1.14341 val_loss= 1.63071 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  193.78504824638367\n",
      "edge_vol 2485.6826\n",
      "Epoch: 0101 train_loss= 1.14076 val_loss= 1.63070 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  196.01760721206665\n",
      "edge_vol 2448.9229\n",
      "Epoch: 0102 train_loss= 1.13817 val_loss= 1.63068 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  197.7200129032135\n",
      "edge_vol 2411.998\n",
      "Epoch: 0103 train_loss= 1.12792 val_loss= 1.63067 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  199.88130235671997\n",
      "edge_vol 2374.1604\n",
      "Epoch: 0104 train_loss= 1.11990 val_loss= 1.63072 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  201.49909663200378\n",
      "edge_vol 2335.893\n",
      "Epoch: 0105 train_loss= 1.11708 val_loss= 1.63076 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  203.13153839111328\n",
      "edge_vol 2297.3662\n",
      "Epoch: 0106 train_loss= 1.11685 val_loss= 1.63073 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  205.23507571220398\n",
      "edge_vol 2258.0325\n",
      "Epoch: 0107 train_loss= 1.10508 val_loss= 1.63072 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  207.37654995918274\n",
      "edge_vol 2218.4834\n",
      "Epoch: 0108 train_loss= 1.10665 val_loss= 1.63075 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  209.33823823928833\n",
      "edge_vol 2178.6562\n",
      "Epoch: 0109 train_loss= 1.10176 val_loss= 1.63088 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  211.59125924110413\n",
      "edge_vol 2138.679\n",
      "Epoch: 0110 train_loss= 1.09616 val_loss= 1.63093 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  213.32703948020935\n",
      "edge_vol 2098.1184\n",
      "Epoch: 0111 train_loss= 1.09554 val_loss= 1.63099 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  215.0494966506958\n",
      "edge_vol 2057.3347\n",
      "Epoch: 0112 train_loss= 1.08902 val_loss= 1.63099 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  216.806743144989\n",
      "edge_vol 2016.1255\n",
      "Epoch: 0113 train_loss= 1.08307 val_loss= 1.63095 train_acc= 1.00000 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  219.10148072242737\n",
      "edge_vol 1974.5933\n",
      "Epoch: 0114 train_loss= 1.07381 val_loss= 1.63094 train_acc= 1.00000 val_acc= 0.67000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  220.7969696521759\n",
      "edge_vol 1932.6534\n",
      "Epoch: 0115 train_loss= 1.07427 val_loss= 1.63103 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  223.02515745162964\n",
      "edge_vol 1890.9312\n",
      "Epoch: 0116 train_loss= 1.06921 val_loss= 1.63112 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  224.94625854492188\n",
      "edge_vol 1849.0002\n",
      "Epoch: 0117 train_loss= 1.07400 val_loss= 1.63120 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  226.96682739257812\n",
      "edge_vol 1806.8235\n",
      "Epoch: 0118 train_loss= 1.06381 val_loss= 1.63131 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  228.73461031913757\n",
      "edge_vol 1764.6654\n",
      "Epoch: 0119 train_loss= 1.05391 val_loss= 1.63129 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  231.09753012657166\n",
      "edge_vol 1722.7125\n",
      "Epoch: 0120 train_loss= 1.05411 val_loss= 1.63133 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  233.7889266014099\n",
      "edge_vol 1680.7216\n",
      "Epoch: 0121 train_loss= 1.04402 val_loss= 1.63125 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  235.38121581077576\n",
      "edge_vol 1639.0189\n",
      "Epoch: 0122 train_loss= 1.04631 val_loss= 1.63121 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  237.19757795333862\n",
      "edge_vol 1597.1951\n",
      "Epoch: 0123 train_loss= 1.04046 val_loss= 1.63108 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  238.79107093811035\n",
      "edge_vol 1555.5256\n",
      "Epoch: 0124 train_loss= 1.03610 val_loss= 1.63105 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  240.37499332427979\n",
      "edge_vol 1514.5841\n",
      "Epoch: 0125 train_loss= 1.03096 val_loss= 1.63095 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  242.31538820266724\n",
      "edge_vol 1473.7657\n",
      "Epoch: 0126 train_loss= 1.02621 val_loss= 1.63077 train_acc= 1.00000 val_acc= 0.66800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  243.98258805274963\n",
      "edge_vol 1433.2327\n",
      "Epoch: 0127 train_loss= 1.02600 val_loss= 1.63071 train_acc= 1.00000 val_acc= 0.66600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  245.60558223724365\n",
      "edge_vol 1392.5922\n",
      "Epoch: 0128 train_loss= 1.01036 val_loss= 1.63065 train_acc= 1.00000 val_acc= 0.66400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  247.33170437812805\n",
      "edge_vol 1352.3037\n",
      "Epoch: 0129 train_loss= 1.00963 val_loss= 1.63067 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  249.1830186843872\n",
      "edge_vol 1312.3015\n",
      "Epoch: 0130 train_loss= 1.00797 val_loss= 1.63086 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  251.0213975906372\n",
      "edge_vol 1273.0198\n",
      "Epoch: 0131 train_loss= 0.99915 val_loss= 1.63107 train_acc= 1.00000 val_acc= 0.65600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  253.58968019485474\n",
      "edge_vol 1234.361\n",
      "Epoch: 0132 train_loss= 0.99605 val_loss= 1.63128 train_acc= 1.00000 val_acc= 0.65600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  257.2605814933777\n",
      "edge_vol 1196.2798\n",
      "Epoch: 0133 train_loss= 0.99030 val_loss= 1.63132 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  259.08426880836487\n",
      "edge_vol 1158.6942\n",
      "Epoch: 0134 train_loss= 0.98399 val_loss= 1.63131 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  261.97104692459106\n",
      "edge_vol 1121.6267\n",
      "Epoch: 0135 train_loss= 0.97536 val_loss= 1.63137 train_acc= 1.00000 val_acc= 0.66200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  263.6888291835785\n",
      "edge_vol 1085.6317\n",
      "Epoch: 0136 train_loss= 0.97206 val_loss= 1.63115 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  266.1042950153351\n",
      "edge_vol 1049.834\n",
      "Epoch: 0137 train_loss= 0.97161 val_loss= 1.63094 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  267.4865779876709\n",
      "edge_vol 1014.46045\n",
      "Epoch: 0138 train_loss= 0.96583 val_loss= 1.63062 train_acc= 1.00000 val_acc= 0.66000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  269.49570965766907\n",
      "edge_vol 979.9818\n",
      "Epoch: 0139 train_loss= 0.95222 val_loss= 1.63017 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  274.1466736793518\n",
      "edge_vol 946.0444\n",
      "Epoch: 0140 train_loss= 0.94679 val_loss= 1.62951 train_acc= 1.00000 val_acc= 0.65400 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  278.117068529129\n",
      "edge_vol 912.7225\n",
      "Epoch: 0141 train_loss= 0.94387 val_loss= 1.62869 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  280.46049284935\n",
      "edge_vol 879.9584\n",
      "Epoch: 0142 train_loss= 0.93414 val_loss= 1.62748 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  283.2376868724823\n",
      "edge_vol 847.7854\n",
      "Epoch: 0143 train_loss= 0.92365 val_loss= 1.62615 train_acc= 1.00000 val_acc= 0.65600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  286.27937388420105\n",
      "edge_vol 816.38086\n",
      "Epoch: 0144 train_loss= 0.91504 val_loss= 1.62469 train_acc= 1.00000 val_acc= 0.65800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  288.6550362110138\n",
      "edge_vol 785.73474\n",
      "Epoch: 0145 train_loss= 0.91071 val_loss= 1.62328 train_acc= 1.00000 val_acc= 0.65600 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  291.70895981788635\n",
      "edge_vol 755.9978\n",
      "Epoch: 0146 train_loss= 0.90108 val_loss= 1.62187 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  295.731032371521\n",
      "edge_vol 727.1162\n",
      "Epoch: 0147 train_loss= 0.89039 val_loss= 1.62042 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  299.5999059677124\n",
      "edge_vol 698.96606\n",
      "Epoch: 0148 train_loss= 0.89315 val_loss= 1.61914 train_acc= 1.00000 val_acc= 0.65200 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  303.25101137161255\n",
      "edge_vol 671.464\n",
      "Epoch: 0149 train_loss= 0.88041 val_loss= 1.61792 train_acc= 1.00000 val_acc= 0.65000 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "time  307.5829424858093\n",
      "edge_vol 644.77356\n",
      "Epoch: 0150 train_loss= 0.87696 val_loss= 1.61684 train_acc= 1.00000 val_acc= 0.64800 best_val_acc_trail= 0.69200 test_acc= 0.68200\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Settings\n",
    "args.dataset=dataset_name\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "import time\n",
    "begin = time.time()\n",
    "\n",
    "model = PTDNetGCN(input_dim=features.shape[1], output_dim=y_train.shape[1])\n",
    "model.set_fea_adj(np.array(range(adj.shape[0])), features_tensor, adj_tensor)\n",
    "\n",
    "best_epoch = 0\n",
    "curr_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    temperature = max(0.05,args.init_temperature * pow(args.temperature_decay, epoch))\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = []\n",
    "        for l in range(args.outL):\n",
    "            output = model.call(temperature,training=True)\n",
    "            preds.append(tf.expand_dims(output,0))\n",
    "        all_preds = tf.concat(preds,axis=0)\n",
    "        mean_preds = tf.reduce_mean(preds,axis=0)\n",
    "        consistency_loss = tf.nn.l2_loss(mean_preds-all_preds)\n",
    "\n",
    "        cross_loss = masked_softmax_cross_entropy(mean_preds, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        lossl0 = model.lossl0(temperature)\n",
    "        #nuclear = model.my_nuclear()\n",
    "        nuclear = model.nuclear()\n",
    "        loss = cross_loss + args.weight_decay*lossL2 + args.lambda1*lossl0 + args.lambda3*nuclear + args.coff_consis*consistency_loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call(None, training=False)\n",
    "    edges_volumn = tf.reduce_sum(model.maskes[0])\n",
    "    print('edge_vol',edges_volumn.numpy())\n",
    "\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_epoch = epoch\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        if val_acc>best_val_acc_trail:\n",
    "            best_test_acc = test_acc\n",
    "            best_val_acc_trail = val_acc\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\"best_val_acc_trail=\", \"{:.5f}\".format(best_val_acc_trail),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))\n",
    "\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    print('time ',(end-begin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_vol 4669.12\n",
      "Epoch: 0001 train_loss= 1.79200 val_loss= 1.79143 train_acc= 0.38333 val_acc= 0.20000 best_val_acc_trail= 0.20000 test_acc= 0.15900\n",
      "time  5.212229251861572\n",
      "edge_vol 4667.5474\n",
      "Epoch: 0002 train_loss= 1.78948 val_loss= 1.79066 train_acc= 0.70000 val_acc= 0.29600 best_val_acc_trail= 0.29600 test_acc= 0.24800\n",
      "time  6.165233373641968\n",
      "edge_vol 4663.0386\n",
      "Epoch: 0003 train_loss= 1.78699 val_loss= 1.78988 train_acc= 0.88333 val_acc= 0.35600 best_val_acc_trail= 0.35600 test_acc= 0.32400\n",
      "time  7.438089847564697\n",
      "edge_vol 4657.0166\n",
      "Epoch: 0004 train_loss= 1.78444 val_loss= 1.78906 train_acc= 0.93333 val_acc= 0.41000 best_val_acc_trail= 0.41000 test_acc= 0.39600\n",
      "time  8.465310096740723\n",
      "edge_vol 4650.0146\n",
      "Epoch: 0005 train_loss= 1.78179 val_loss= 1.78818 train_acc= 0.95000 val_acc= 0.46800 best_val_acc_trail= 0.46800 test_acc= 0.44500\n",
      "time  9.648217678070068\n",
      "edge_vol 4642.327\n",
      "Epoch: 0006 train_loss= 1.77923 val_loss= 1.78723 train_acc= 0.95000 val_acc= 0.51200 best_val_acc_trail= 0.51200 test_acc= 0.49300\n",
      "time  10.41557765007019\n",
      "edge_vol 4634.1562\n",
      "Epoch: 0007 train_loss= 1.77637 val_loss= 1.78619 train_acc= 0.94167 val_acc= 0.54200 best_val_acc_trail= 0.54200 test_acc= 0.52500\n",
      "time  11.386597394943237\n",
      "edge_vol 4625.6475\n",
      "Epoch: 0008 train_loss= 1.77326 val_loss= 1.78507 train_acc= 0.94167 val_acc= 0.55800 best_val_acc_trail= 0.55800 test_acc= 0.54200\n",
      "time  12.387759447097778\n",
      "edge_vol 4616.8516\n",
      "Epoch: 0009 train_loss= 1.77007 val_loss= 1.78386 train_acc= 0.95000 val_acc= 0.59000 best_val_acc_trail= 0.59000 test_acc= 0.56700\n",
      "time  13.491902351379395\n",
      "edge_vol 4607.7603\n",
      "Epoch: 0010 train_loss= 1.76680 val_loss= 1.78255 train_acc= 0.95000 val_acc= 0.60400 best_val_acc_trail= 0.60400 test_acc= 0.57900\n",
      "time  14.54421591758728\n",
      "edge_vol 4598.4404\n",
      "Epoch: 0011 train_loss= 1.76341 val_loss= 1.78117 train_acc= 0.95000 val_acc= 0.61400 best_val_acc_trail= 0.61400 test_acc= 0.59200\n",
      "time  15.513126611709595\n",
      "edge_vol 4588.9053\n",
      "Epoch: 0012 train_loss= 1.75957 val_loss= 1.77971 train_acc= 0.95000 val_acc= 0.62200 best_val_acc_trail= 0.62200 test_acc= 0.59600\n",
      "time  16.60332441329956\n",
      "edge_vol 4579.1816\n",
      "Epoch: 0013 train_loss= 1.75559 val_loss= 1.77818 train_acc= 0.95000 val_acc= 0.62600 best_val_acc_trail= 0.62600 test_acc= 0.60200\n",
      "time  17.710696697235107\n",
      "edge_vol 4569.296\n",
      "Epoch: 0014 train_loss= 1.75152 val_loss= 1.77659 train_acc= 0.95000 val_acc= 0.62200 best_val_acc_trail= 0.62600 test_acc= 0.60200\n",
      "time  18.738338947296143\n",
      "edge_vol 4559.2773\n",
      "Epoch: 0015 train_loss= 1.74724 val_loss= 1.77495 train_acc= 0.95000 val_acc= 0.63400 best_val_acc_trail= 0.63400 test_acc= 0.60700\n",
      "time  20.044585466384888\n",
      "edge_vol 4549.117\n",
      "Epoch: 0016 train_loss= 1.74265 val_loss= 1.77325 train_acc= 0.95000 val_acc= 0.63400 best_val_acc_trail= 0.63400 test_acc= 0.60700\n",
      "time  21.238510370254517\n",
      "edge_vol 4538.82\n",
      "Epoch: 0017 train_loss= 1.73815 val_loss= 1.77149 train_acc= 0.95000 val_acc= 0.63800 best_val_acc_trail= 0.63800 test_acc= 0.61500\n",
      "time  22.295836210250854\n",
      "edge_vol 4528.4014\n",
      "Epoch: 0018 train_loss= 1.73326 val_loss= 1.76968 train_acc= 0.95000 val_acc= 0.63600 best_val_acc_trail= 0.63800 test_acc= 0.61500\n",
      "time  23.190420866012573\n",
      "edge_vol 4517.8633\n",
      "Epoch: 0019 train_loss= 1.72833 val_loss= 1.76782 train_acc= 0.95000 val_acc= 0.64200 best_val_acc_trail= 0.64200 test_acc= 0.62300\n",
      "time  23.96980571746826\n",
      "edge_vol 4507.2124\n",
      "Epoch: 0020 train_loss= 1.72278 val_loss= 1.76592 train_acc= 0.95833 val_acc= 0.64200 best_val_acc_trail= 0.64200 test_acc= 0.62300\n",
      "time  25.233185291290283\n",
      "edge_vol 4496.458\n",
      "Epoch: 0021 train_loss= 1.71759 val_loss= 1.76398 train_acc= 0.95833 val_acc= 0.64200 best_val_acc_trail= 0.64200 test_acc= 0.62300\n",
      "time  25.88878631591797\n",
      "edge_vol 4485.564\n",
      "Epoch: 0022 train_loss= 1.71161 val_loss= 1.76199 train_acc= 0.95833 val_acc= 0.64200 best_val_acc_trail= 0.64200 test_acc= 0.62300\n",
      "time  26.9310405254364\n",
      "edge_vol 4474.57\n",
      "Epoch: 0023 train_loss= 1.70624 val_loss= 1.75995 train_acc= 0.95833 val_acc= 0.64200 best_val_acc_trail= 0.64200 test_acc= 0.62300\n",
      "time  27.899527549743652\n",
      "edge_vol 4463.446\n",
      "Epoch: 0024 train_loss= 1.70031 val_loss= 1.75787 train_acc= 0.95833 val_acc= 0.64400 best_val_acc_trail= 0.64400 test_acc= 0.63100\n",
      "time  28.90414786338806\n",
      "edge_vol 4452.196\n",
      "Epoch: 0025 train_loss= 1.69480 val_loss= 1.75576 train_acc= 0.95833 val_acc= 0.64400 best_val_acc_trail= 0.64400 test_acc= 0.63100\n",
      "time  29.989628314971924\n",
      "edge_vol 4440.8438\n",
      "Epoch: 0026 train_loss= 1.68814 val_loss= 1.75361 train_acc= 0.95833 val_acc= 0.64200 best_val_acc_trail= 0.64400 test_acc= 0.63100\n",
      "time  30.729390382766724\n",
      "edge_vol 4429.384\n",
      "Epoch: 0027 train_loss= 1.68187 val_loss= 1.75143 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64600 test_acc= 0.63100\n",
      "time  31.729353666305542\n",
      "edge_vol 4417.878\n",
      "Epoch: 0028 train_loss= 1.67577 val_loss= 1.74921 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64600 test_acc= 0.63100\n",
      "time  32.93436026573181\n",
      "edge_vol 4406.301\n",
      "Epoch: 0029 train_loss= 1.66908 val_loss= 1.74695 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64600 test_acc= 0.63100\n",
      "time  33.99455904960632\n",
      "edge_vol 4394.621\n",
      "Epoch: 0030 train_loss= 1.66233 val_loss= 1.74467 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64600 test_acc= 0.63100\n",
      "time  35.02366518974304\n",
      "edge_vol 4382.8965\n",
      "Epoch: 0031 train_loss= 1.65599 val_loss= 1.74237 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  36.07799983024597\n",
      "edge_vol 4371.0557\n",
      "Epoch: 0032 train_loss= 1.64892 val_loss= 1.74003 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  36.927571058273315\n",
      "edge_vol 4359.2754\n",
      "Epoch: 0033 train_loss= 1.64242 val_loss= 1.73767 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  38.30104351043701\n",
      "edge_vol 4347.228\n",
      "Epoch: 0034 train_loss= 1.63498 val_loss= 1.73528 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  39.73487710952759\n",
      "edge_vol 4335.1226\n",
      "Epoch: 0035 train_loss= 1.62901 val_loss= 1.73287 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  41.03782916069031\n",
      "edge_vol 4322.9404\n",
      "Epoch: 0036 train_loss= 1.62116 val_loss= 1.73044 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  42.10092043876648\n",
      "edge_vol 4310.841\n",
      "Epoch: 0037 train_loss= 1.61327 val_loss= 1.72800 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.64800 test_acc= 0.63100\n",
      "time  43.37723445892334\n",
      "edge_vol 4299.0137\n",
      "Epoch: 0038 train_loss= 1.60509 val_loss= 1.72553 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  44.268229246139526\n",
      "edge_vol 4287.1836\n",
      "Epoch: 0039 train_loss= 1.59927 val_loss= 1.72304 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  45.47026515007019\n",
      "edge_vol 4275.2246\n",
      "Epoch: 0040 train_loss= 1.59050 val_loss= 1.72052 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  46.80217504501343\n",
      "edge_vol 4263.25\n",
      "Epoch: 0041 train_loss= 1.58491 val_loss= 1.71800 train_acc= 0.96667 val_acc= 0.64600 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  47.63340783119202\n",
      "edge_vol 4251.1245\n",
      "Epoch: 0042 train_loss= 1.57650 val_loss= 1.71544 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  48.61296987533569\n",
      "edge_vol 4239.019\n",
      "Epoch: 0043 train_loss= 1.56860 val_loss= 1.71287 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  49.702624559402466\n",
      "edge_vol 4226.8154\n",
      "Epoch: 0044 train_loss= 1.55974 val_loss= 1.71029 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  50.890995264053345\n",
      "edge_vol 4214.878\n",
      "Epoch: 0045 train_loss= 1.55360 val_loss= 1.70769 train_acc= 0.96667 val_acc= 0.64800 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  51.80137276649475\n",
      "edge_vol 4203.2505\n",
      "Epoch: 0046 train_loss= 1.54393 val_loss= 1.70506 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  52.78146767616272\n",
      "edge_vol 4191.591\n",
      "Epoch: 0047 train_loss= 1.53705 val_loss= 1.70244 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  53.745402336120605\n",
      "edge_vol 4180.033\n",
      "Epoch: 0048 train_loss= 1.52769 val_loss= 1.69980 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  54.937652587890625\n",
      "edge_vol 4168.569\n",
      "Epoch: 0049 train_loss= 1.52124 val_loss= 1.69716 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  55.93775963783264\n",
      "edge_vol 4157.255\n",
      "Epoch: 0050 train_loss= 1.51238 val_loss= 1.69453 train_acc= 0.96667 val_acc= 0.65000 best_val_acc_trail= 0.65000 test_acc= 0.63500\n",
      "time  56.75367975234985\n",
      "edge_vol 4145.8984\n",
      "Epoch: 0051 train_loss= 1.50504 val_loss= 1.69188 train_acc= 0.96667 val_acc= 0.65200 best_val_acc_trail= 0.65200 test_acc= 0.64800\n",
      "time  57.38811135292053\n",
      "edge_vol 4134.4316\n",
      "Epoch: 0052 train_loss= 1.49616 val_loss= 1.68922 train_acc= 0.96667 val_acc= 0.65400 best_val_acc_trail= 0.65400 test_acc= 0.65100\n",
      "time  58.180017948150635\n",
      "edge_vol 4122.9907\n",
      "Epoch: 0053 train_loss= 1.48775 val_loss= 1.68662 train_acc= 0.96667 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.65400\n",
      "time  59.359747648239136\n",
      "edge_vol 4111.574\n",
      "Epoch: 0054 train_loss= 1.48007 val_loss= 1.68405 train_acc= 0.97500 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.65400\n",
      "time  60.05707669258118\n",
      "edge_vol 4100.6987\n",
      "Epoch: 0055 train_loss= 1.47295 val_loss= 1.68153 train_acc= 0.97500 val_acc= 0.65800 best_val_acc_trail= 0.65800 test_acc= 0.65400\n",
      "time  61.05505657196045\n",
      "edge_vol 4090.3306\n",
      "Epoch: 0056 train_loss= 1.46363 val_loss= 1.67900 train_acc= 0.97500 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  61.69310760498047\n",
      "edge_vol 4079.8674\n",
      "Epoch: 0057 train_loss= 1.45339 val_loss= 1.67649 train_acc= 0.97500 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  62.79993748664856\n",
      "edge_vol 4070.3232\n",
      "Epoch: 0058 train_loss= 1.45032 val_loss= 1.67403 train_acc= 0.97500 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  64.20293545722961\n",
      "edge_vol 4061.2056\n",
      "Epoch: 0059 train_loss= 1.43944 val_loss= 1.67158 train_acc= 0.97500 val_acc= 0.65800 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  65.1521475315094\n",
      "edge_vol 4053.3325\n",
      "Epoch: 0060 train_loss= 1.43059 val_loss= 1.66912 train_acc= 0.97500 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  66.34000635147095\n",
      "edge_vol 4046.1377\n",
      "Epoch: 0061 train_loss= 1.42591 val_loss= 1.66672 train_acc= 0.97500 val_acc= 0.66000 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  67.21571946144104\n",
      "edge_vol 4039.0945\n",
      "Epoch: 0062 train_loss= 1.41454 val_loss= 1.66433 train_acc= 0.97500 val_acc= 0.66200 best_val_acc_trail= 0.66200 test_acc= 0.65600\n",
      "time  68.20286464691162\n",
      "edge_vol 4033.4988\n",
      "Epoch: 0063 train_loss= 1.40823 val_loss= 1.66197 train_acc= 0.97500 val_acc= 0.66400 best_val_acc_trail= 0.66400 test_acc= 0.66500\n",
      "time  69.62006282806396\n",
      "edge_vol 4029.7568\n",
      "Epoch: 0064 train_loss= 1.40209 val_loss= 1.65965 train_acc= 0.97500 val_acc= 0.66400 best_val_acc_trail= 0.66400 test_acc= 0.66500\n",
      "time  70.76488137245178\n",
      "edge_vol 4027.1855\n",
      "Epoch: 0065 train_loss= 1.39098 val_loss= 1.65738 train_acc= 0.97500 val_acc= 0.66600 best_val_acc_trail= 0.66600 test_acc= 0.66600\n",
      "time  71.86322140693665\n",
      "edge_vol 4024.4285\n",
      "Epoch: 0066 train_loss= 1.38137 val_loss= 1.65514 train_acc= 0.97500 val_acc= 0.66600 best_val_acc_trail= 0.66600 test_acc= 0.66600\n",
      "time  73.06124305725098\n",
      "edge_vol 4023.7993\n",
      "Epoch: 0067 train_loss= 1.37816 val_loss= 1.65294 train_acc= 0.97500 val_acc= 0.66600 best_val_acc_trail= 0.66600 test_acc= 0.66600\n",
      "time  73.77074360847473\n",
      "edge_vol 4025.7695\n",
      "Epoch: 0068 train_loss= 1.37323 val_loss= 1.65081 train_acc= 0.97500 val_acc= 0.67000 best_val_acc_trail= 0.67000 test_acc= 0.66600\n",
      "time  75.14723062515259\n",
      "edge_vol 4030.1548\n",
      "Epoch: 0069 train_loss= 1.36415 val_loss= 1.64872 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.67200 test_acc= 0.66800\n",
      "time  76.31003832817078\n",
      "edge_vol 4035.1934\n",
      "Epoch: 0070 train_loss= 1.35713 val_loss= 1.64669 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.66700\n",
      "time  77.67529845237732\n",
      "edge_vol 4041.1638\n",
      "Epoch: 0071 train_loss= 1.35219 val_loss= 1.64473 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67400 test_acc= 0.66700\n",
      "time  78.83063578605652\n",
      "edge_vol 4048.9263\n",
      "Epoch: 0072 train_loss= 1.34487 val_loss= 1.64280 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  79.87542843818665\n",
      "edge_vol 4058.046\n",
      "Epoch: 0073 train_loss= 1.33128 val_loss= 1.64099 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  81.0833067893982\n",
      "edge_vol 4067.2668\n",
      "Epoch: 0074 train_loss= 1.32617 val_loss= 1.63923 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  81.92162036895752\n",
      "edge_vol 4078.532\n",
      "Epoch: 0075 train_loss= 1.32508 val_loss= 1.63755 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  82.98144721984863\n",
      "edge_vol 4091.2224\n",
      "Epoch: 0076 train_loss= 1.31800 val_loss= 1.63594 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  84.1359179019928\n",
      "edge_vol 4105.434\n",
      "Epoch: 0077 train_loss= 1.31360 val_loss= 1.63437 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  85.10198879241943\n",
      "edge_vol 4121.246\n",
      "Epoch: 0078 train_loss= 1.30624 val_loss= 1.63290 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  86.0614743232727\n",
      "edge_vol 4137.9224\n",
      "Epoch: 0079 train_loss= 1.30089 val_loss= 1.63149 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  86.97883629798889\n",
      "edge_vol 4155.7827\n",
      "Epoch: 0080 train_loss= 1.28982 val_loss= 1.63022 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  88.10388851165771\n",
      "edge_vol 4173.6846\n",
      "Epoch: 0081 train_loss= 1.28409 val_loss= 1.62896 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  88.78270983695984\n",
      "edge_vol 4191.911\n",
      "Epoch: 0082 train_loss= 1.28152 val_loss= 1.62772 train_acc= 0.98333 val_acc= 0.67400 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  89.54540371894836\n",
      "edge_vol 4211.0713\n",
      "Epoch: 0083 train_loss= 1.27896 val_loss= 1.62654 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  90.60581636428833\n",
      "edge_vol 4230.859\n",
      "Epoch: 0084 train_loss= 1.27337 val_loss= 1.62535 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  91.6261739730835\n",
      "edge_vol 4251.0366\n",
      "Epoch: 0085 train_loss= 1.26712 val_loss= 1.62422 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  92.67894911766052\n",
      "edge_vol 4272.0864\n",
      "Epoch: 0086 train_loss= 1.26418 val_loss= 1.62306 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  93.72576856613159\n",
      "edge_vol 4292.941\n",
      "Epoch: 0087 train_loss= 1.26003 val_loss= 1.62195 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  94.91050481796265\n",
      "edge_vol 4314.493\n",
      "Epoch: 0088 train_loss= 1.25600 val_loss= 1.62090 train_acc= 0.98333 val_acc= 0.67600 best_val_acc_trail= 0.67800 test_acc= 0.67200\n",
      "time  95.85321092605591\n",
      "edge_vol 4336.2295\n",
      "Epoch: 0089 train_loss= 1.24747 val_loss= 1.61984 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.68000 test_acc= 0.67800\n",
      "time  97.24500036239624\n",
      "edge_vol 4358.1934\n",
      "Epoch: 0090 train_loss= 1.24208 val_loss= 1.61882 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68200 test_acc= 0.67800\n",
      "time  98.5020534992218\n",
      "edge_vol 4380.377\n",
      "Epoch: 0091 train_loss= 1.24135 val_loss= 1.61790 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  99.80393886566162\n",
      "edge_vol 4402.8564\n",
      "Epoch: 0092 train_loss= 1.23278 val_loss= 1.61703 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  100.563396692276\n",
      "edge_vol 4426.2627\n",
      "Epoch: 0093 train_loss= 1.23527 val_loss= 1.61627 train_acc= 0.98333 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  101.39985990524292\n",
      "edge_vol 4450.4297\n",
      "Epoch: 0094 train_loss= 1.23150 val_loss= 1.61557 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  102.26848101615906\n",
      "edge_vol 4474.6323\n",
      "Epoch: 0095 train_loss= 1.22033 val_loss= 1.61489 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  103.22291707992554\n",
      "edge_vol 4498.7207\n",
      "Epoch: 0096 train_loss= 1.21543 val_loss= 1.61417 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  104.24958205223083\n",
      "edge_vol 4523.043\n",
      "Epoch: 0097 train_loss= 1.22009 val_loss= 1.61355 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  105.21780633926392\n",
      "edge_vol 4547.105\n",
      "Epoch: 0098 train_loss= 1.21241 val_loss= 1.61299 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  106.11450505256653\n",
      "edge_vol 4571.015\n",
      "Epoch: 0099 train_loss= 1.20702 val_loss= 1.61244 train_acc= 0.97500 val_acc= 0.67200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  107.5929229259491\n",
      "edge_vol 4595.003\n",
      "Epoch: 0100 train_loss= 1.20527 val_loss= 1.61198 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  108.5957555770874\n",
      "edge_vol 4619.4346\n",
      "Epoch: 0101 train_loss= 1.20379 val_loss= 1.61155 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  109.6817696094513\n",
      "edge_vol 4644.1245\n",
      "Epoch: 0102 train_loss= 1.20138 val_loss= 1.61116 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  110.80722165107727\n",
      "edge_vol 4669.209\n",
      "Epoch: 0103 train_loss= 1.19850 val_loss= 1.61074 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  111.8284215927124\n",
      "edge_vol 4694.2695\n",
      "Epoch: 0104 train_loss= 1.19168 val_loss= 1.61040 train_acc= 0.97500 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  112.94716358184814\n",
      "edge_vol 4719.7544\n",
      "Epoch: 0105 train_loss= 1.18967 val_loss= 1.61011 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  113.9048855304718\n",
      "edge_vol 4745.351\n",
      "Epoch: 0106 train_loss= 1.18461 val_loss= 1.60983 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  114.76863360404968\n",
      "edge_vol 4770.787\n",
      "Epoch: 0107 train_loss= 1.17919 val_loss= 1.60954 train_acc= 0.97500 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  116.09553956985474\n",
      "edge_vol 4795.9336\n",
      "Epoch: 0108 train_loss= 1.17610 val_loss= 1.60926 train_acc= 0.97500 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  116.87390112876892\n",
      "edge_vol 4820.5815\n",
      "Epoch: 0109 train_loss= 1.17710 val_loss= 1.60890 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  117.86773133277893\n",
      "edge_vol 4844.1807\n",
      "Epoch: 0110 train_loss= 1.17295 val_loss= 1.60847 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  119.05211758613586\n",
      "edge_vol 4866.5723\n",
      "Epoch: 0111 train_loss= 1.17648 val_loss= 1.60809 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  120.22768235206604\n",
      "edge_vol 4887.242\n",
      "Epoch: 0112 train_loss= 1.17249 val_loss= 1.60774 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  120.97654342651367\n",
      "edge_vol 4905.7275\n",
      "Epoch: 0113 train_loss= 1.16496 val_loss= 1.60734 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  121.94380617141724\n",
      "edge_vol 4922.8604\n",
      "Epoch: 0114 train_loss= 1.16660 val_loss= 1.60688 train_acc= 0.97500 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  122.95926547050476\n",
      "edge_vol 4939.8867\n",
      "Epoch: 0115 train_loss= 1.16594 val_loss= 1.60644 train_acc= 0.97500 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  123.93735384941101\n",
      "edge_vol 4957.13\n",
      "Epoch: 0116 train_loss= 1.15803 val_loss= 1.60583 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  124.89871406555176\n",
      "edge_vol 4975.0767\n",
      "Epoch: 0117 train_loss= 1.15946 val_loss= 1.60529 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  125.94353342056274\n",
      "edge_vol 4994.0215\n",
      "Epoch: 0118 train_loss= 1.15245 val_loss= 1.60474 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  126.72686219215393\n",
      "edge_vol 5013.8105\n",
      "Epoch: 0119 train_loss= 1.15162 val_loss= 1.60419 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  127.83463525772095\n",
      "edge_vol 5034.335\n",
      "Epoch: 0120 train_loss= 1.14979 val_loss= 1.60368 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68000\n",
      "time  128.7999427318573\n",
      "edge_vol 5055.2573\n",
      "Epoch: 0121 train_loss= 1.14515 val_loss= 1.60318 train_acc= 0.97500 val_acc= 0.68400 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  129.87235379219055\n",
      "edge_vol 5076.5415\n",
      "Epoch: 0122 train_loss= 1.14789 val_loss= 1.60276 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  131.2839720249176\n",
      "edge_vol 5098.1665\n",
      "Epoch: 0123 train_loss= 1.14372 val_loss= 1.60243 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  132.32472395896912\n",
      "edge_vol 5120.2603\n",
      "Epoch: 0124 train_loss= 1.14009 val_loss= 1.60208 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  133.4082715511322\n",
      "edge_vol 5142.36\n",
      "Epoch: 0125 train_loss= 1.14615 val_loss= 1.60168 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  134.49239230155945\n",
      "edge_vol 5165.07\n",
      "Epoch: 0126 train_loss= 1.13998 val_loss= 1.60115 train_acc= 0.96667 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  135.6740186214447\n",
      "edge_vol 5188.089\n",
      "Epoch: 0127 train_loss= 1.13965 val_loss= 1.60049 train_acc= 0.96667 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  136.71377873420715\n",
      "edge_vol 5211.997\n",
      "Epoch: 0128 train_loss= 1.14031 val_loss= 1.59973 train_acc= 0.96667 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  137.83816003799438\n",
      "edge_vol 5236.2725\n",
      "Epoch: 0129 train_loss= 1.13564 val_loss= 1.59895 train_acc= 0.96667 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  138.6066484451294\n",
      "edge_vol 5260.76\n",
      "Epoch: 0130 train_loss= 1.12579 val_loss= 1.59810 train_acc= 0.96667 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  139.5731086730957\n",
      "edge_vol 5285.745\n",
      "Epoch: 0131 train_loss= 1.13220 val_loss= 1.59722 train_acc= 0.96667 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  140.81202673912048\n",
      "edge_vol 5311.3047\n",
      "Epoch: 0132 train_loss= 1.13069 val_loss= 1.59628 train_acc= 0.96667 val_acc= 0.67400 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  141.7983090877533\n",
      "edge_vol 5337.2314\n",
      "Epoch: 0133 train_loss= 1.12614 val_loss= 1.59529 train_acc= 0.96667 val_acc= 0.67600 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  142.93925189971924\n",
      "edge_vol 5363.5547\n",
      "Epoch: 0134 train_loss= 1.12344 val_loss= 1.59432 train_acc= 0.96667 val_acc= 0.67800 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  143.8077208995819\n",
      "edge_vol 5389.9346\n",
      "Epoch: 0135 train_loss= 1.12098 val_loss= 1.59336 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  144.92042756080627\n",
      "edge_vol 5416.7207\n",
      "Epoch: 0136 train_loss= 1.11687 val_loss= 1.59230 train_acc= 0.96667 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  146.04465341567993\n",
      "edge_vol 5443.466\n",
      "Epoch: 0137 train_loss= 1.11842 val_loss= 1.59111 train_acc= 0.96667 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  147.11170935630798\n",
      "edge_vol 5470.7446\n",
      "Epoch: 0138 train_loss= 1.10919 val_loss= 1.58974 train_acc= 0.96667 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  148.07512974739075\n",
      "edge_vol 5498.3086\n",
      "Epoch: 0139 train_loss= 1.10416 val_loss= 1.58834 train_acc= 0.97500 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  149.25364875793457\n",
      "edge_vol 5525.865\n",
      "Epoch: 0140 train_loss= 1.10503 val_loss= 1.58699 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  150.36330318450928\n",
      "edge_vol 5553.7617\n",
      "Epoch: 0141 train_loss= 1.10704 val_loss= 1.58548 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  151.2291214466095\n",
      "edge_vol 5581.667\n",
      "Epoch: 0142 train_loss= 1.09764 val_loss= 1.58393 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  152.29303240776062\n",
      "edge_vol 5610.034\n",
      "Epoch: 0143 train_loss= 1.09496 val_loss= 1.58225 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  153.5613615512848\n",
      "edge_vol 5638.4014\n",
      "Epoch: 0144 train_loss= 1.08654 val_loss= 1.58043 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  154.71897220611572\n",
      "edge_vol 5667.669\n",
      "Epoch: 0145 train_loss= 1.08447 val_loss= 1.57850 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  155.75059294700623\n",
      "edge_vol 5697.794\n",
      "Epoch: 0146 train_loss= 1.09024 val_loss= 1.57650 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  156.9158055782318\n",
      "edge_vol 5728.289\n",
      "Epoch: 0147 train_loss= 1.08464 val_loss= 1.57446 train_acc= 0.97500 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  158.10869026184082\n",
      "edge_vol 5759.295\n",
      "Epoch: 0148 train_loss= 1.07916 val_loss= 1.57229 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  158.9014298915863\n",
      "edge_vol 5790.76\n",
      "Epoch: 0149 train_loss= 1.07322 val_loss= 1.56998 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  159.9874827861786\n",
      "edge_vol 5822.374\n",
      "Epoch: 0150 train_loss= 1.06711 val_loss= 1.56759 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  161.02734804153442\n",
      "edge_vol 5854.331\n",
      "Epoch: 0151 train_loss= 1.06637 val_loss= 1.56518 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  161.8049418926239\n",
      "edge_vol 5886.838\n",
      "Epoch: 0152 train_loss= 1.05965 val_loss= 1.56278 train_acc= 0.98333 val_acc= 0.68000 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  163.0963728427887\n",
      "edge_vol 5919.918\n",
      "Epoch: 0153 train_loss= 1.05405 val_loss= 1.56031 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68400 test_acc= 0.68400\n",
      "time  164.14404129981995\n",
      "edge_vol 5953.3413\n",
      "Epoch: 0154 train_loss= 1.05261 val_loss= 1.55794 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  165.16054153442383\n",
      "edge_vol 5987.4937\n",
      "Epoch: 0155 train_loss= 1.04844 val_loss= 1.55556 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  166.3270800113678\n",
      "edge_vol 6022.953\n",
      "Epoch: 0156 train_loss= 1.04202 val_loss= 1.55316 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  167.4713408946991\n",
      "edge_vol 6059.246\n",
      "Epoch: 0157 train_loss= 1.03594 val_loss= 1.55069 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  168.3881814479828\n",
      "edge_vol 6096.1475\n",
      "Epoch: 0158 train_loss= 1.02786 val_loss= 1.54814 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  169.2547242641449\n",
      "edge_vol 6134.2607\n",
      "Epoch: 0159 train_loss= 1.03212 val_loss= 1.54561 train_acc= 0.98333 val_acc= 0.68200 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  170.24835014343262\n",
      "edge_vol 6173.066\n",
      "Epoch: 0160 train_loss= 1.01799 val_loss= 1.54301 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  171.20917868614197\n",
      "edge_vol 6212.367\n",
      "Epoch: 0161 train_loss= 1.01522 val_loss= 1.54028 train_acc= 0.98333 val_acc= 0.68400 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  172.14858889579773\n",
      "edge_vol 6252.3447\n",
      "Epoch: 0162 train_loss= 1.01218 val_loss= 1.53749 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  173.18283653259277\n",
      "edge_vol 6292.752\n",
      "Epoch: 0163 train_loss= 1.00575 val_loss= 1.53468 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.68600 test_acc= 0.67800\n",
      "time  174.13880586624146\n",
      "edge_vol 6333.7637\n",
      "Epoch: 0164 train_loss= 0.99883 val_loss= 1.53175 train_acc= 0.98333 val_acc= 0.68800 best_val_acc_trail= 0.68800 test_acc= 0.68400\n",
      "time  175.4934585094452\n",
      "edge_vol 6375.376\n",
      "Epoch: 0165 train_loss= 0.99773 val_loss= 1.52875 train_acc= 0.98333 val_acc= 0.68800 best_val_acc_trail= 0.68800 test_acc= 0.68400\n",
      "time  176.59837675094604\n",
      "edge_vol 6416.9707\n",
      "Epoch: 0166 train_loss= 0.98954 val_loss= 1.52571 train_acc= 0.98333 val_acc= 0.68800 best_val_acc_trail= 0.68800 test_acc= 0.68400\n",
      "time  177.52704191207886\n",
      "edge_vol 6458.9565\n",
      "Epoch: 0167 train_loss= 0.98823 val_loss= 1.52266 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.68600\n",
      "time  178.59057807922363\n",
      "edge_vol 6501.506\n",
      "Epoch: 0168 train_loss= 0.97867 val_loss= 1.51966 train_acc= 0.97500 val_acc= 0.69000 best_val_acc_trail= 0.69000 test_acc= 0.68600\n",
      "time  179.90356492996216\n",
      "edge_vol 6544.3867\n",
      "Epoch: 0169 train_loss= 0.97282 val_loss= 1.51665 train_acc= 0.98333 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  181.03567743301392\n",
      "edge_vol 6587.667\n",
      "Epoch: 0170 train_loss= 0.97206 val_loss= 1.51359 train_acc= 0.98333 val_acc= 0.69000 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  182.033034324646\n",
      "edge_vol 6630.749\n",
      "Epoch: 0171 train_loss= 0.95754 val_loss= 1.51054 train_acc= 0.98333 val_acc= 0.69000 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  183.1010503768921\n",
      "edge_vol 6674.339\n",
      "Epoch: 0172 train_loss= 0.96096 val_loss= 1.50746 train_acc= 0.98333 val_acc= 0.69000 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  183.98157453536987\n",
      "edge_vol 6718.7163\n",
      "Epoch: 0173 train_loss= 0.95103 val_loss= 1.50436 train_acc= 0.98333 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  184.94030261039734\n",
      "edge_vol 6763.662\n",
      "Epoch: 0174 train_loss= 0.94480 val_loss= 1.50129 train_acc= 0.98333 val_acc= 0.69200 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  186.25490188598633\n",
      "edge_vol 6808.3584\n",
      "Epoch: 0175 train_loss= 0.94255 val_loss= 1.49823 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  187.36569809913635\n",
      "edge_vol 6853.3066\n",
      "Epoch: 0176 train_loss= 0.94058 val_loss= 1.49519 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  188.79938459396362\n",
      "edge_vol 6898.917\n",
      "Epoch: 0177 train_loss= 0.93446 val_loss= 1.49214 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  189.74624633789062\n",
      "edge_vol 6944.9434\n",
      "Epoch: 0178 train_loss= 0.93168 val_loss= 1.48910 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  190.57468008995056\n",
      "edge_vol 6990.9453\n",
      "Epoch: 0179 train_loss= 0.92483 val_loss= 1.48602 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  191.32011222839355\n",
      "edge_vol 7036.89\n",
      "Epoch: 0180 train_loss= 0.91833 val_loss= 1.48299 train_acc= 0.98333 val_acc= 0.68600 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  192.56406927108765\n",
      "edge_vol 7082.6797\n",
      "Epoch: 0181 train_loss= 0.91573 val_loss= 1.48008 train_acc= 0.98333 val_acc= 0.68800 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  193.72517585754395\n",
      "edge_vol 7128.641\n",
      "Epoch: 0182 train_loss= 0.91405 val_loss= 1.47724 train_acc= 0.98333 val_acc= 0.69000 best_val_acc_trail= 0.69200 test_acc= 0.68600\n",
      "time  194.94475984573364\n",
      "edge_vol 7174.1836\n",
      "Epoch: 0183 train_loss= 0.90554 val_loss= 1.47436 train_acc= 0.98333 val_acc= 0.69400 best_val_acc_trail= 0.69400 test_acc= 0.68700\n",
      "time  195.7666003704071\n",
      "edge_vol 7219.9023\n",
      "Epoch: 0184 train_loss= 0.90309 val_loss= 1.47158 train_acc= 0.98333 val_acc= 0.69800 best_val_acc_trail= 0.69800 test_acc= 0.68900\n",
      "time  197.01958513259888\n",
      "edge_vol 7265.167\n",
      "Epoch: 0185 train_loss= 0.89611 val_loss= 1.46878 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70000 test_acc= 0.69200\n",
      "time  198.03946781158447\n",
      "edge_vol 7310.265\n",
      "Epoch: 0186 train_loss= 0.89067 val_loss= 1.46596 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70000 test_acc= 0.69200\n",
      "time  198.91651844978333\n",
      "edge_vol 7355.409\n",
      "Epoch: 0187 train_loss= 0.88434 val_loss= 1.46329 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70000 test_acc= 0.69200\n",
      "time  199.87744665145874\n",
      "edge_vol 7400.5894\n",
      "Epoch: 0188 train_loss= 0.88572 val_loss= 1.46066 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70000 test_acc= 0.69200\n",
      "time  201.11999678611755\n",
      "edge_vol 7445.0664\n",
      "Epoch: 0189 train_loss= 0.87413 val_loss= 1.45805 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70200 test_acc= 0.69200\n",
      "time  202.02405643463135\n",
      "edge_vol 7488.966\n",
      "Epoch: 0190 train_loss= 0.87599 val_loss= 1.45547 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70200 test_acc= 0.69200\n",
      "time  203.20703887939453\n",
      "edge_vol 7532.5254\n",
      "Epoch: 0191 train_loss= 0.87039 val_loss= 1.45295 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70200 test_acc= 0.69200\n",
      "time  204.31610369682312\n",
      "edge_vol 7575.534\n",
      "Epoch: 0192 train_loss= 0.86251 val_loss= 1.45045 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70200 test_acc= 0.69200\n",
      "time  205.16952323913574\n",
      "edge_vol 7617.5015\n",
      "Epoch: 0193 train_loss= 0.86050 val_loss= 1.44791 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  206.03886699676514\n",
      "edge_vol 7658.718\n",
      "Epoch: 0194 train_loss= 0.85829 val_loss= 1.44535 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  206.8812038898468\n",
      "edge_vol 7699.6167\n",
      "Epoch: 0195 train_loss= 0.85546 val_loss= 1.44288 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  207.86724948883057\n",
      "edge_vol 7739.8877\n",
      "Epoch: 0196 train_loss= 0.85238 val_loss= 1.44044 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  208.9667136669159\n",
      "edge_vol 7779.337\n",
      "Epoch: 0197 train_loss= 0.84644 val_loss= 1.43796 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  210.0551471710205\n",
      "edge_vol 7817.788\n",
      "Epoch: 0198 train_loss= 0.84207 val_loss= 1.43542 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  210.9452610015869\n",
      "edge_vol 7855.7246\n",
      "Epoch: 0199 train_loss= 0.83867 val_loss= 1.43288 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  211.9803261756897\n",
      "edge_vol 7893.4395\n",
      "Epoch: 0200 train_loss= 0.83521 val_loss= 1.43034 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  213.16676306724548\n",
      "edge_vol 7930.666\n",
      "Epoch: 0201 train_loss= 0.83136 val_loss= 1.42778 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  214.4434835910797\n",
      "edge_vol 7967.528\n",
      "Epoch: 0202 train_loss= 0.82945 val_loss= 1.42527 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  215.35889172554016\n",
      "edge_vol 8003.572\n",
      "Epoch: 0203 train_loss= 0.82407 val_loss= 1.42276 train_acc= 0.98333 val_acc= 0.70000 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  216.57859587669373\n",
      "edge_vol 8038.582\n",
      "Epoch: 0204 train_loss= 0.81634 val_loss= 1.42026 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.70400 test_acc= 0.69100\n",
      "time  217.61188292503357\n",
      "edge_vol 8073.1313\n",
      "Epoch: 0205 train_loss= 0.81974 val_loss= 1.41772 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  218.63967847824097\n",
      "edge_vol 8107.1357\n",
      "Epoch: 0206 train_loss= 0.80900 val_loss= 1.41521 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  219.573322057724\n",
      "edge_vol 8141.1396\n",
      "Epoch: 0207 train_loss= 0.80845 val_loss= 1.41275 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  220.3814172744751\n",
      "edge_vol 8174.4277\n",
      "Epoch: 0208 train_loss= 0.80363 val_loss= 1.41019 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  221.39762449264526\n",
      "edge_vol 8206.854\n",
      "Epoch: 0209 train_loss= 0.79576 val_loss= 1.40761 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  222.25757598876953\n",
      "edge_vol 8238.799\n",
      "Epoch: 0210 train_loss= 0.79664 val_loss= 1.40487 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  223.23403143882751\n",
      "edge_vol 8270.372\n",
      "Epoch: 0211 train_loss= 0.78910 val_loss= 1.40205 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  224.25460767745972\n",
      "edge_vol 8301.0625\n",
      "Epoch: 0212 train_loss= 0.79110 val_loss= 1.39919 train_acc= 0.97500 val_acc= 0.70400 best_val_acc_trail= 0.70600 test_acc= 0.69600\n",
      "time  224.97788763046265\n",
      "edge_vol 8331.158\n",
      "Epoch: 0213 train_loss= 0.78488 val_loss= 1.39630 train_acc= 0.97500 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.70000\n",
      "time  226.01391124725342\n",
      "edge_vol 8360.519\n",
      "Epoch: 0214 train_loss= 0.77888 val_loss= 1.39335 train_acc= 0.97500 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.70000\n",
      "time  227.10666632652283\n",
      "edge_vol 8389.5\n",
      "Epoch: 0215 train_loss= 0.77371 val_loss= 1.39053 train_acc= 0.97500 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.70000\n",
      "time  228.130211353302\n",
      "edge_vol 8417.509\n",
      "Epoch: 0216 train_loss= 0.76756 val_loss= 1.38766 train_acc= 0.97500 val_acc= 0.70600 best_val_acc_trail= 0.70600 test_acc= 0.70000\n",
      "time  229.05523920059204\n",
      "edge_vol 8444.885\n",
      "Epoch: 0217 train_loss= 0.76268 val_loss= 1.38481 train_acc= 0.97500 val_acc= 0.70800 best_val_acc_trail= 0.70800 test_acc= 0.70100\n",
      "time  230.03334951400757\n",
      "edge_vol 8471.0625\n",
      "Epoch: 0218 train_loss= 0.75942 val_loss= 1.38188 train_acc= 0.97500 val_acc= 0.70800 best_val_acc_trail= 0.70800 test_acc= 0.70100\n",
      "time  231.14484167099\n",
      "edge_vol 8496.529\n",
      "Epoch: 0219 train_loss= 0.75649 val_loss= 1.37894 train_acc= 0.97500 val_acc= 0.70800 best_val_acc_trail= 0.70800 test_acc= 0.70100\n",
      "time  232.18796277046204\n",
      "edge_vol 8521.115\n",
      "Epoch: 0220 train_loss= 0.75365 val_loss= 1.37598 train_acc= 0.97500 val_acc= 0.70800 best_val_acc_trail= 0.70800 test_acc= 0.70100\n",
      "time  233.13784408569336\n",
      "edge_vol 8544.916\n",
      "Epoch: 0221 train_loss= 0.74595 val_loss= 1.37296 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  234.3125455379486\n",
      "edge_vol 8567.837\n",
      "Epoch: 0222 train_loss= 0.74148 val_loss= 1.36999 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  235.16590023040771\n",
      "edge_vol 8590.301\n",
      "Epoch: 0223 train_loss= 0.73714 val_loss= 1.36705 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  236.05857276916504\n",
      "edge_vol 8612.244\n",
      "Epoch: 0224 train_loss= 0.73543 val_loss= 1.36414 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  237.0726351737976\n",
      "edge_vol 8633.843\n",
      "Epoch: 0225 train_loss= 0.72997 val_loss= 1.36124 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  238.15297317504883\n",
      "edge_vol 8654.484\n",
      "Epoch: 0226 train_loss= 0.72746 val_loss= 1.35837 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  239.2668309211731\n",
      "edge_vol 8674.268\n",
      "Epoch: 0227 train_loss= 0.72308 val_loss= 1.35553 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  240.18067526817322\n",
      "edge_vol 8693.575\n",
      "Epoch: 0228 train_loss= 0.71866 val_loss= 1.35278 train_acc= 0.97500 val_acc= 0.71000 best_val_acc_trail= 0.71000 test_acc= 0.70600\n",
      "time  241.36547470092773\n",
      "edge_vol 8712.245\n",
      "Epoch: 0229 train_loss= 0.71334 val_loss= 1.35005 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71200 test_acc= 0.70500\n",
      "time  242.50757694244385\n",
      "edge_vol 8730.595\n",
      "Epoch: 0230 train_loss= 0.71108 val_loss= 1.34727 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71200 test_acc= 0.70500\n",
      "time  243.39626026153564\n",
      "edge_vol 8748.52\n",
      "Epoch: 0231 train_loss= 0.70398 val_loss= 1.34458 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71400 test_acc= 0.70700\n",
      "time  244.36328172683716\n",
      "edge_vol 8765.715\n",
      "Epoch: 0232 train_loss= 0.70012 val_loss= 1.34188 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71400 test_acc= 0.70700\n",
      "time  245.2617528438568\n",
      "edge_vol 8782.307\n",
      "Epoch: 0233 train_loss= 0.69657 val_loss= 1.33920 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71400 test_acc= 0.70700\n",
      "time  246.43291115760803\n",
      "edge_vol 8798.433\n",
      "Epoch: 0234 train_loss= 0.69331 val_loss= 1.33650 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71400 test_acc= 0.70700\n",
      "time  247.42622184753418\n",
      "edge_vol 8813.805\n",
      "Epoch: 0235 train_loss= 0.68810 val_loss= 1.33371 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71400 test_acc= 0.70700\n",
      "time  248.6449511051178\n",
      "edge_vol 8829.111\n",
      "Epoch: 0236 train_loss= 0.68286 val_loss= 1.33093 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  249.72209191322327\n",
      "edge_vol 8843.625\n",
      "Epoch: 0237 train_loss= 0.68053 val_loss= 1.32814 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  250.67622590065002\n",
      "edge_vol 8857.596\n",
      "Epoch: 0238 train_loss= 0.67643 val_loss= 1.32528 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  251.59540510177612\n",
      "edge_vol 8871.008\n",
      "Epoch: 0239 train_loss= 0.67316 val_loss= 1.32248 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  252.57107305526733\n",
      "edge_vol 8884.086\n",
      "Epoch: 0240 train_loss= 0.67141 val_loss= 1.31969 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  253.78645849227905\n",
      "edge_vol 8897.113\n",
      "Epoch: 0241 train_loss= 0.66737 val_loss= 1.31688 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  254.49034714698792\n",
      "edge_vol 8909.882\n",
      "Epoch: 0242 train_loss= 0.66337 val_loss= 1.31400 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  255.76842713356018\n",
      "edge_vol 8922.05\n",
      "Epoch: 0243 train_loss= 0.65737 val_loss= 1.31104 train_acc= 0.98333 val_acc= 0.71600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  256.85266733169556\n",
      "edge_vol 8933.83\n",
      "Epoch: 0244 train_loss= 0.65259 val_loss= 1.30807 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  258.08448696136475\n",
      "edge_vol 8945.223\n",
      "Epoch: 0245 train_loss= 0.65013 val_loss= 1.30513 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  259.1555380821228\n",
      "edge_vol 8956.496\n",
      "Epoch: 0246 train_loss= 0.64649 val_loss= 1.30229 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  260.5332634449005\n",
      "edge_vol 8967.585\n",
      "Epoch: 0247 train_loss= 0.64029 val_loss= 1.29960 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  261.6073522567749\n",
      "edge_vol 8978.306\n",
      "Epoch: 0248 train_loss= 0.63548 val_loss= 1.29683 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  262.55847454071045\n",
      "edge_vol 8988.525\n",
      "Epoch: 0249 train_loss= 0.63324 val_loss= 1.29405 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  263.5062973499298\n",
      "edge_vol 8998.3545\n",
      "Epoch: 0250 train_loss= 0.62863 val_loss= 1.29129 train_acc= 0.98333 val_acc= 0.71400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  264.5585038661957\n",
      "edge_vol 9007.973\n",
      "Epoch: 0251 train_loss= 0.62528 val_loss= 1.28857 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  265.52392411231995\n",
      "edge_vol 9017.32\n",
      "Epoch: 0252 train_loss= 0.62349 val_loss= 1.28590 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  266.9931833744049\n",
      "edge_vol 9026.248\n",
      "Epoch: 0253 train_loss= 0.61960 val_loss= 1.28327 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  268.2399034500122\n",
      "edge_vol 9034.787\n",
      "Epoch: 0254 train_loss= 0.61323 val_loss= 1.28058 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  269.40342235565186\n",
      "edge_vol 9043.243\n",
      "Epoch: 0255 train_loss= 0.61102 val_loss= 1.27789 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  270.47103214263916\n",
      "edge_vol 9051.494\n",
      "Epoch: 0256 train_loss= 0.60952 val_loss= 1.27515 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  271.4602224826813\n",
      "edge_vol 9059.489\n",
      "Epoch: 0257 train_loss= 0.60265 val_loss= 1.27238 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  272.7870833873749\n",
      "edge_vol 9067.218\n",
      "Epoch: 0258 train_loss= 0.60228 val_loss= 1.26972 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  273.80687046051025\n",
      "edge_vol 9074.792\n",
      "Epoch: 0259 train_loss= 0.59456 val_loss= 1.26713 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  274.94406485557556\n",
      "edge_vol 9082.217\n",
      "Epoch: 0260 train_loss= 0.59387 val_loss= 1.26452 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  275.8758964538574\n",
      "edge_vol 9089.609\n",
      "Epoch: 0261 train_loss= 0.58991 val_loss= 1.26188 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  277.2676396369934\n",
      "edge_vol 9096.675\n",
      "Epoch: 0262 train_loss= 0.58598 val_loss= 1.25918 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  278.27114510536194\n",
      "edge_vol 9103.491\n",
      "Epoch: 0263 train_loss= 0.58301 val_loss= 1.25657 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  279.494206905365\n",
      "edge_vol 9110.062\n",
      "Epoch: 0264 train_loss= 0.58019 val_loss= 1.25390 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  280.7384145259857\n",
      "edge_vol 9116.463\n",
      "Epoch: 0265 train_loss= 0.57575 val_loss= 1.25122 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  281.7995374202728\n",
      "edge_vol 9122.719\n",
      "Epoch: 0266 train_loss= 0.57334 val_loss= 1.24871 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  282.7103998661041\n",
      "edge_vol 9128.712\n",
      "Epoch: 0267 train_loss= 0.57077 val_loss= 1.24629 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  283.70701932907104\n",
      "edge_vol 9134.547\n",
      "Epoch: 0268 train_loss= 0.56846 val_loss= 1.24382 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  284.7240061759949\n",
      "edge_vol 9140.211\n",
      "Epoch: 0269 train_loss= 0.56560 val_loss= 1.24141 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  285.6395926475525\n",
      "edge_vol 9145.699\n",
      "Epoch: 0270 train_loss= 0.55896 val_loss= 1.23900 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  286.59952092170715\n",
      "edge_vol 9151.213\n",
      "Epoch: 0271 train_loss= 0.55821 val_loss= 1.23649 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  287.54675459861755\n",
      "edge_vol 9156.659\n",
      "Epoch: 0272 train_loss= 0.55529 val_loss= 1.23398 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  288.5341603755951\n",
      "edge_vol 9162.105\n",
      "Epoch: 0273 train_loss= 0.55052 val_loss= 1.23167 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  290.00180292129517\n",
      "edge_vol 9167.348\n",
      "Epoch: 0274 train_loss= 0.54733 val_loss= 1.22936 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  290.95800828933716\n",
      "edge_vol 9172.357\n",
      "Epoch: 0275 train_loss= 0.54667 val_loss= 1.22704 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  291.95427441596985\n",
      "edge_vol 9177.154\n",
      "Epoch: 0276 train_loss= 0.54440 val_loss= 1.22467 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  293.094762802124\n",
      "edge_vol 9181.931\n",
      "Epoch: 0277 train_loss= 0.53992 val_loss= 1.22227 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  294.1020438671112\n",
      "edge_vol 9186.547\n",
      "Epoch: 0278 train_loss= 0.53789 val_loss= 1.21996 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  295.1884789466858\n",
      "edge_vol 9190.876\n",
      "Epoch: 0279 train_loss= 0.53386 val_loss= 1.21768 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  296.7933552265167\n",
      "edge_vol 9194.995\n",
      "Epoch: 0280 train_loss= 0.53139 val_loss= 1.21558 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  297.5971157550812\n",
      "edge_vol 9198.921\n",
      "Epoch: 0281 train_loss= 0.52899 val_loss= 1.21341 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  298.93178629875183\n",
      "edge_vol 9202.551\n",
      "Epoch: 0282 train_loss= 0.52420 val_loss= 1.21118 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  299.7840361595154\n",
      "edge_vol 9206.174\n",
      "Epoch: 0283 train_loss= 0.52229 val_loss= 1.20896 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  300.79841709136963\n",
      "edge_vol 9209.697\n",
      "Epoch: 0284 train_loss= 0.52102 val_loss= 1.20684 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  301.7440288066864\n",
      "edge_vol 9212.9795\n",
      "Epoch: 0285 train_loss= 0.51832 val_loss= 1.20462 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  302.64680337905884\n",
      "edge_vol 9216.096\n",
      "Epoch: 0286 train_loss= 0.51565 val_loss= 1.20240 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  303.9205975532532\n",
      "edge_vol 9219.079\n",
      "Epoch: 0287 train_loss= 0.51151 val_loss= 1.20029 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  304.8920407295227\n",
      "edge_vol 9222.033\n",
      "Epoch: 0288 train_loss= 0.51034 val_loss= 1.19813 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  306.0382070541382\n",
      "edge_vol 9224.875\n",
      "Epoch: 0289 train_loss= 0.50680 val_loss= 1.19598 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  306.8963813781738\n",
      "edge_vol 9227.553\n",
      "Epoch: 0290 train_loss= 0.50443 val_loss= 1.19393 train_acc= 0.98333 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  307.96785163879395\n",
      "edge_vol 9230.114\n",
      "Epoch: 0291 train_loss= 0.50287 val_loss= 1.19189 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  308.5731945037842\n",
      "edge_vol 9232.539\n",
      "Epoch: 0292 train_loss= 0.49922 val_loss= 1.18990 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  309.3125903606415\n",
      "edge_vol 9234.752\n",
      "Epoch: 0293 train_loss= 0.49696 val_loss= 1.18791 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  310.1228744983673\n",
      "edge_vol 9236.767\n",
      "Epoch: 0294 train_loss= 0.49522 val_loss= 1.18598 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  311.39708042144775\n",
      "edge_vol 9238.575\n",
      "Epoch: 0295 train_loss= 0.49272 val_loss= 1.18412 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  312.46814584732056\n",
      "edge_vol 9240.241\n",
      "Epoch: 0296 train_loss= 0.48918 val_loss= 1.18230 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  313.64481019973755\n",
      "edge_vol 9241.751\n",
      "Epoch: 0297 train_loss= 0.48681 val_loss= 1.18049 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  315.14566016197205\n",
      "edge_vol 9243.244\n",
      "Epoch: 0298 train_loss= 0.48459 val_loss= 1.17862 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  316.2149019241333\n",
      "edge_vol 9244.562\n",
      "Epoch: 0299 train_loss= 0.48142 val_loss= 1.17675 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  317.154545545578\n",
      "edge_vol 9245.608\n",
      "Epoch: 0300 train_loss= 0.48099 val_loss= 1.17499 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  318.4182286262512\n",
      "edge_vol 9246.425\n",
      "Epoch: 0301 train_loss= 0.47822 val_loss= 1.17327 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  319.38691902160645\n",
      "edge_vol 9246.996\n",
      "Epoch: 0302 train_loss= 0.47540 val_loss= 1.17156 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  320.6036944389343\n",
      "edge_vol 9247.379\n",
      "Epoch: 0303 train_loss= 0.47308 val_loss= 1.16979 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  321.9323408603668\n",
      "edge_vol 9247.628\n",
      "Epoch: 0304 train_loss= 0.47183 val_loss= 1.16802 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  322.9907078742981\n",
      "edge_vol 9247.787\n",
      "Epoch: 0305 train_loss= 0.47000 val_loss= 1.16623 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  324.13472604751587\n",
      "edge_vol 9247.875\n",
      "Epoch: 0306 train_loss= 0.46767 val_loss= 1.16445 train_acc= 0.98333 val_acc= 0.70200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  325.26699113845825\n",
      "edge_vol 9247.925\n",
      "Epoch: 0307 train_loss= 0.46606 val_loss= 1.16271 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  326.3309817314148\n",
      "edge_vol 9247.951\n",
      "Epoch: 0308 train_loss= 0.46469 val_loss= 1.16088 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  327.8100483417511\n",
      "edge_vol 9247.968\n",
      "Epoch: 0309 train_loss= 0.46167 val_loss= 1.15921 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  328.9705193042755\n",
      "edge_vol 9247.977\n",
      "Epoch: 0310 train_loss= 0.45915 val_loss= 1.15763 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  330.0213830471039\n",
      "edge_vol 9247.984\n",
      "Epoch: 0311 train_loss= 0.45708 val_loss= 1.15598 train_acc= 0.98333 val_acc= 0.70400 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  331.434205532074\n",
      "edge_vol 9247.989\n",
      "Epoch: 0312 train_loss= 0.45568 val_loss= 1.15433 train_acc= 0.98333 val_acc= 0.70600 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  332.72117137908936\n",
      "edge_vol 9247.992\n",
      "Epoch: 0313 train_loss= 0.45246 val_loss= 1.15266 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  334.03375697135925\n",
      "edge_vol 9247.995\n",
      "Epoch: 0314 train_loss= 0.45214 val_loss= 1.15107 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  335.28092193603516\n",
      "edge_vol 9247.997\n",
      "Epoch: 0315 train_loss= 0.45066 val_loss= 1.14956 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  336.49085664749146\n",
      "edge_vol 9247.998\n",
      "Epoch: 0316 train_loss= 0.44853 val_loss= 1.14798 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  337.8739972114563\n",
      "edge_vol 9248.0\n",
      "Epoch: 0317 train_loss= 0.44609 val_loss= 1.14642 train_acc= 0.98333 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  339.1122124195099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsp/ljx/PTDNet/models.py:14: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_vol 9248.0\n",
      "Epoch: 0318 train_loss= 0.44401 val_loss= 1.14480 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  340.0719335079193\n",
      "edge_vol 9248.0\n",
      "Epoch: 0319 train_loss= 0.44205 val_loss= 1.14327 train_acc= 0.98333 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  340.8832378387451\n",
      "edge_vol 9248.0\n",
      "Epoch: 0320 train_loss= 0.43985 val_loss= 1.14182 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  341.91943097114563\n",
      "edge_vol 9248.0\n",
      "Epoch: 0321 train_loss= 0.43818 val_loss= 1.14028 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  342.80277609825134\n",
      "edge_vol 9248.0\n",
      "Epoch: 0322 train_loss= 0.43706 val_loss= 1.13880 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  344.1110498905182\n",
      "edge_vol 9248.0\n",
      "Epoch: 0323 train_loss= 0.43513 val_loss= 1.13729 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  344.96969056129456\n",
      "edge_vol 9248.0\n",
      "Epoch: 0324 train_loss= 0.43388 val_loss= 1.13580 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  345.9356665611267\n",
      "edge_vol 9248.0\n",
      "Epoch: 0325 train_loss= 0.43050 val_loss= 1.13440 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  346.85892391204834\n",
      "edge_vol 9248.0\n",
      "Epoch: 0326 train_loss= 0.42906 val_loss= 1.13302 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  347.9382903575897\n",
      "edge_vol 9248.0\n",
      "Epoch: 0327 train_loss= 0.42849 val_loss= 1.13167 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  348.9789819717407\n",
      "edge_vol 9248.0\n",
      "Epoch: 0328 train_loss= 0.42723 val_loss= 1.13036 train_acc= 0.99167 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  350.149982213974\n",
      "edge_vol 9248.0\n",
      "Epoch: 0329 train_loss= 0.42539 val_loss= 1.12904 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  351.3004605770111\n",
      "edge_vol 9248.0\n",
      "Epoch: 0330 train_loss= 0.42341 val_loss= 1.12771 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  352.5668089389801\n",
      "edge_vol 9248.0\n",
      "Epoch: 0331 train_loss= 0.42166 val_loss= 1.12639 train_acc= 0.99167 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  353.90954971313477\n",
      "edge_vol 9248.0\n",
      "Epoch: 0332 train_loss= 0.41991 val_loss= 1.12501 train_acc= 0.99167 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  355.03415966033936\n",
      "edge_vol 9248.0\n",
      "Epoch: 0333 train_loss= 0.41878 val_loss= 1.12364 train_acc= 0.99167 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  355.91022419929504\n",
      "edge_vol 9248.0\n",
      "Epoch: 0334 train_loss= 0.41620 val_loss= 1.12226 train_acc= 0.99167 val_acc= 0.70800 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  357.19109654426575\n",
      "edge_vol 9248.0\n",
      "Epoch: 0335 train_loss= 0.41603 val_loss= 1.12091 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  358.2045953273773\n",
      "edge_vol 9248.0\n",
      "Epoch: 0336 train_loss= 0.41463 val_loss= 1.11947 train_acc= 0.99167 val_acc= 0.71200 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "time  359.18830132484436\n",
      "edge_vol 9248.0\n",
      "Epoch: 0337 train_loss= 0.41355 val_loss= 1.11803 train_acc= 0.99167 val_acc= 0.71000 best_val_acc_trail= 0.71600 test_acc= 0.70800\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from utils import *\n",
    "from models import GCN, PTDNetGCN\n",
    "from metrics import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Settings\n",
    "args.dataset=dataset_name\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "all_labels = y_train + y_test+y_val\n",
    "single_label = np.argmax(all_labels,axis=-1)\n",
    "\n",
    "nodesize = features.shape[0]\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=dtype)\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=dtype)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=dtype)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=dtype)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc_trail = 0\n",
    "best_val_loss = 10000\n",
    "import time\n",
    "begin = time.time()\n",
    "\n",
    "model = PTDNetGCN(input_dim=features.shape[1], output_dim=y_train.shape[1])\n",
    "model.set_fea_adj(np.array(range(adj.shape[0])), features_tensor, adj_tensor)\n",
    "\n",
    "best_epoch = 0\n",
    "curr_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    temperature = max(0.05,args.init_temperature * pow(args.temperature_decay, epoch))\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = []\n",
    "        for l in range(args.outL):\n",
    "            output = model.call(temperature,training=True)\n",
    "            preds.append(tf.expand_dims(output,0))\n",
    "        all_preds = tf.concat(preds,axis=0)\n",
    "        mean_preds = tf.reduce_mean(preds,axis=0)\n",
    "        consistency_loss = tf.nn.l2_loss(mean_preds-all_preds)\n",
    "\n",
    "        cross_loss = masked_softmax_cross_entropy(mean_preds, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        lossl0 = model.lossl0(temperature)\n",
    "        nuclear = model.my_nuclear()\n",
    "        #nuclear = model.nuclear()\n",
    "        loss = cross_loss + args.weight_decay*lossL2 + args.lambda1*lossl0 + args.lambda3*nuclear + args.coff_consis*consistency_loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call(None, training=False)\n",
    "    edges_volumn = tf.reduce_sum(model.maskes[0])\n",
    "    print('edge_vol',edges_volumn.numpy())\n",
    "\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_epoch = epoch\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        if val_acc>best_val_acc_trail:\n",
    "            best_test_acc = test_acc\n",
    "            best_val_acc_trail = val_acc\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\"best_val_acc_trail=\", \"{:.5f}\".format(best_val_acc_trail),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))\n",
    "\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    print('time ',(end-begin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.79149 val_loss= 1.79123 train_acc= 0.24200 val_acc= 0.24200 test_acc= 0.27200\n",
      "Epoch: 0002 train_loss= 1.78913 val_loss= 1.79051 train_acc= 0.33800 val_acc= 0.33800 test_acc= 0.37900\n",
      "Epoch: 0003 train_loss= 1.78676 val_loss= 1.78976 train_acc= 0.44800 val_acc= 0.44800 test_acc= 0.45900\n",
      "Epoch: 0004 train_loss= 1.78438 val_loss= 1.78897 train_acc= 0.50600 val_acc= 0.50600 test_acc= 0.50600\n",
      "Epoch: 0005 train_loss= 1.78194 val_loss= 1.78811 train_acc= 0.54400 val_acc= 0.54400 test_acc= 0.53400\n",
      "Epoch: 0006 train_loss= 1.77940 val_loss= 1.78719 train_acc= 0.57400 val_acc= 0.57400 test_acc= 0.55600\n",
      "Epoch: 0007 train_loss= 1.77672 val_loss= 1.78618 train_acc= 0.59400 val_acc= 0.59400 test_acc= 0.57800\n",
      "Epoch: 0008 train_loss= 1.77390 val_loss= 1.78508 train_acc= 0.61000 val_acc= 0.61000 test_acc= 0.60000\n",
      "Epoch: 0009 train_loss= 1.77090 val_loss= 1.78390 train_acc= 0.61600 val_acc= 0.61600 test_acc= 0.60600\n",
      "Epoch: 0010 train_loss= 1.76772 val_loss= 1.78264 train_acc= 0.61800 val_acc= 0.61800 test_acc= 0.61100\n",
      "Epoch: 0011 train_loss= 1.76436 val_loss= 1.78130 train_acc= 0.62000 val_acc= 0.62000 test_acc= 0.61300\n",
      "Epoch: 0012 train_loss= 1.76082 val_loss= 1.77989 train_acc= 0.62400 val_acc= 0.62400 test_acc= 0.62000\n",
      "Epoch: 0013 train_loss= 1.75710 val_loss= 1.77841 train_acc= 0.62400 val_acc= 0.62400 test_acc= 0.62000\n",
      "Epoch: 0014 train_loss= 1.75320 val_loss= 1.77686 train_acc= 0.62800 val_acc= 0.62800 test_acc= 0.62200\n",
      "Epoch: 0015 train_loss= 1.74913 val_loss= 1.77525 train_acc= 0.62800 val_acc= 0.62800 test_acc= 0.62200\n",
      "Epoch: 0016 train_loss= 1.74489 val_loss= 1.77358 train_acc= 0.63200 val_acc= 0.63200 test_acc= 0.62700\n",
      "Epoch: 0017 train_loss= 1.74048 val_loss= 1.77185 train_acc= 0.63600 val_acc= 0.63600 test_acc= 0.62600\n",
      "Epoch: 0018 train_loss= 1.73590 val_loss= 1.77006 train_acc= 0.63600 val_acc= 0.63600 test_acc= 0.62600\n",
      "Epoch: 0019 train_loss= 1.73117 val_loss= 1.76823 train_acc= 0.63400 val_acc= 0.63400 test_acc= 0.62600\n",
      "Epoch: 0020 train_loss= 1.72628 val_loss= 1.76634 train_acc= 0.63400 val_acc= 0.63400 test_acc= 0.62600\n",
      "Epoch: 0021 train_loss= 1.72124 val_loss= 1.76440 train_acc= 0.64000 val_acc= 0.64000 test_acc= 0.62800\n",
      "Epoch: 0022 train_loss= 1.71606 val_loss= 1.76241 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.63000\n",
      "Epoch: 0023 train_loss= 1.71073 val_loss= 1.76039 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.63000\n",
      "Epoch: 0024 train_loss= 1.70528 val_loss= 1.75832 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.63000\n",
      "Epoch: 0025 train_loss= 1.69970 val_loss= 1.75621 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.62900\n",
      "Epoch: 0026 train_loss= 1.69399 val_loss= 1.75405 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.62900\n",
      "Epoch: 0027 train_loss= 1.68817 val_loss= 1.75186 train_acc= 0.64200 val_acc= 0.64200 test_acc= 0.62900\n",
      "Epoch: 0028 train_loss= 1.68223 val_loss= 1.74964 train_acc= 0.64600 val_acc= 0.64600 test_acc= 0.62700\n",
      "Epoch: 0029 train_loss= 1.67617 val_loss= 1.74737 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.62800\n",
      "Epoch: 0030 train_loss= 1.67000 val_loss= 1.74506 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.62800\n",
      "Epoch: 0031 train_loss= 1.66372 val_loss= 1.74271 train_acc= 0.64400 val_acc= 0.64400 test_acc= 0.62800\n",
      "Epoch: 0032 train_loss= 1.65734 val_loss= 1.74031 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.62800\n",
      "Epoch: 0033 train_loss= 1.65085 val_loss= 1.73788 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.62800\n",
      "Epoch: 0034 train_loss= 1.64426 val_loss= 1.73541 train_acc= 0.64600 val_acc= 0.64600 test_acc= 0.62800\n",
      "Epoch: 0035 train_loss= 1.63756 val_loss= 1.73289 train_acc= 0.64600 val_acc= 0.64600 test_acc= 0.62800\n",
      "Epoch: 0036 train_loss= 1.63077 val_loss= 1.73034 train_acc= 0.64600 val_acc= 0.64600 test_acc= 0.62800\n",
      "Epoch: 0037 train_loss= 1.62389 val_loss= 1.72773 train_acc= 0.64800 val_acc= 0.64800 test_acc= 0.62900\n",
      "Epoch: 0038 train_loss= 1.61691 val_loss= 1.72509 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.63000\n",
      "Epoch: 0039 train_loss= 1.60982 val_loss= 1.72241 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.63000\n",
      "Epoch: 0040 train_loss= 1.60266 val_loss= 1.71968 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.63000\n",
      "Epoch: 0041 train_loss= 1.59540 val_loss= 1.71692 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.63000\n",
      "Epoch: 0042 train_loss= 1.58805 val_loss= 1.71412 train_acc= 0.65000 val_acc= 0.65000 test_acc= 0.63000\n",
      "Epoch: 0043 train_loss= 1.58062 val_loss= 1.71127 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.63200\n",
      "Epoch: 0044 train_loss= 1.57309 val_loss= 1.70838 train_acc= 0.65200 val_acc= 0.65200 test_acc= 0.63200\n",
      "Epoch: 0045 train_loss= 1.56548 val_loss= 1.70545 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63500\n",
      "Epoch: 0046 train_loss= 1.55779 val_loss= 1.70247 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63500\n",
      "Epoch: 0047 train_loss= 1.55002 val_loss= 1.69945 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63500\n",
      "Epoch: 0048 train_loss= 1.54217 val_loss= 1.69640 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63500\n",
      "Epoch: 0049 train_loss= 1.53424 val_loss= 1.69330 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63500\n",
      "Epoch: 0050 train_loss= 1.52624 val_loss= 1.69017 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0051 train_loss= 1.51815 val_loss= 1.68699 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0052 train_loss= 1.51000 val_loss= 1.68378 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0053 train_loss= 1.50178 val_loss= 1.68053 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0054 train_loss= 1.49349 val_loss= 1.67724 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0055 train_loss= 1.48513 val_loss= 1.67391 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0056 train_loss= 1.47671 val_loss= 1.67054 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0057 train_loss= 1.46823 val_loss= 1.66714 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0058 train_loss= 1.45969 val_loss= 1.66371 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0059 train_loss= 1.45109 val_loss= 1.66025 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0060 train_loss= 1.44245 val_loss= 1.65674 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0061 train_loss= 1.43375 val_loss= 1.65321 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0062 train_loss= 1.42500 val_loss= 1.64963 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0063 train_loss= 1.41621 val_loss= 1.64602 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0064 train_loss= 1.40737 val_loss= 1.64238 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0065 train_loss= 1.39849 val_loss= 1.63872 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0066 train_loss= 1.38958 val_loss= 1.63504 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0067 train_loss= 1.38064 val_loss= 1.63132 train_acc= 0.65400 val_acc= 0.65400 test_acc= 0.63700\n",
      "Epoch: 0068 train_loss= 1.37167 val_loss= 1.62757 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0069 train_loss= 1.36267 val_loss= 1.62379 train_acc= 0.65600 val_acc= 0.65600 test_acc= 0.63700\n",
      "Epoch: 0070 train_loss= 1.35365 val_loss= 1.61999 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64700\n",
      "Epoch: 0071 train_loss= 1.34461 val_loss= 1.61617 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64700\n",
      "Epoch: 0072 train_loss= 1.33555 val_loss= 1.61232 train_acc= 0.65800 val_acc= 0.65800 test_acc= 0.64700\n",
      "Epoch: 0073 train_loss= 1.32647 val_loss= 1.60845 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.65000\n",
      "Epoch: 0074 train_loss= 1.31739 val_loss= 1.60455 train_acc= 0.66000 val_acc= 0.66000 test_acc= 0.65000\n",
      "Epoch: 0075 train_loss= 1.30830 val_loss= 1.60064 train_acc= 0.66200 val_acc= 0.66200 test_acc= 0.65300\n",
      "Epoch: 0076 train_loss= 1.29921 val_loss= 1.59671 train_acc= 0.66400 val_acc= 0.66400 test_acc= 0.65500\n",
      "Epoch: 0077 train_loss= 1.29012 val_loss= 1.59277 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.65600\n",
      "Epoch: 0078 train_loss= 1.28103 val_loss= 1.58882 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.65600\n",
      "Epoch: 0079 train_loss= 1.27194 val_loss= 1.58485 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0080 train_loss= 1.26287 val_loss= 1.58086 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0081 train_loss= 1.25381 val_loss= 1.57685 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0082 train_loss= 1.24477 val_loss= 1.57284 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.65800\n",
      "Epoch: 0083 train_loss= 1.23573 val_loss= 1.56882 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.65800\n",
      "Epoch: 0084 train_loss= 1.22673 val_loss= 1.56479 train_acc= 0.66600 val_acc= 0.66600 test_acc= 0.65800\n",
      "Epoch: 0085 train_loss= 1.21775 val_loss= 1.56076 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0086 train_loss= 1.20880 val_loss= 1.55672 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0087 train_loss= 1.19988 val_loss= 1.55266 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.65800\n",
      "Epoch: 0088 train_loss= 1.19099 val_loss= 1.54861 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.65800\n",
      "Epoch: 0089 train_loss= 1.18215 val_loss= 1.54456 train_acc= 0.66800 val_acc= 0.66800 test_acc= 0.65800\n",
      "Epoch: 0090 train_loss= 1.17334 val_loss= 1.54052 train_acc= 0.67000 val_acc= 0.67000 test_acc= 0.65800\n",
      "Epoch: 0091 train_loss= 1.16456 val_loss= 1.53646 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.65800\n",
      "Epoch: 0092 train_loss= 1.15585 val_loss= 1.53242 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.65800\n",
      "Epoch: 0093 train_loss= 1.14717 val_loss= 1.52837 train_acc= 0.67200 val_acc= 0.67200 test_acc= 0.65800\n",
      "Epoch: 0094 train_loss= 1.13855 val_loss= 1.52432 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.66000\n",
      "Epoch: 0095 train_loss= 1.12997 val_loss= 1.52029 train_acc= 0.67400 val_acc= 0.67400 test_acc= 0.66000\n",
      "Epoch: 0096 train_loss= 1.12145 val_loss= 1.51626 train_acc= 0.67600 val_acc= 0.67600 test_acc= 0.66300\n",
      "Epoch: 0097 train_loss= 1.11298 val_loss= 1.51223 train_acc= 0.67800 val_acc= 0.67800 test_acc= 0.66300\n",
      "Epoch: 0098 train_loss= 1.10458 val_loss= 1.50821 train_acc= 0.68000 val_acc= 0.68000 test_acc= 0.66400\n",
      "Epoch: 0099 train_loss= 1.09622 val_loss= 1.50421 train_acc= 0.68200 val_acc= 0.68200 test_acc= 0.66400\n",
      "Epoch: 0100 train_loss= 1.08795 val_loss= 1.50021 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66500\n",
      "Epoch: 0101 train_loss= 1.07972 val_loss= 1.49622 train_acc= 0.68400 val_acc= 0.68400 test_acc= 0.66500\n",
      "Epoch: 0102 train_loss= 1.07155 val_loss= 1.49227 train_acc= 0.68600 val_acc= 0.68600 test_acc= 0.66800\n",
      "Epoch: 0103 train_loss= 1.06347 val_loss= 1.48832 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66800\n",
      "Epoch: 0104 train_loss= 1.05544 val_loss= 1.48437 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66800\n",
      "Epoch: 0105 train_loss= 1.04748 val_loss= 1.48045 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66900\n",
      "Epoch: 0106 train_loss= 1.03959 val_loss= 1.47656 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0107 train_loss= 1.03178 val_loss= 1.47269 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0108 train_loss= 1.02403 val_loss= 1.46881 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.67000\n",
      "Epoch: 0109 train_loss= 1.01635 val_loss= 1.46495 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67200\n",
      "Epoch: 0110 train_loss= 1.00876 val_loss= 1.46111 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67200\n",
      "Epoch: 0111 train_loss= 1.00123 val_loss= 1.45731 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67200\n",
      "Epoch: 0112 train_loss= 0.99378 val_loss= 1.45353 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67100\n",
      "Epoch: 0113 train_loss= 0.98640 val_loss= 1.44978 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67100\n",
      "Epoch: 0114 train_loss= 0.97911 val_loss= 1.44605 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67100\n",
      "Epoch: 0115 train_loss= 0.97189 val_loss= 1.44234 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.67100\n",
      "Epoch: 0116 train_loss= 0.96474 val_loss= 1.43865 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.67000\n",
      "Epoch: 0117 train_loss= 0.95768 val_loss= 1.43498 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0118 train_loss= 0.95068 val_loss= 1.43134 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0119 train_loss= 0.94378 val_loss= 1.42774 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0120 train_loss= 0.93693 val_loss= 1.42416 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0121 train_loss= 0.93019 val_loss= 1.42061 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0122 train_loss= 0.92351 val_loss= 1.41708 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0123 train_loss= 0.91690 val_loss= 1.41359 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0124 train_loss= 0.91038 val_loss= 1.41011 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0125 train_loss= 0.90393 val_loss= 1.40667 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0126 train_loss= 0.89756 val_loss= 1.40324 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0127 train_loss= 0.89125 val_loss= 1.39985 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0128 train_loss= 0.88504 val_loss= 1.39651 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0129 train_loss= 0.87890 val_loss= 1.39319 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0130 train_loss= 0.87282 val_loss= 1.38988 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0131 train_loss= 0.86682 val_loss= 1.38662 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0132 train_loss= 0.86090 val_loss= 1.38338 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0133 train_loss= 0.85506 val_loss= 1.38017 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0134 train_loss= 0.84928 val_loss= 1.37699 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0135 train_loss= 0.84358 val_loss= 1.37383 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0136 train_loss= 0.83794 val_loss= 1.37071 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0137 train_loss= 0.83240 val_loss= 1.36761 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0138 train_loss= 0.82690 val_loss= 1.36455 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0139 train_loss= 0.82148 val_loss= 1.36152 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0140 train_loss= 0.81613 val_loss= 1.35853 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0141 train_loss= 0.81085 val_loss= 1.35556 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0142 train_loss= 0.80564 val_loss= 1.35260 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0143 train_loss= 0.80047 val_loss= 1.34969 train_acc= 0.69800 val_acc= 0.69800 test_acc= 0.66900\n",
      "Epoch: 0144 train_loss= 0.79540 val_loss= 1.34682 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0145 train_loss= 0.79038 val_loss= 1.34397 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0146 train_loss= 0.78543 val_loss= 1.34116 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0147 train_loss= 0.78053 val_loss= 1.33835 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0148 train_loss= 0.77571 val_loss= 1.33558 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0149 train_loss= 0.77093 val_loss= 1.33283 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0150 train_loss= 0.76623 val_loss= 1.33012 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0151 train_loss= 0.76159 val_loss= 1.32745 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0152 train_loss= 0.75700 val_loss= 1.32479 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0153 train_loss= 0.75246 val_loss= 1.32216 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0154 train_loss= 0.74799 val_loss= 1.31955 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0155 train_loss= 0.74358 val_loss= 1.31699 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0156 train_loss= 0.73924 val_loss= 1.31442 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0157 train_loss= 0.73493 val_loss= 1.31191 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0158 train_loss= 0.73068 val_loss= 1.30941 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0159 train_loss= 0.72649 val_loss= 1.30695 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0160 train_loss= 0.72236 val_loss= 1.30451 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0161 train_loss= 0.71826 val_loss= 1.30208 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0162 train_loss= 0.71423 val_loss= 1.29970 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0163 train_loss= 0.71024 val_loss= 1.29734 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0164 train_loss= 0.70630 val_loss= 1.29500 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0165 train_loss= 0.70242 val_loss= 1.29267 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0166 train_loss= 0.69858 val_loss= 1.29038 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0167 train_loss= 0.69479 val_loss= 1.28812 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0168 train_loss= 0.69105 val_loss= 1.28588 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0169 train_loss= 0.68735 val_loss= 1.28366 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0170 train_loss= 0.68369 val_loss= 1.28147 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0171 train_loss= 0.68009 val_loss= 1.27928 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0172 train_loss= 0.67652 val_loss= 1.27712 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0173 train_loss= 0.67300 val_loss= 1.27499 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0174 train_loss= 0.66953 val_loss= 1.27291 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0175 train_loss= 0.66610 val_loss= 1.27081 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0176 train_loss= 0.66270 val_loss= 1.26874 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0177 train_loss= 0.65934 val_loss= 1.26670 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0178 train_loss= 0.65602 val_loss= 1.26468 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0179 train_loss= 0.65276 val_loss= 1.26268 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.66900\n",
      "Epoch: 0180 train_loss= 0.64951 val_loss= 1.26071 train_acc= 0.68800 val_acc= 0.68800 test_acc= 0.66900\n",
      "Epoch: 0181 train_loss= 0.64632 val_loss= 1.25874 train_acc= 0.69000 val_acc= 0.69000 test_acc= 0.66900\n",
      "Epoch: 0182 train_loss= 0.64315 val_loss= 1.25680 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0183 train_loss= 0.64003 val_loss= 1.25488 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0184 train_loss= 0.63694 val_loss= 1.25298 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0185 train_loss= 0.63388 val_loss= 1.25112 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0186 train_loss= 0.63087 val_loss= 1.24928 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0187 train_loss= 0.62790 val_loss= 1.24743 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0188 train_loss= 0.62495 val_loss= 1.24559 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0189 train_loss= 0.62203 val_loss= 1.24378 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0190 train_loss= 0.61915 val_loss= 1.24198 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0191 train_loss= 0.61630 val_loss= 1.24022 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0192 train_loss= 0.61349 val_loss= 1.23848 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0193 train_loss= 0.61070 val_loss= 1.23674 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0194 train_loss= 0.60795 val_loss= 1.23504 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0195 train_loss= 0.60524 val_loss= 1.23334 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0196 train_loss= 0.60254 val_loss= 1.23164 train_acc= 0.69200 val_acc= 0.69200 test_acc= 0.66900\n",
      "Epoch: 0197 train_loss= 0.59987 val_loss= 1.23000 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0198 train_loss= 0.59723 val_loss= 1.22835 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0199 train_loss= 0.59464 val_loss= 1.22670 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0200 train_loss= 0.59205 val_loss= 1.22507 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0201 train_loss= 0.58949 val_loss= 1.22347 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0202 train_loss= 0.58698 val_loss= 1.22189 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0203 train_loss= 0.58448 val_loss= 1.22032 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0204 train_loss= 0.58202 val_loss= 1.21876 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0205 train_loss= 0.57957 val_loss= 1.21723 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0206 train_loss= 0.57715 val_loss= 1.21571 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0207 train_loss= 0.57476 val_loss= 1.21421 train_acc= 0.69400 val_acc= 0.69400 test_acc= 0.66900\n",
      "Epoch: 0208 train_loss= 0.57240 val_loss= 1.21270 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0209 train_loss= 0.57005 val_loss= 1.21122 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0210 train_loss= 0.56773 val_loss= 1.20973 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0211 train_loss= 0.56542 val_loss= 1.20828 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0212 train_loss= 0.56316 val_loss= 1.20683 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0213 train_loss= 0.56091 val_loss= 1.20540 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0214 train_loss= 0.55868 val_loss= 1.20400 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0215 train_loss= 0.55648 val_loss= 1.20260 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0216 train_loss= 0.55431 val_loss= 1.20119 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Epoch: 0217 train_loss= 0.55214 val_loss= 1.19983 train_acc= 0.69600 val_acc= 0.69600 test_acc= 0.66900\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "from config import args\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import *\n",
    "from models import GCN_dropedge\n",
    "from metrics import *\n",
    "\n",
    "# Settings\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "args.dataset=dataset_name\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\n",
    "adj=modified_adj\n",
    "\n",
    "tuple_adj = sparse_to_tuple(adj.tocoo())\n",
    "adj_tensor = tf.SparseTensor(*tuple_adj)\n",
    "\n",
    "features = preprocess_features(features)\n",
    "\n",
    "model = GCN_dropedge(input_dim=features.shape[1], output_dim=y_train.shape[1], adj=adj_tensor)\n",
    "\n",
    "\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=tf.float32)\n",
    "train_mask_tensor = tf.convert_to_tensor(train_mask)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=tf.float32)\n",
    "test_mask_tensor = tf.convert_to_tensor(test_mask)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val,dtype=tf.float32)\n",
    "val_mask_tensor = tf.convert_to_tensor(val_mask)\n",
    "\n",
    "best_test_acc = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = 10000\n",
    "\n",
    "\n",
    "curr_step = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model.call((features_tensor),training=True)\n",
    "        cross_loss = masked_softmax_cross_entropy(output, y_train_tensor,train_mask_tensor)\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "        loss = cross_loss + args.weight_decay*lossL2\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    output = model.call((features_tensor), training=False)\n",
    "    train_acc = masked_accuracy(output, y_train_tensor,train_mask_tensor)\n",
    "    val_acc  = masked_accuracy(output, y_val_tensor,val_mask_tensor)\n",
    "    val_loss = masked_softmax_cross_entropy(output, y_val_tensor, val_mask_tensor)\n",
    "    test_acc  = masked_accuracy(output, y_test_tensor,test_mask_tensor)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        curr_step = 0\n",
    "        best_test_acc = test_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss= val_loss\n",
    "        # Print results\n",
    "\n",
    "    else:\n",
    "        curr_step +=1\n",
    "    if curr_step > args.early_stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cross_loss),\"val_loss=\", \"{:.5f}\".format(val_loss),\n",
    "      \"train_acc=\", \"{:.5f}\".format(val_acc), \"val_acc=\", \"{:.5f}\".format(val_acc),\n",
    "      \"test_acc=\", \"{:.5f}\".format(best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
